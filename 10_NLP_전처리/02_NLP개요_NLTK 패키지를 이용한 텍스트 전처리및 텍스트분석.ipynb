{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP(Natural Language Processing) 자연어 처리란"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어\n",
    "- 사람이 일상적으로 사용하는 언어를 말한다.\n",
    "    - 어떤 목적을 가지고 사람이 만든 것이 아니라 **자연적으로 만들어진 언어**를 말한다.\n",
    "- **인공언어**\n",
    "    - 특정 목적을 위해 사람이 인위적으로 만든 언어로 자연어의 대척점에 있는 언어 개념.\n",
    "    - 예: 프로그래밍 언어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리 (NLP)\n",
    "- 사람이 사용하는 자연어를 컴퓨터가 이해 할 수 있도록 처리하는 컴퓨터 공학의 한 분야.\n",
    "- 자연어 처리는 오래된 분야이지만, 딥러닝이 적용되면서 큰 발전을 이루었고, 특히 대규모 언어 모델(LLM)의 등장 이후로 획기적인 발전을 이루었다.\n",
    "\n",
    "### 자연어 처리 응용분야\n",
    "- **텍스트 분류(Text classification)**\n",
    "   - 입력받은 문장을 분류하는 문제로 대표적으로 감성분석(sementic analysis)가 있다.\n",
    "   - 감성분석\n",
    "       -  입력받은 텍스트가 어떤 감정의 글인지를 분류한다. 일반적으로 긍정, 중립, 부정적 인지를 분류한다.\n",
    "- **개체명인식(Named Entity Recognition)**\n",
    "   - 문장의 각 단어가 어떤 종류(의미) 인지를 찾는 문제\n",
    "   - 미국에 사는 톰은 스무살입니다. ==> 미국: 위치, 톰: 이름, 스므살: 나이\n",
    "- **품사태깅(Pos tagging)**\n",
    "   - 문장의 각 토큰의 품사를 찾는 문제\n",
    "   - 미국에 사는 톰은 스무살입니다. ==> 미국: 대명사, 예: 조사,  톰: 대명사, 은: 조사, 스무: 명사, 살: 명사, 이다: 동사\n",
    "- **문서요약**\n",
    "  - 제공 받은 문서의 내용을 요약하는 문제\n",
    "- **텍스트 생성**\n",
    "   - 주어진 문장을 기반으로 문장을 생성하는 문제\n",
    "- **대화형 시스템(Chatbot)**\n",
    "   - 입력받은 문장에 대한 답을 하는 시스템.\n",
    "   - Encoder는 질문을 받아 처리하고 Decoder는 답변을 생성하는 seq2seq 구조를 사용한다.\n",
    "- **기계번역(Machine translation)**\n",
    "   - 번역 시스템\n",
    "   - Encoder는 번역 대상문장을 입력받아 처리하고 Decoder는 번역 문장을 생성하는 seq2seq 구조를 사용한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 용어\n",
    "- **말뭉치(corpus, corpora)**: NLP 머신러닝 모델을 학습 시키기 위한  자연어 데이터 셋.\n",
    "- **어간(stem)**\n",
    "\t- 어간은 접미사나 다른 변화 형태가 추가되기 전의 **단어 기본 형태**를 말한다. 즉 활용시 변하지 않는 부분을 말한다.\n",
    "\t- **view** + ing, **view** + er\n",
    "\t- **먹** + 습니다.  **먹** + 었다.  **먹** + 고\n",
    "\t- **예쁘**  + 다, **예쁘**+ 고, **예쁘** + 지만, **예쁘**+ 어서(예뻐서)\n",
    "- **어미**\n",
    "\t- 어미는  어간(주로  용언-동사,형용사) 뒤에 붙어서 단어의 문법적 기능이나 의미를 구체화하는 요소.\n",
    "\t- walked: walk+**ed**: 과거시제, run+**s**: 3인칭 단수 주어의 동사에 붙는 어미.\n",
    "\t- 먹 + **었다**,  먹 + **다**, 갔습니까? : 가 + **ㅆ습니까?**\n",
    "- **조사**\n",
    "\t- 조사는 체언(명사, 대명사) 뒤에 붙어 그 단어의 문법적인 기능이나 관계(예: 주체, 목적, 소유, 방향 등)를 나타내는 요소.\n",
    "\t- 학교 + **에서** (장소를 나타내는 조사), 책+**을** 읽다. (목적어를 타나내는 조사.), 우리+**가** 간다. (주어를 나타내는 조사)\n",
    "- **어절(word segment)**\n",
    "\t- 문장 성분의 최소 단위로 띄어 쓰기의 기준이 된다. \n",
    "\t- 나는 학교에 간다. -> \"**나는**\", \"**학교에**\", \"**간다**\"\t\n",
    "- **형태소(morpheme)**: 의미(뜻)을 가진 가장 작은 언어의 단위, 더 이상 나눌 수 없는 언어의 조각 단위. 형태소는 그 자체로 의미를 가지며 단어를 형성하거나 변형 시키는데 사용한다.\n",
    "\t- 자립 형태소:  명사, 동사, 형용사 같이 독립적으로 사용될 수 있는 형태소\n",
    "\t\t- 나, 너, 택시, 가다, you, have\n",
    "\t- 의존 형태소: 조사, 접미사, 접두사 같이 다른 형태소와 결합해서 사용 되야 하는 형태소.\n",
    "\t\t- \\~의, \\~가, un\\~, \\~able\n",
    "\n",
    "> - **어간과 형태소의 차이**\n",
    "> \t-  **형태소**는 언어의 의미를 가진 최소 단위.   \n",
    "> \t  **어간**은 특정 단어에서 그 단어의 핵심 의미를 담은 부분. 형태소에서 주로 자립 형태소에 해당하고 어미와 같은 의존 형태소가 결합하여 문법적 기능이나 형태를 변화시킬 수 있다.\n",
    "> \t-  **형태소**는 의미를 구성하는 기본 단위로서의 역할을 하며, **어간**은 특히 단어를 형성하고 변형 시키는 기반으로서의 역할을 한다.\n",
    "\n",
    "> - **언어 형태 유형 종류**\n",
    ">     - **교착어**    \n",
    ">         - **정의**: 교착어는 단어에 하나 이상의 접사가 추가되어 새로운 의미나 문법적 기능을 형성하는 언어\n",
    ">         \t- 예) \"책\"\n",
    ">         \t\t- 책이\n",
    ">         \t\t- 책의\n",
    ">         \t\t- 책에\n",
    ">         \t\t- 책을\n",
    ">         - **주요 언어**: 한국어, 몽골어, 튀르키예어, 우랄어족(핀란드어, 헝가리어) 등\n",
    ">     \n",
    ">     - **굴절어**\n",
    ">         - **정의**: 굴절어는 단어 내의 모음이나 자음의 변화로 문법적 기능을 나타내는 언어이다. 이 변화는 단어의 뜻을 변경하지 않으면서도 시제, 인칭, 수, 성 등을 표현할 수 있다.\n",
    ">         \t- 단어 내의 변화로 문법적 기능을 표현.\n",
    ">         - 예) 라틴어에서 \"liber\"는 책을 의미\n",
    ">         \t-  **liber** (단수 주격, 남성) - \"책이/책은\"\n",
    ">         \t- **libri** (단수 소유격, 남성) - \"책의\"\n",
    ">         \t- **libro** (단수 여격, 남성) - \"책에\"\n",
    ">         \t- **librum** (단수 목적격, 남성) - \"책을\"\n",
    ">         - **주요 언어**: 라틴어, 그리스어, 이탈리아어, 스페인어, 독일어, 러시아어, 힌디어 등\n",
    ">     \n",
    ">     - **고립어**\n",
    ">         - **정의**: 어형 변화나 접사가 없고 위치에 의해서 단어가 문장 속에서 가지는 여러가지 관계가 결정되는 언어. 분석어, 위치어라고도 한다.\n",
    ">         - **주요 언어**: 영어, 중국어, 베트남어, 태국어 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리 모델링 프로세스\n",
    "\n",
    "1. **데이터 수집**\n",
    "    - **Corpus(말뭉치)**: 자연어 학습을 위해 수집한 언어 표본 집합을 \"말뭉치\" 또는 \"Corpus\"라고 한다.\n",
    "    - **수집 방법**\n",
    "        - 공개 데이터 사용\n",
    "        - 데이터 구매\n",
    "        - 웹 크롤링\n",
    "\n",
    "2. **텍스트 전처리**\n",
    "     - 분석 목적에 맞게 텍스트를 전처리한다.\n",
    "     - **정제 (Cleaning)**\n",
    "       - 문서 내 노이즈(불필요한 문자, 기호, 빈도가 낮은 단어 등)를 제거한다.\n",
    "       - 불용어(Stop word) 제거: 분석에 불필요한 단어(예: \"의\", \"에\", \"은\" 등)를 삭제하여 모델 성능을 향상시킨다.\n",
    "     - **정규화 (Normalization)**\n",
    "       - 같은 의미의 단어들을 하나의 형태로 통일한다. (예: \"말하다\", \"말하면\", \"말하기\" → \"말\")\n",
    "       - **주요 기법**\n",
    "         - 어간 추출 (Stemming), 원형 복원 (Lemmatization), 형태소 분석\n",
    "\n",
    "3. **텍스트 토큰화**\n",
    "    - 문서의 텍스트를 분석하기 위해 최소 단위로 나누는 작업이다.\n",
    "    - 보통 단어 단위나 글자 단위로 나누며, 토큰을 기계가 이해할 수 있도록 정수 형태로 변환한다.\n",
    "\n",
    "4. **임베딩**\n",
    "    - 각 토큰(단어)의 의미나 특성을 보다 잘 표현할 수 있도록 단어를 고차원 벡터로 변환한다.\n",
    "    - Feature Extraction(특성 추출)과정이라고 볼 수 있다.\n",
    "    - 빈도수 기반 통계적 방식과 뉴럴 네트워크를 이용한 학습 방식이 있다.\n",
    "    - **주요 기법**\n",
    "      - TF-IDF, Word2Vec, FastText\n",
    "\n",
    "5. **모델링**\n",
    "    - 임베딩된 데이터를 입력으로 받아 자연어 관련 문제를 해결하는 머신러닝 또는 딥러닝 모델을 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 전처리\n",
    "\n",
    "## 다양한 text 전처리 패키지(Library)\n",
    "### 영문\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "- [spaCy](https://spacy.io/)\n",
    "### 한글\n",
    "- [KoNLPy](https://konlpy.org/ko/latest/)\n",
    "- [KiwiPiePy](https://github.com/bab2min/kiwipiepy)\n",
    "- [soynlp](https://github.com/lovit/soynlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK \n",
    "- Natural Language ToolKit\n",
    "- https://www.nltk.org/\n",
    "- 자연어 처리를 위한 대표적인 파이썬 패키지. 한국어는 지원하지 않는다.\n",
    "\n",
    "## NLTK 설치\n",
    "- nltk 패키지 설치\n",
    "    - `pip install nltk`\n",
    "- NLTK 추가 패키지 설치\n",
    "```python\n",
    "import nltk\n",
    "nltk.download() # 설치 GUI 프로그램 실행을 실행해 다운로드\n",
    "nltk.download('패키지명')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK 주요기능\n",
    "\n",
    "- ### 텍스트 토큰화/정규화/전처리등 처리를 위한 기능 제공\n",
    "    - 토큰화(Tokenization)\n",
    "    - Stop word(불용어) 제공\n",
    "    - 형태소 분석\n",
    "        - 형태소\n",
    "            - 의미가 있는 가장 작은 말의 단위\n",
    "        - 형태소 분석\n",
    "            - 말뭉치에서 의미있는(분석시 필요한) 형태소들만 추출하는 것           \n",
    "        - 어간추출(Stemming)\n",
    "        - 원형복원(Lemmatization)\n",
    "        - 품사부착(POS tagging - Part Of Speech)\n",
    "        \n",
    "- ### 텍스트 분석 기능을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK 텍스트 정규화 기본 문법\n",
    "\n",
    "## Tokenization (토큰화)\n",
    "\n",
    "- 문자열을 분석의 최소단위(Token)으로 나누는 작업.\n",
    "- 글자단위, 단어단위, 형태소 단위, 문장단위 등 다양 방식으로 나눌 수있다.\n",
    "- 분석을 위해 문서를 작은 단위로 나누는 작업.\n",
    "- **주요 Tokenizer (함수)**\n",
    "    - **sent_tokenize()** : 문장단위로 나눠준다.\n",
    "    - **word_tokenize()** : 단어단위로 나눠준다.\n",
    "    - **regexp_tokenize()** : 토큰의 단위를 정규표현식으로 지정\n",
    "    - return: 분리된 토큰들을 원소로 하는 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = \"\"\"Beautiful is better than ugly.\n",
    "Explicit is better than implicit.\n",
    "Simple is better than complex.\n",
    "Complex is better than complicated.\n",
    "Flat is better than nested.\n",
    "Sparse is better than dense.\n",
    "Readability counts.\n",
    "Special cases aren't special enough to break the rules.\n",
    "Although practicality beats purity.\n",
    "Errors should never pass silently.\n",
    "Unless explicitly silenced.\n",
    "In the face of ambiguity, refuse the temptation to guess.\n",
    "There should be one-- and preferably only one --obvious way to do it.\n",
    "Although that way may not be obvious at first unless you're Dutch.\n",
    "Now is better than never.\n",
    "Although never is often better than *right* now.\n",
    "If the implementation is hard to explain, it's a bad idea.\n",
    "If the implementation is easy to explain, it may be a good idea.\n",
    "Namespaces are one honking great idea -- let's do more of those!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = \"Many of life’s failures are people who didn't realize how close they were to success when they gave up.\"\n",
    "txt2 = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penn Treebank 토큰화 - word 단위 토큰화\n",
    "## 규칙1:  하이픈(-) 으로 구성된 것은 하나의 토큰으로 유지 \"home-base\"\n",
    "## 규칙2:  didn't  접어를 이용해서 축약한 경우는 분할 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword (불용어)\n",
    "- 분석에 필요 없는 단어들을 말한다.\n",
    "    - 문장내에서는 많이 사용되지만 문장의 전체 맥락이나 내용 파악에는 필요 없는 단어들을 말한다.\n",
    "    - 많이 나오기 때문에 중요한 단어로 인식 할 수 있다. 그래서 제거하는 것이 좋다.\n",
    "    - 대표적인 불용어들은 조사, 접미사, 접속사, 대명사 등이 있다. \n",
    "-  실제 분석 대상에 맞게 Stop word 목록을 만들어서 사용한다. \n",
    "    - 보통 불용어로 사용할 단어들을 text 파일에 저장해 놓고 필요할 때 loading 해서 사용한다.\n",
    "    - nltk는 언어별로 일반적인 Stop word 사전을 제공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T02:03:51.905615Z",
     "start_time": "2021-04-13T02:03:51.892618Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    전달받은 text 토큰화해 반환하는 함수\n",
    "    문장별로 단어 리스트(의미를 파악하는데 중요한 단어들)를 2차원 배열 형태로 반환\n",
    "       1차원: 문장 리스트, 2차원: 문장내 토큰.\n",
    "    구두점/특수문자, 숫자, 불용어(stop words)들은 모두 제거한다.\n",
    "    parameters:\n",
    "        text: string - 변환하려는 전체문장\n",
    "    return:\n",
    "        2차원 리스트. 1차원: 문장 리스트, 2차원: 문장내 토큰.\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_text(text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 분석\n",
    "- 형태소\n",
    "    - 일정한 의미가 있는 가장 작은 말의 단위\n",
    "- 형태소 분석  \n",
    "    - 말뭉치에서 의미있는(분석에 필요한) 형태소들만 추출하는 것\n",
    "    - 보통 단어로 부터 어근, 접두사, 접미사, 품사등 언어적 속성을 파악하여 처리한다. \n",
    "- 형태소 분석을 위한 기법\n",
    "    - 어간추출(Stemming)\n",
    "    - 원형(기본형) 복원 (Lemmatization)\n",
    "    - 품사부착 (POS tagging - Part Of Speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어간추출(Stemming)\n",
    "- 어간: 활용어에서 변하지 않는 부분\n",
    "    - painted, paint, painting → 어간: paint\n",
    "    - 보다, 보니, 보고 → 어간: `보`\n",
    "- 어간 추출 목적\n",
    "    - 같은 의미를 가지는 단어의 여러가지 활용이 있을 경우 다른 단어로 카운트 되는 문제점을 해결한다.\n",
    "        - flower, flowers 가 두개의 단어로 카운트 되는 것을 flower로 통일한다.        \n",
    "- nltk의 주요 어간 추출 알고리즘\n",
    "    - Porter Stemmer\n",
    "    - Lancaster Stemmer\n",
    "    - Snowball Stemmer\n",
    "- 메소드\n",
    "    - `stemmer객체.stem(단어)`\n",
    "- stemming의 문제\n",
    "    - 완벽하지 않다는 것이 문제이다.        \n",
    "        - ex) new와 news는 다른 단어 인데 둘다 new로 처리한다.\n",
    "    - 처리후 눈으로 직접 확인해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어에서 토큰화가 어려운 이유\n",
    "- 영어는 띄어쓰기(공백)을 기준으로 토큰화를 진행해도 큰 문제가 없다.\n",
    "- 한국어는 교착어이기 때문에 띄어쓰기를 기준으로 토큰화를 하면 같은 단어가 다른 토큰으로 인식되어 여러개 추출되는 문제가 발생한다.\n",
    "    - 예) \"그가\", \"그는\", \"그의\", \"그와\" 모두 \"그\"를 나타내지만 붙은 조사가 달라 다 다른 토큰으로 추출되게 된다.\n",
    "- 그래서 한국어는 어절 단위 토큰화는 하지 않도록 한다.\n",
    "- 대신 형태소에 기반한 토큰화를 하는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"Working\",\n",
    "    \"works\",\n",
    "    \"worked\",\n",
    "    \"Painting\",\n",
    "    \"Painted\",\n",
    "    \"paints\",\n",
    "    \"Happy\",\n",
    "    \"happier\",\n",
    "    \"happiest\",\n",
    "    \"am\",\n",
    "    \"are\",\n",
    "    \"is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원형(기본형)복원(Lemmatization)\n",
    "- 단어의 원형(기본형)을 반환한다.\n",
    "    - ex) am, is, are => be\n",
    "- 단어의 품사를 지정하면 정확한 결과를 얻을 수 있다. \n",
    "- `WordNetLemmatizer객체.lemmatize(단어 [, pos=품사])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어간추출과 원형복원의 장단점\n",
    "\n",
    "- **원형 복원 (Lemmatization)**\n",
    "\n",
    "   - **장점**:\n",
    "      - 문맥과 품사를 고려하여 정확한 사전적 원형을 반환하므로 의미적으로 정확한 결과를 얻을 수 있다.\n",
    "      - 품사 정보를 유지하기 때문에 의미론적 분석에 유리하다.\n",
    "   - **단점**:\n",
    "      - 형태소 분석과 사전 조회 과정이 필요하기 때문에 어간 추출보다 처리속도가 느리다.\n",
    "      - 구현이 더 복잡하며, 품사 태깅 및 사전 데이터베이스가 필요하다.\n",
    "\n",
    "- **어간 추출 (Stemming)**\n",
    "\n",
    "   - **장점**:\n",
    "     - 규칙 기반 알고리즘으로 동작하므로 처리속도가 빠르다.\n",
    "     - 처리속도가 빠르므로 대량의 텍스트를 효율적으로 처리할 수 있다.\n",
    "     - 정보 검색이나 텍스트 분류 등 단순한 텍스트 정규화 작업에 적합하다\n",
    "   - **단점**:\n",
    "      - 문법적 차이를 고려하지 않고 기계적으로 자르기 때문에 의미 손실이 발생하거나 다른 단어들이 같은 어간으로 추출될 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 품사부착-POS Tagging(Part-Of-Speech Tagging)\n",
    "- 형태소에 품사를 붙이는 작업.\n",
    "    - 품사의 구분이나 표현은 언어, 학자마다 다르다. \n",
    "- NLTK는 [펜 트리뱅크 태그세트](https://bluebreeze.co.kr/1357)(Penn Treebank Tagset) 이용\n",
    "    - 명사 : N으로 시작 (NN-일반명사, NNP-고유명사)\n",
    "    - 형용사 : J로 시작(JJ, JJR-비교급, JJS-최상급)\n",
    "    - 동사: V로 시작 (VB-동사원형, VBP-3인칭 아닌 현재형 동사)\n",
    "    - 부사: R로 시작 (RB-부사)\n",
    "    - `nltk.help.upenn_tagset('키워드')` : 도움말\n",
    "- `pos_tag(단어_리스트)`    \n",
    "    - 단어와 품사를 튜플로 묶은 리스트를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "words = [\"Book\", \"car\", \"have\", \"Korea\", \"is\", 'well', 'can']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 품사부착과 원형복원을 이용해 import this 원형복원하기.\n",
    "- 품사부착으로 품사 조회\n",
    "    - pos_tag와 lemmatization이 사용하는 품사 형태 다르기 때문에 변환함수 만듬\n",
    "- lemmatization하기.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pos-tag 에서 반환한 품사표기(펜 트리뱅크 태그세트)을 WordNetLemmatizer의 품사표기로 변환\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \"\"\"\n",
    "    펜 트리뱅크 품사표기를 WordNetLemmatizer에서 사용하는 품사표기로 변환\n",
    "    형용사/동사/명사/부사 표기 변환\n",
    "    \"\"\"\n",
    "    if pos_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 전처리 프로세스\n",
    "- 클렌징(cleansing)\n",
    "    - 특수문자, 기호 필요없는 문자 제거\n",
    "    - 대소문자 변경\n",
    "- stop word(분석에 필요 없는 토큰) 제거\n",
    "- 텍스트 토큰화\n",
    "    - 분석의 최소단위로 나누는 작업\n",
    "    - 보통 단어단위나 글자단위로 나눈다.\n",
    "- 어근 추출(Stemming/Lemmatization)을 통한 텍스트 정규화 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK의 분석을 위한 클래스들\n",
    "\n",
    "## Text클래스\n",
    "- 문서 분석에 유용한 여러 메소드 제공\n",
    "- **토큰 리스트**을 입력해 객체생성 후 제공되는 메소드를 이용해 분석한다.\n",
    "- ### 생성\n",
    "    - Text(토큰리스트, [name=이름])\n",
    "- ### 주요 메소드\n",
    "    - count(단어)\n",
    "        - 매개변수로 전달한 단어의 빈도수\n",
    "    - plot(N)\n",
    "        - 빈도수 상위 N개 단어를 선그래프로 시각화\n",
    "    - dispersion_plot(단어리스트)\n",
    "        - 매개변수로 전달한 단어들이 전체 말뭉치의 어느 부분에 나오는지 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FreqDist\n",
    "- document에서 사용된 토큰(단어)의 사용 빈도수와 관련 정보를 조회할 수있는 분석 클래스.\n",
    "    - 토큰(단어)를 key, 개수를 value로 가지는 딕셔너리 형태\n",
    "- 생성\n",
    "    - Text 객체의 vocab() 메소드로 조회한다.\n",
    "    - 생성자(Initializer)에 토큰 List를 직접 넣어 생성가능\n",
    "- 주요 메소드\n",
    "    - B(): 출연한 고유 단어의 개수\n",
    "        - [Apple, Apple] -> 1\n",
    "    - N(): 총 단어수 \n",
    "        - [Apple, Apple] -> 2\n",
    "    - get(단어) 또는 FreqDist['단어'] : 특정 단어의 출연 빈도수\n",
    "    - freq(단어): 총 단어수 대비 특정단어의 출연비율\n",
    "    - most_common() : 빈도수 순서로 정렬하여 리스트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud\n",
    "\n",
    "- 텍스트의 단어의 빈도수를 시각적으로 표현하는 도구로 **문서의 주요 키워드들을 시각적으로 확인할 때 사용한다.**\n",
    "     - 빈도수가 높은 단어는 크고 굵게, 빈도수가 낮은 단어는 작고 얇게 표현한다.\n",
    "- wordcloud 패키지 사용\n",
    "     - 설치: `pip install wordcloud`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
