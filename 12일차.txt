특정 목적의 sql문을 실행하도록 하는 프로그램.
클라이언트 프로그램에 들어가는 쿼리문을 우리가 짜야함.
RDB-공통적 기능을 제공함.
python을 DB에 연동해주려면 라이브러리가 필요한데 종류가 너무 많아서 라이브러리들의 표준을 제공해줌.
구현은 각자 만들었지만 함수나 호출값은 같다.

외부자원 : DBMS
 프로그램이 외부 자원과 연동하여 외부자원에 있는 값을 읽거나 씀,
1.연결 2.r/w(읽고 씀) 3.연결 닫기
r(select) 
w(나머지)
연결을 어떻게 하고 읽고 쓰는 함수는 어떤것을 쓰고 닫는 것은 뭘 써야하는가?

-DB연결 : connect
힌트는 정의할때만 사용
호출할때는 사용하지 않음.

connection : 연결.
쿼리문을 실행하기 위해 cursor생성
cursor : 연결된 네트워크를 가지고 Database에 sql문을 전송하고 select결과 조회 기능을 제공하는 객체
-->sql작업

# sql 문 마지막에 `;` 은 붙이지 않는다. --> 한번의 하나의 명령만 수행할 수 있으므로 ;가 필요없음.

auto_increment - 자동증가

 # connection을 저장할 변수 
-con = None

인터넷 상에 연결되는 컴퓨터를 식별하기 위한 주소 = ip주소.
컴퓨터 안에있는 네트워크 서비스 해주는 프로그램에 부여되는 번호 = port번호
ex)mysql = 3306

- pymysql에서의 insert
sql문을 작성. 
문자열은 작은 따옴표로 넣어줌.

with (실행함수) as return함수 대신 나올 값
with (with-cursor를 close)-conn을 close)

연결한 후에 insert하면 영구저장이 아니라 임시적으로 저장해줌
db에 적용하려면 commit을 해주어야 최종적으로 db에 넣어줌.

- roll back -commit이 일어나기 전단계로 되돌려주는 것.
-transaction
commit은 connection을 기반으로 하므로 commection에 commit함수를 적용하여 사용하면 됨.


# Parameterized Query
SQL 문에서 컬럼 값이 들어가는 자리에 값대신 %s placeholder를 사용한뒤 execute()에서 placeholder에 넣을 값을 list나 tuple로 제공한다. -->db마다 placeholder는 달라지지만 개념은 같음.
동일한 쿼리문을 값을 바꿔가면서 여러번 실행할 때 유용하다.

#execute() --> 한번에 하나의 쿼리만 처리
#executemay() -->리스트를 넣어 처리하면 한번에 여러개의 쿼리 처리 (반복문 넣어서 execute하는 것을 대신 처리)

# fetch메소드
- fetchall()
조회한 모든 행을을 반환
- fetchmany(size=개수)
전체 조회한 행들 중 지정한 size개수 만큼 반환.
연속적으로 실행하면 다음 size개수 만큼씩 반환한다.
더 이상 조회한 결과가 없으면 빈 튜플을 반환한다.
- fetchone()
조회결과 중 첫번째 행만 반환
주로 pk 동등 조건으로 조회한 경우 사용

#조회결과를 dictionary로 반환
- pymysql.cursors.DictCursor 사용
- Connection 생성시 또는 Cursor 생성시 지정한다.

fetchmany()
순서대로 주는데 조회를 여러개

크롤링 - 여러사이트를 다니면서 필요한 정보를 긁어오는 것.
            웹상에 데이터를 가져오는것.

# 크롬개발자 도구
=엣지와 동일
: 웹개발자들이 보는 tool.
-- F12 나 팝업 메뉴에서 검사를 선택한다.
-네트워크 상황을 보거나 (어떻게 요청해서 어떻게 받아왔는지 헤더정보볼떄 사용)
-내가 가져오고자하는 정보가 있는 코드가 어디에 있는지 확인할 때 사용.(요소) = 크롤링할 문서 분석시 사용

#HTML
: markup language 중 하나.
: 언어에 ML이 붙으면 markup언어라고보면 됨.
ex)XML 

*markup 언어는 표시할 때 태그를 사용, 마킹 해주는 언어. (/사용)
이것을 이용해서 웹문서를 만들자해서 나온 언어가 HTML
=>화면을 어떤 식으로 구성할지 만드는 언어가 HTML이다.

* 원하는 것을 찾아가는 방법
xpath
select

*BeautifulSoup

request로 가져온 다음 BeautifulSoup에서 파생해서 사용.
selenium 웹브라우저가 잘 작동하는지 테스트하는 tool.

* 태그의 속성값 조회 -->태그안에 있는 내용 추출
tag객체.get('속성명')
tag객체['속성명']
ex) tag.get('href') 또는 tag['href']
* 태그내 text값 조회
tag객체.get_text()
tag객체.text
ex) tag.get_text() 또는 tag.text

* contents 속성 --> 태그 안에있는 다른 컨텐츠 조회할때 사용.
조회한 태그의 모든 자식 요소들을 리스트로 반환
ex) child_list = tag.contents


* 태그의 이름으로 조회
- find_all() -->id를 찾을 때 사용 - tag 객체로 줌,
find()
- css selector를 이용해 조회
select(), select_one()

select(selector='css셀렉터')
css 셀렉터와 일치하는 tag들을 반환한다.
select_one(selector='css셀렉터')
css 셀렉터와 일치하는 tag를 반환한다.
일치하는 것이 여러개일 경우 첫번째 것 하나만 반환한다.

<!--ul/ol: 목록태그. li: item들 지정.--> ul은 점태그 ol은 숫자태그
 <!--a: 링크, href 속성: 이동할 url을 지정.-->


	조상
	부모
형제 -자식-형제
	자손


#requests
HTTP 요청과 응답을 처리하는 파이썬 패키지(웹브라우저처럼 특정 사이트안에있는 문서를 요청하고 받아오는 것을 처리하는 라이브러리) = 웹브라우저가 하는일을 파이썬으로 대신하는 것.

* 프로토콜(protocol)- 통신규약
server - client
-- client가 server에 요청할때 정해진 규약(어떤식으로 데이터를 보내야하고 )
-- server가 응답할때 정해진 규약(어떤식으로 답을 보내야하는지)
HTTP - WEB프로토콜  : web페이지를 주고받기위한 프로토콜
client = 웹브라우저
server = 

http프로토콜에 보안추가 = https


# Crawling을 위한 requests 코딩 패턴
1. requests의 get()/post() 함수를 이용해 url을 넣어 서버 요청한다.
2. 응답받은 내용(일반적으로 HTML 페이지)을 처리
- text(HTML)은 BeautifulSoup를 이용해 원하는 내용을 추출한다.
- binary 파일(text를 제외한 모든 파일-이미지, 동영상등)의 경우 파일출력을 이용해 local에 저장.

# 요청 함수
- get(): GET방식 요청
	GET 방식(기본방식): 목적 - client가 자원을 요청하는 것 목적(달라는 것.)

- post(): POST방식 요청
	POST 방식: 목적 - client가 자기의 자원을 server로 전송하는 것이 목적.


# HTTP 요청 방식(HTTP Method)

- HTTP 프로토콜은 클라이언트가 서버에 요청하는 목적에 따라 다음과 같은 방식을 정의한다.
	-GET, POST, PUT, PATCH, DELETE, HEADER, OPTIONS, TRACE, CONECT
		- 전통적인 Web은 GET과 POST 방식 지원하는데 Restful 기반 API에서는 GET, POST, PUT, PATCH, DELETE를 이용한다.
	GET: 기본 요청방식으로 서버가 가진 데이터를 요청한다. (RETRIEVE) -->크롤링할때 많이 사용.
	POST: 클라이언트의 데이터를 서버에 전송(저장)한다. (CREATE)
	PUT: 서버가 가진 데이터를 클라이언트가 전송한 데이터로 변경한다. (UPDATE - 전체 변경)
	PATCH: 서버가 가진 데이터의 일부를 클라이언트가 전송한 데이터로 변경한다. (UPDATE - 부분 변경)
	DELETE: 서버의 데이터를 삭제한다. (DELETE)
* put, patch, delete는 잘 사용하지 않음.

* 요청할때 전달해 주는 값 --> 파라미터 (클라이언트가 서버에게 보내주는 값.)
검색 결과를 달라고 할때 넣어주는 키워드 = 파라미터

#get 방식
URL 뒤에 ?를 붙이고 그 뒤에 요청파라미터를 붙여 구성한다. (?가 url과 요청파라미터를 구분하는 구분자로 사용된다.)
- params: 요청파라미터를 dictionary로 전달
- headers: HTTP 요청 header를 dictionary로 전달
	- 'User-Agent', 'Referer' 등 헤더 설정


#HTTP 요청 헤더
HTTP 요청시 웹브라우저가 client의 여러 부가적인 정보들을 Key-Value 쌍 형식으로 전달한다.
HTTP 응답 상태코드 300번대도 크롤링하는 입장에서는 Error.











