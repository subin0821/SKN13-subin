{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXO0dld-IUqW"
   },
   "source": [
    "# 학습된 모델 저장\n",
    "\n",
    "- 학습이 완료된 모델을 파일로 저장하여, 이후 추가 학습이나 예측 서비스에 사용할 수 있도록 한다.\n",
    "- 파이토치(PyTorch)는 **모델의 파라미터만 저장**하는 방법과 **모델의 구조와 파라미터를 모두 저장**하는 두 가지 방식을 제공한다.\n",
    "- 저장 함수\n",
    "  - `torch.save(저장할 객체, 저장 경로)`\n",
    "- 보통 저장 파일의 확장자는 `.pt`나 `.pth`를 사용한다.\n",
    "\n",
    "## 모델 전체 저장 및 불러오기\n",
    "\n",
    "- 저장하기\n",
    "  - `torch.save(model, 저장 경로)`\n",
    "- 불러오기\n",
    "  - `load_model = torch.load(저장 경로)`\n",
    "- 모델 저장 시 **피클(pickle)**을 사용해 직렬화되므로, 모델을 불러오는 실행 환경에도 저장할 때 사용한 클래스 정의가 필요하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCHBya16IUqZ"
   },
   "source": [
    "## 모델의 파라미터만 저장\n",
    "\n",
    "-   모델을 구성하는 파라미터만 저장한다.\n",
    "-   모델의 구조는 저장하지 않기 때문에 불러올 때 **모델을 먼저 생성하고 생성한 모델에 불러온 파라미터를 덮어씌운다.**\n",
    "-   모델의 파라미터는 **state_dict** 형식으로 저장한다.\n",
    "\n",
    "### state_dict\n",
    "\n",
    "-   모델의 파라미터 Tensor들을 레이어 단위별로 나누어 저장한 Ordered Dictionary (OrderedDict)\n",
    "-   `모델객체.state_dict()` 메소드를 이용해 조회한다.\n",
    "-   모델의 state_dict을 조회 후 저장한다.\n",
    "    -   `torch.save(model.state_dict(), \"저장경로\")`\n",
    "-   생성된 모델에 읽어온 state_dict를 덮어씌운다.\n",
    "    -   `new_model.load_state_dict(torch.load(\"state_dict저장경로\"))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSyf-BgvIUqZ"
   },
   "source": [
    "## Checkpoint 저장 및 불러오기\n",
    "\n",
    "- 학습이 끝나지 않은 모델을 저장하고, 나중에 이어서 학습시키려면 모델의 구조와 파라미터뿐만 아니라 optimizer, loss 함수 등 학습에 필요한 객체들도 함께 저장해야 한다.\n",
    "- 딕셔너리(Dictionary)에 저장하려는 값들을 key-value 쌍으로 구성하여 `torch.save()`를 이용해 저장한다.\n",
    "\n",
    "```python\n",
    "# 저장\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': train_loss\n",
    "}, \"저장경로\")\n",
    "\n",
    "# 불러오기\n",
    "model = MyModel()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 불러온 checkpoint를 이용해 이전 학습 상태 복원\n",
    "checkpoint = torch.load(\"저장경로\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cn3fQu3FIUqa"
   },
   "outputs": [],
   "source": [
    "# 간단한 모델 정의\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(3, 4) # 3 x 4 + 4 (Weight: Input개수 * Output개수, bias: Output개수)\n",
    "        self.lr2 = nn.Linear(4, 2) # 4 x 2 + 2\n",
    "        self.relu = nn.ReLU() # activation함수->파라미터가 없는 단순 계산함수. relu(X) = max(X, 0)\n",
    "    def forward(self, X):\n",
    "        X = self.lr1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lr2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwrMvmykIUqb",
    "outputId": "a006d453-d19a-432e-9a33-8571858fa7f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 생성성\n",
    "model = MyModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vd5dPmK_IUqb",
    "outputId": "8e50b60c-ecfa-4e91-c80e-4927fbc11fea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3, out_features=4, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 레이어의 파라미터를 조회\n",
    "# 모델에서 레이어객체를 조회\n",
    "lr1 = model.lr1\n",
    "lr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqlmVXDjIUqb",
    "outputId": "5f1646e5-4ec6-4ff5-bd7a-14afe9b1c979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1182, -0.5024, -0.4231],\n",
       "        [ 0.2442,  0.5510, -0.0766],\n",
       "        [ 0.3018,  0.0804, -0.2998],\n",
       "        [-0.4359,  0.0421,  0.4237]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 레이어에서 weight/bias 조회\n",
    "w = lr1.weight\n",
    "print(w.shape)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnYs5mKWIUqc",
    "outputId": "649d51bc-ccae-4788-da68-ded7322984f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.4341,  0.1166,  0.5019,  0.5518], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = lr1.bias\n",
    "print(b.shape)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TITTVNnPIUqc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"saved_models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iri7krShIUqc"
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "#  모델을 저장\n",
    "################################################\n",
    "torch.save(model, \"saved_models/my_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "jBT66TdbIUqc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "#  저장된 모델 Load\n",
    "################################################\n",
    "load_model = torch.load(\"saved_models/my_model.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWCq1Pm-IUqc",
    "outputId": "e90d6290-090c-4cc7-b970-6c63fa1bcd07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecYp6n2JIUqd",
    "outputId": "6300d1c7-779f-42c5-a37e-ad11a57bc5dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1182, -0.5024, -0.4231],\n",
       "        [ 0.2442,  0.5510, -0.0766],\n",
       "        [ 0.3018,  0.0804, -0.2998],\n",
       "        [-0.4359,  0.0421,  0.4237]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_lr1 = load_model.lr1\n",
    "lr1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJOWsI40IUqd",
    "outputId": "fd3e535f-9b0a-4d47-cbeb-1b95d5bfd412"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.4341,  0.1166,  0.5019,  0.5518], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7I3kDAF-IUqd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skXGM1jpIUqd",
    "outputId": "81319289-3cd7-467d-cdb0-71ef4e221637"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4, out_features=2, bias=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################\n",
    "#  모델에 Layer들을 조회. 모델.instance변수명\n",
    "################################################\n",
    "lr_layer = model.lr2\n",
    "lr_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDjaushmIUqd"
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "#  Layer의 파라미터(weight/bias) 조회\n",
    "################################################\n",
    "lr1_weight = lr_layer.weight\n",
    "lr1_bias = lr_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_vb0cOMIUqd",
    "outputId": "65f28e45-58c8-456e-c222-10ca147f9e9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4164,  0.4696,  0.0233,  0.4031],\n",
       "        [ 0.3847, -0.0529,  0.0882, -0.0960]], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37AawuqAIUqd",
    "outputId": "b62e2a2e-b7fe-4b13-b736-23d2f64cf307"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.3716, -0.3975], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6rJDuAJIUqd",
    "outputId": "ddaeff0e-2c3e-48b4-f516-e437ff24a1d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr1.weight',\n",
       "              tensor([[ 0.1182, -0.5024, -0.4231],\n",
       "                      [ 0.2442,  0.5510, -0.0766],\n",
       "                      [ 0.3018,  0.0804, -0.2998],\n",
       "                      [-0.4359,  0.0421,  0.4237]])),\n",
       "             ('lr1.bias', tensor([-0.4341,  0.1166,  0.5019,  0.5518])),\n",
       "             ('lr2.weight',\n",
       "              tensor([[-0.4164,  0.4696,  0.0233,  0.4031],\n",
       "                      [ 0.3847, -0.0529,  0.0882, -0.0960]])),\n",
       "             ('lr2.bias', tensor([ 0.3716, -0.3975]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "# 모델의 파라미터들(weight들, bias들)만 저장/불러오기\n",
    "######################################################\n",
    "state_dict = model.state_dict()\n",
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vc_niVLOIUqd",
    "outputId": "1718ac01-7b98-4c97-acc5-628856213d99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['lr1.weight', 'lr1.bias', 'lr2.weight', 'lr2.bias'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kS8HR7DIUqd",
    "outputId": "b1865fb0-f87d-4db1-e4df-0a9a8e6003dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4341,  0.1166,  0.5019,  0.5518])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['lr1.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5N7lRLxDIUqd"
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# state_dict 저장\n",
    "###################\n",
    "\n",
    "torch.save(state_dict, \"saved_models/my_model_parameter.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9X2D-EvIUqe"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# state_dict load\n",
    "#####################\n",
    "sd = torch.load(\"saved_models/my_model_parameter.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXbqpJoHIUqe",
    "outputId": "839dc556-6b6c-4198-bfc6-407de41ef326"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr1.weight',\n",
       "              tensor([[ 0.4946, -0.1881, -0.2906],\n",
       "                      [ 0.3572, -0.3518, -0.0322],\n",
       "                      [ 0.4324, -0.4250,  0.1640],\n",
       "                      [ 0.1107,  0.2065,  0.5298]])),\n",
       "             ('lr1.bias', tensor([ 0.4650, -0.0652, -0.4243, -0.4428])),\n",
       "             ('lr2.weight',\n",
       "              tensor([[ 0.2751, -0.3926, -0.4087,  0.1688],\n",
       "                      [-0.1865,  0.4473,  0.0923, -0.3348]])),\n",
       "             ('lr2.bias', tensor([ 0.1144, -0.4082]))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load한 state_dict를 모델 파라미터에 적용(덮어 씌운다.)\n",
    "new_model = MyModel()\n",
    "new_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21qDPpLsIUqe",
    "outputId": "f95dd5dc-718e-4f9f-8df5-0895f11ba113"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr1.weight',\n",
       "              tensor([[ 0.1182, -0.5024, -0.4231],\n",
       "                      [ 0.2442,  0.5510, -0.0766],\n",
       "                      [ 0.3018,  0.0804, -0.2998],\n",
       "                      [-0.4359,  0.0421,  0.4237]])),\n",
       "             ('lr1.bias', tensor([-0.4341,  0.1166,  0.5019,  0.5518])),\n",
       "             ('lr2.weight',\n",
       "              tensor([[-0.4164,  0.4696,  0.0233,  0.4031],\n",
       "                      [ 0.3847, -0.0529,  0.0882, -0.0960]])),\n",
       "             ('lr2.bias', tensor([ 0.3716, -0.3975]))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOTryqE1IUqe",
    "outputId": "70bbe658-50c7-491a-974a-5ab99814f4db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvIg0GkIIUqe",
    "outputId": "4b38865b-6f9b-46a6-c14d-d2349b41e5dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr1.weight',\n",
       "              tensor([[ 0.1182, -0.5024, -0.4231],\n",
       "                      [ 0.2442,  0.5510, -0.0766],\n",
       "                      [ 0.3018,  0.0804, -0.2998],\n",
       "                      [-0.4359,  0.0421,  0.4237]])),\n",
       "             ('lr1.bias', tensor([-0.4341,  0.1166,  0.5019,  0.5518])),\n",
       "             ('lr2.weight',\n",
       "              tensor([[-0.4164,  0.4696,  0.0233,  0.4031],\n",
       "                      [ 0.3847, -0.0529,  0.0882, -0.0960]])),\n",
       "             ('lr2.bias', tensor([ 0.3716, -0.3975]))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qW5BoaBCIUqe",
    "outputId": "93d43756-7f27-4671-f227-792402217973"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UewBw5rIIUqe",
    "outputId": "30db74c8-572b-479f-98f1-f50190e8ec10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Using cached torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Using cached torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# torchinof 패키지 설치: 파이토치 모델 구조를 조사해주는 패키지.\n",
    "%pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97jH3e9bIUqe",
    "outputId": "d6ff8289-6d5c-48cd-fd28-25845b003816"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MyModel                                  --\n",
       "├─Linear: 1-1                            16\n",
       "├─Linear: 1-2                            10\n",
       "├─ReLU: 1-3                              --\n",
       "=================================================================\n",
       "Total params: 26\n",
       "Trainable params: 26\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNM8T8c4IUqe",
    "outputId": "c5f29aeb-4454-44dc-ef85-424edcae0c6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyModel                                  [100, 2]                  --\n",
       "├─Linear: 1-1                            [100, 4]                  16\n",
       "├─ReLU: 1-2                              [100, 4]                  --\n",
       "├─Linear: 1-3                            [100, 2]                  10\n",
       "==========================================================================================\n",
       "Total params: 26\n",
       "Trainable params: 26\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input data 의 shape을 지정하면 각 Layer의 output shape을 출력한다.\n",
    "summary(model, (100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yaaXEQSMIUqe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXch3QQiIUqe"
   },
   "source": [
    "# 문제 유형별 MLP 네트워크\n",
    "- 해결하려는 문제 유형에 따라 출력 Layer의 구조가 바뀐다.\n",
    "- 딥러닝 구조에서 **Feature를 추출하는 Layer 들을 Backbone** 이라고 하고 **추론하는 Layer들을 Head** 라고 한다.\n",
    "\n",
    "\n",
    "> - MLP(Multi Layer Perceptron), DNN(Deep Neural Network), ANN(Artificial Neural Network)\n",
    ">     -   Fully Connected Layer(nn.Linear)로 구성된 딥러닝 모델\n",
    ">     -   input feature들 모두에 대응하는 weight들(가중치)을 사용한다.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaWrmYrfIUqe"
   },
   "source": [
    "## Boston Housing Dataset - **Regression(회귀) 문제**\n",
    "\n",
    "보스턴 주택가격 dataset은 다음과 같은 속성을 바탕으로 해당 타운 주택 가격의 중앙값을 예측하는 문제.\n",
    "\n",
    "-   CRIM: 범죄율\n",
    "-   ZN: 25,000 평방피트당 주거지역 비율\n",
    "-   INDUS: 비소매 상업지구 비율\n",
    "-   CHAS: 찰스강에 인접해 있는지 여부(인접:1, 아니면:0)\n",
    "-   NOX: 일산화질소 농도(단위: 0.1ppm)\n",
    "-   RM: 주택당 방의 수\n",
    "-   AGE: 1940년 이전에 건설된 주택의 비율\n",
    "-   DIS: 5개의 보스턴 직업고용센터와의 거리(가중 평균)\n",
    "-   RAD: 고속도로 접근성\n",
    "-   TAX: 재산세율\n",
    "-   PTRATIO: 학생/교사 비율\n",
    "-   B: 흑인 비율\n",
    "-   LSTAT: 하위 계층 비율\n",
    "    <br><br>\n",
    "-   **Target**\n",
    "    -   MEDV: 타운의 주택가격 중앙값(단위: 1,000달러)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3235,
     "status": "ok",
     "timestamp": 1747014466599,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "5EinyA9GLRHw",
    "outputId": "c8b4bf3d-8d04-4e58-f980-7a80a9746b68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "%pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1747025471913,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "2KmvF_coIUqf"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn # Layer들을 정의한 모듈\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1747025472134,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "sxpgmucXIUqf",
    "outputId": "eb8db7e8-5799-40d5-8609-02a1280d5af9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3586,
     "status": "ok",
     "timestamp": 1747025475890,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "Kgpsnd4vQc24",
    "outputId": "84bdc560-bac7-40fc-f41d-96c10bc886d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747025475891,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "hM5s6llxQGJO",
    "outputId": "24c119ac-419e-4159-b0c7-c1ff122aeea7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset 읽어오기\n",
    "data_path = \"data/boston_hosing.csv\"\n",
    "data_path = \"/content/drive/MyDrive/09_딥러닝 기본/data/boston_hosing.csv\"\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1747025476034,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "hsuGlbCvQGGV",
    "outputId": "2841eafc-e5b5-448b-a7a8-8ccc1ad40997"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"data\",\n  \"rows\": 506,\n  \"fields\": [\n    {\n      \"column\": \"CRIM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.601545105332487,\n        \"min\": 0.00632,\n        \"max\": 88.9762,\n        \"num_unique_values\": 504,\n        \"samples\": [\n          0.09178,\n          0.05644,\n          0.10574\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ZN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.322452994515036,\n        \"min\": 0.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          25.0,\n          30.0,\n          18.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"INDUS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.8603529408975845,\n        \"min\": 0.46,\n        \"max\": 27.74,\n        \"num_unique_values\": 76,\n        \"samples\": [\n          8.14,\n          1.47,\n          1.22\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CHAS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2539940413404118,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NOX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11587767566755611,\n        \"min\": 0.385,\n        \"max\": 0.871,\n        \"num_unique_values\": 81,\n        \"samples\": [\n          0.401,\n          0.538\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7026171434153237,\n        \"min\": 3.561,\n        \"max\": 8.78,\n        \"num_unique_values\": 446,\n        \"samples\": [\n          6.849,\n          4.88\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AGE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28.148861406903638,\n        \"min\": 2.9,\n        \"max\": 100.0,\n        \"num_unique_values\": 356,\n        \"samples\": [\n          51.8,\n          33.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DIS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.1057101266276104,\n        \"min\": 1.1296,\n        \"max\": 12.1265,\n        \"num_unique_values\": 412,\n        \"samples\": [\n          2.2955,\n          4.2515\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RAD\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.707259384239377,\n        \"min\": 1.0,\n        \"max\": 24.0,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          7.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TAX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 168.53711605495926,\n        \"min\": 187.0,\n        \"max\": 711.0,\n        \"num_unique_values\": 66,\n        \"samples\": [\n          370.0,\n          666.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PTRATIO\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.164945523714446,\n        \"min\": 12.6,\n        \"max\": 22.0,\n        \"num_unique_values\": 46,\n        \"samples\": [\n          19.6,\n          15.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 91.29486438415779,\n        \"min\": 0.32,\n        \"max\": 396.9,\n        \"num_unique_values\": 357,\n        \"samples\": [\n          396.24,\n          395.11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LSTAT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.141061511348571,\n        \"min\": 1.73,\n        \"max\": 37.97,\n        \"num_unique_values\": 455,\n        \"samples\": [\n          6.15,\n          4.32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MEDV\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.19710408737982,\n        \"min\": 5.0,\n        \"max\": 50.0,\n        \"num_unique_values\": 229,\n        \"samples\": [\n          14.1,\n          22.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "data"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-6e2b2313-b8ec-4860-8f0e-3eadeb08b652\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e2b2313-b8ec-4860-8f0e-3eadeb08b652')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6e2b2313-b8ec-4860-8f0e-3eadeb08b652 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6e2b2313-b8ec-4860-8f0e-3eadeb08b652');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-e68c472d-9395-4f9a-8327-aba6320143d4\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e68c472d-9395-4f9a-8327-aba6320143d4')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-e68c472d-9395-4f9a-8327-aba6320143d4 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1747025476058,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "K9cZiyWKQGEj"
   },
   "outputs": [],
   "source": [
    "X = data.drop(columns='MEDV').values\n",
    "y = data['MEDV'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747025476064,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "qibSl9IpQGBx",
    "outputId": "53057357-1699-4394-9e85-75e57712d27e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (102, 13), (404, 1), (102, 1))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train/test set 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0\n",
    ")\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "# ((404, 13), (102, 13), (404, 1), (102, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1747025476101,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "zKKItGWMIUqf"
   },
   "outputs": [],
   "source": [
    "# Feature Scaling - Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747025476106,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "xf8blSTpdnQI",
    "outputId": "a5f1e9d8-5039-47de-c070-727569583adf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747025476110,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "XPu0tloZSfNO"
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# Dataset 생성\n",
    "# X, y가 ndarray 타입으로 메모리에 있는 경우 => TensorDataset\n",
    "###############\n",
    "trainset = TensorDataset(\n",
    "    #모델 파파미터와 타입을 동일하게 설정.\n",
    "    torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "testset = TensorDataset(\n",
    "    torch.tensor(X_test_scaled, dtype=torch.float32),\n",
    "    torch.tensor(y_test, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747025476111,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "ZmAz_lhlSfJ7",
    "outputId": "b7b8112a-e204-4c1d-f3ca-c7ce3081e9cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 102)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1747025476127,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "A-9vWNBzSfHT",
    "outputId": "487821a3-f795-405c-cac2-47f9bfa5f5f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.3726, -0.4996, -0.7049,  3.6645, -0.4249,  0.9357,  0.6937, -0.4372,\n",
       "         -0.1622, -0.5617, -0.4846,  0.3717, -0.4110]),\n",
       " tensor([26.7000]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1747025476136,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "zVChfqOGSfE6"
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# DataLoader 생성\n",
    "##################\n",
    "train_loader = DataLoader(\n",
    "    trainset, batch_size=100, shuffle=True, drop_last=True\n",
    ")\n",
    "test_loader = DataLoader(testset, batch_size=102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1747025476151,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "m00pFCCvUFJG",
    "outputId": "e431fea4-8c6c-40c4-8379-1cc26f1cc194"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epoch당 step수\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1747025476220,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "laZyk5EuSfBw"
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# 모델 정의\n",
    "###################\n",
    "class BostonModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(13, 32) # 13(in_features): X의 feature개수\n",
    "        self.lr2 = nn.Linear(32, 16)\n",
    "        self.lr3 = nn.Linear(16, 1)  # 1 (out_features): 예측할 y의 개수(집값)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # lr1 -> relu -> lr2 -> relu -> lr3\n",
    "        X = self.lr1(X)\n",
    "        X = nn.ReLU()(X)\n",
    "        X = self.lr2(X)\n",
    "        X = nn.ReLU()(X)\n",
    "        X = self.lr3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747025477441,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "5QYW-AmWSe5i",
    "outputId": "fe343d74-d2ae-4832-8824-0611c103be1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [100, 1]                  --\n",
       "├─Linear: 1-1                            [100, 32]                 448\n",
       "├─Linear: 1-2                            [100, 16]                 528\n",
       "├─Linear: 1-3                            [100, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.10\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_model = BostonModel()\n",
    "torchinfo.summary(boston_model, (100, 13))\n",
    "# (100:batch_size, 13: feature 수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747025478657,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "dcMdITMJW79V"
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# 학습\n",
    "#################\n",
    "# 필요할 객체들 생성 - model, loss함수, optimizer\n",
    "# device로 옮길 대상: model, x, y\n",
    "boston_model = BostonModel()\n",
    "boston_model = boston_model.to(device)\n",
    "loss_fn = nn.MSELoss() # 회귀의 loss함수 -> MSE\n",
    "optimizer = torch.optim.RMSprop(boston_model.parameters(), lr=0.01)\n",
    "# 옵티마이저 파라미터(모델의 파라미터, 학습률)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11670,
     "status": "ok",
     "timestamp": 1747025490819,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "EHur8ravW767",
    "outputId": "a0b582da-8924-4a3d-ead8-319d12a8a654"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000] train loss: 570.67377, valid loss: 527.22186\n",
      "[2/1000] train loss: 527.82892, valid loss: 466.85306\n",
      "[3/1000] train loss: 446.39790, valid loss: 364.62421\n",
      "[4/1000] train loss: 324.10976, valid loss: 229.58211\n",
      "[5/1000] train loss: 174.44072, valid loss: 120.73502\n",
      "[6/1000] train loss: 92.00523, valid loss: 117.88840\n",
      "[7/1000] train loss: 101.57580, valid loss: 105.06705\n",
      "[8/1000] train loss: 68.12966, valid loss: 61.09102\n",
      "[9/1000] train loss: 33.15622, valid loss: 53.09137\n",
      "[10/1000] train loss: 30.89159, valid loss: 52.40312\n",
      "[11/1000] train loss: 27.53714, valid loss: 44.63232\n",
      "[12/1000] train loss: 20.89509, valid loss: 40.67122\n",
      "[13/1000] train loss: 21.23802, valid loss: 40.23915\n",
      "[14/1000] train loss: 20.79331, valid loss: 37.13201\n",
      "[15/1000] train loss: 17.74723, valid loss: 34.23205\n",
      "[16/1000] train loss: 16.71534, valid loss: 33.16691\n",
      "[17/1000] train loss: 15.81131, valid loss: 31.48139\n",
      "[18/1000] train loss: 14.69502, valid loss: 30.22983\n",
      "[19/1000] train loss: 14.31299, valid loss: 29.18543\n",
      "[20/1000] train loss: 13.79659, valid loss: 28.37613\n",
      "[21/1000] train loss: 13.12554, valid loss: 28.02201\n",
      "[22/1000] train loss: 12.25966, valid loss: 27.63544\n",
      "[23/1000] train loss: 12.17288, valid loss: 27.53360\n",
      "[24/1000] train loss: 11.95608, valid loss: 26.88290\n",
      "[25/1000] train loss: 11.63628, valid loss: 26.19909\n",
      "[26/1000] train loss: 11.27860, valid loss: 25.93201\n",
      "[27/1000] train loss: 11.06480, valid loss: 25.38320\n",
      "[28/1000] train loss: 10.82177, valid loss: 24.91941\n",
      "[29/1000] train loss: 10.68291, valid loss: 25.18774\n",
      "[30/1000] train loss: 10.18273, valid loss: 24.49011\n",
      "[31/1000] train loss: 10.18430, valid loss: 23.69684\n",
      "[32/1000] train loss: 10.00908, valid loss: 23.63013\n",
      "[33/1000] train loss: 9.93705, valid loss: 23.88979\n",
      "[34/1000] train loss: 9.77531, valid loss: 23.24334\n",
      "[35/1000] train loss: 9.41176, valid loss: 23.49957\n",
      "[36/1000] train loss: 9.41122, valid loss: 23.59568\n",
      "[37/1000] train loss: 9.33058, valid loss: 23.18152\n",
      "[38/1000] train loss: 9.15763, valid loss: 22.79086\n",
      "[39/1000] train loss: 9.11899, valid loss: 22.44191\n",
      "[40/1000] train loss: 9.01968, valid loss: 22.66363\n",
      "[41/1000] train loss: 8.91945, valid loss: 22.72789\n",
      "[42/1000] train loss: 8.84507, valid loss: 22.42966\n",
      "[43/1000] train loss: 8.72444, valid loss: 22.31432\n",
      "[44/1000] train loss: 8.64935, valid loss: 22.21880\n",
      "[45/1000] train loss: 8.55926, valid loss: 22.40339\n",
      "[46/1000] train loss: 8.51619, valid loss: 21.92175\n",
      "[47/1000] train loss: 8.42081, valid loss: 21.86151\n",
      "[48/1000] train loss: 8.40302, valid loss: 22.11265\n",
      "[49/1000] train loss: 8.37185, valid loss: 21.88339\n",
      "[50/1000] train loss: 8.30359, valid loss: 21.96595\n",
      "[51/1000] train loss: 8.21845, valid loss: 21.86897\n",
      "[52/1000] train loss: 8.09887, valid loss: 21.72381\n",
      "[53/1000] train loss: 8.11138, valid loss: 21.59900\n",
      "[54/1000] train loss: 8.07378, valid loss: 21.54251\n",
      "[55/1000] train loss: 8.06364, valid loss: 22.02523\n",
      "[56/1000] train loss: 7.96085, valid loss: 21.37426\n",
      "[57/1000] train loss: 7.95917, valid loss: 21.73111\n",
      "[58/1000] train loss: 7.81401, valid loss: 21.65186\n",
      "[59/1000] train loss: 7.83252, valid loss: 21.08772\n",
      "[60/1000] train loss: 7.78198, valid loss: 21.34284\n",
      "[61/1000] train loss: 7.70116, valid loss: 21.00731\n",
      "[62/1000] train loss: 7.70010, valid loss: 21.19931\n",
      "[63/1000] train loss: 7.39235, valid loss: 21.56819\n",
      "[64/1000] train loss: 7.51293, valid loss: 21.27479\n",
      "[65/1000] train loss: 7.53411, valid loss: 20.75013\n",
      "[66/1000] train loss: 7.49364, valid loss: 21.04097\n",
      "[67/1000] train loss: 7.42122, valid loss: 21.57292\n",
      "[68/1000] train loss: 7.35934, valid loss: 20.86779\n",
      "[69/1000] train loss: 7.24111, valid loss: 20.70725\n",
      "[70/1000] train loss: 7.28948, valid loss: 21.17001\n",
      "[71/1000] train loss: 7.22535, valid loss: 20.67192\n",
      "[72/1000] train loss: 7.17497, valid loss: 20.75523\n",
      "[73/1000] train loss: 7.16068, valid loss: 20.90227\n",
      "[74/1000] train loss: 7.03095, valid loss: 20.55744\n",
      "[75/1000] train loss: 7.09344, valid loss: 20.62252\n",
      "[76/1000] train loss: 6.98187, valid loss: 20.64700\n",
      "[77/1000] train loss: 6.94836, valid loss: 20.54044\n",
      "[78/1000] train loss: 6.97764, valid loss: 20.53794\n",
      "[79/1000] train loss: 6.92309, valid loss: 20.34422\n",
      "[80/1000] train loss: 6.92441, valid loss: 20.29347\n",
      "[81/1000] train loss: 6.83601, valid loss: 20.41989\n",
      "[82/1000] train loss: 6.64432, valid loss: 20.38255\n",
      "[83/1000] train loss: 6.79690, valid loss: 20.37789\n",
      "[84/1000] train loss: 6.77174, valid loss: 20.30581\n",
      "[85/1000] train loss: 6.71711, valid loss: 20.37778\n",
      "[86/1000] train loss: 6.78759, valid loss: 20.80238\n",
      "[87/1000] train loss: 6.71336, valid loss: 19.79162\n",
      "[88/1000] train loss: 6.56023, valid loss: 19.97591\n",
      "[89/1000] train loss: 6.66023, valid loss: 20.12013\n",
      "[90/1000] train loss: 6.55570, valid loss: 20.11856\n",
      "[91/1000] train loss: 6.45000, valid loss: 19.90669\n",
      "[92/1000] train loss: 6.51729, valid loss: 19.69875\n",
      "[93/1000] train loss: 6.50531, valid loss: 19.41977\n",
      "[94/1000] train loss: 6.39393, valid loss: 20.26390\n",
      "[95/1000] train loss: 6.46869, valid loss: 20.02420\n",
      "[96/1000] train loss: 6.39069, valid loss: 19.68495\n",
      "[97/1000] train loss: 6.39946, valid loss: 19.33427\n",
      "[98/1000] train loss: 6.33378, valid loss: 19.96689\n",
      "[99/1000] train loss: 6.41864, valid loss: 20.23787\n",
      "[100/1000] train loss: 6.35794, valid loss: 18.91971\n",
      "[101/1000] train loss: 6.38200, valid loss: 19.28859\n",
      "[102/1000] train loss: 6.22015, valid loss: 20.40855\n",
      "[103/1000] train loss: 5.84902, valid loss: 19.45811\n",
      "[104/1000] train loss: 6.21478, valid loss: 19.01735\n",
      "[105/1000] train loss: 6.32399, valid loss: 19.35446\n",
      "[106/1000] train loss: 6.15298, valid loss: 19.68836\n",
      "[107/1000] train loss: 6.26226, valid loss: 19.77923\n",
      "[108/1000] train loss: 6.17630, valid loss: 19.01026\n",
      "[109/1000] train loss: 6.09156, valid loss: 19.07200\n",
      "[110/1000] train loss: 6.09172, valid loss: 19.62048\n",
      "[111/1000] train loss: 5.97912, valid loss: 19.14145\n",
      "[112/1000] train loss: 6.01390, valid loss: 19.01492\n",
      "[113/1000] train loss: 5.95600, valid loss: 19.28012\n",
      "[114/1000] train loss: 5.95509, valid loss: 19.33977\n",
      "[115/1000] train loss: 5.92495, valid loss: 19.23442\n",
      "[116/1000] train loss: 5.88834, valid loss: 19.25409\n",
      "[117/1000] train loss: 5.86634, valid loss: 19.18853\n",
      "[118/1000] train loss: 5.84675, valid loss: 19.36290\n",
      "[119/1000] train loss: 5.93853, valid loss: 19.15080\n",
      "[120/1000] train loss: 5.74688, valid loss: 19.07087\n",
      "[121/1000] train loss: 5.82375, valid loss: 19.22532\n",
      "[122/1000] train loss: 5.78662, valid loss: 18.94512\n",
      "[123/1000] train loss: 5.71049, valid loss: 18.80653\n",
      "[124/1000] train loss: 5.61970, valid loss: 18.68371\n",
      "[125/1000] train loss: 5.61274, valid loss: 19.40558\n",
      "[126/1000] train loss: 5.66288, valid loss: 19.19277\n",
      "[127/1000] train loss: 5.74586, valid loss: 18.47552\n",
      "[128/1000] train loss: 5.75010, valid loss: 19.34055\n",
      "[129/1000] train loss: 5.69288, valid loss: 18.74289\n",
      "[130/1000] train loss: 5.45500, valid loss: 18.62788\n",
      "[131/1000] train loss: 5.67631, valid loss: 19.04922\n",
      "[132/1000] train loss: 5.56750, valid loss: 18.87496\n",
      "[133/1000] train loss: 5.59219, valid loss: 18.87558\n",
      "[134/1000] train loss: 5.60664, valid loss: 18.68522\n",
      "[135/1000] train loss: 5.22741, valid loss: 19.15748\n",
      "[136/1000] train loss: 5.17222, valid loss: 18.68902\n",
      "[137/1000] train loss: 5.56333, valid loss: 18.49716\n",
      "[138/1000] train loss: 5.54774, valid loss: 18.48763\n",
      "[139/1000] train loss: 5.43374, valid loss: 18.70086\n",
      "[140/1000] train loss: 5.49918, valid loss: 19.00934\n",
      "[141/1000] train loss: 5.44868, valid loss: 18.78103\n",
      "[142/1000] train loss: 5.40858, valid loss: 18.65336\n",
      "[143/1000] train loss: 5.43223, valid loss: 18.97874\n",
      "[144/1000] train loss: 5.30542, valid loss: 18.40781\n",
      "[145/1000] train loss: 5.44614, valid loss: 18.82153\n",
      "[146/1000] train loss: 5.45625, valid loss: 19.18586\n",
      "[147/1000] train loss: 5.49454, valid loss: 18.35373\n",
      "[148/1000] train loss: 5.38126, valid loss: 19.03173\n",
      "[149/1000] train loss: 5.36231, valid loss: 18.70044\n",
      "[150/1000] train loss: 5.37514, valid loss: 18.49053\n",
      "[151/1000] train loss: 5.44080, valid loss: 19.06839\n",
      "[152/1000] train loss: 5.30478, valid loss: 18.27069\n",
      "[153/1000] train loss: 5.31248, valid loss: 18.72879\n",
      "[154/1000] train loss: 5.16768, valid loss: 18.37164\n",
      "[155/1000] train loss: 5.25120, valid loss: 18.37023\n",
      "[156/1000] train loss: 5.32274, valid loss: 19.17113\n",
      "[157/1000] train loss: 5.24315, valid loss: 18.49892\n",
      "[158/1000] train loss: 4.94539, valid loss: 18.21224\n",
      "[159/1000] train loss: 5.29726, valid loss: 18.26083\n",
      "[160/1000] train loss: 5.23238, valid loss: 18.56283\n",
      "[161/1000] train loss: 5.23281, valid loss: 18.33454\n",
      "[162/1000] train loss: 5.27679, valid loss: 18.94881\n",
      "[163/1000] train loss: 5.06270, valid loss: 18.47708\n",
      "[164/1000] train loss: 5.32606, valid loss: 17.98252\n",
      "[165/1000] train loss: 5.21806, valid loss: 18.84683\n",
      "[166/1000] train loss: 5.00027, valid loss: 18.90135\n",
      "[167/1000] train loss: 5.06553, valid loss: 18.50862\n",
      "[168/1000] train loss: 5.16025, valid loss: 18.51107\n",
      "[169/1000] train loss: 5.12384, valid loss: 18.79516\n",
      "[170/1000] train loss: 5.07940, valid loss: 18.39693\n",
      "[171/1000] train loss: 5.06409, valid loss: 18.31702\n",
      "[172/1000] train loss: 5.05485, valid loss: 18.55438\n",
      "[173/1000] train loss: 5.07256, valid loss: 18.55919\n",
      "[174/1000] train loss: 5.14893, valid loss: 18.85542\n",
      "[175/1000] train loss: 4.93887, valid loss: 18.61790\n",
      "[176/1000] train loss: 5.01791, valid loss: 18.45339\n",
      "[177/1000] train loss: 4.82732, valid loss: 18.63865\n",
      "[178/1000] train loss: 5.01765, valid loss: 18.97179\n",
      "[179/1000] train loss: 4.97466, valid loss: 18.17832\n",
      "[180/1000] train loss: 4.92497, valid loss: 19.30564\n",
      "[181/1000] train loss: 4.98382, valid loss: 18.72369\n",
      "[182/1000] train loss: 4.90981, valid loss: 18.44523\n",
      "[183/1000] train loss: 4.97895, valid loss: 18.70983\n",
      "[184/1000] train loss: 4.93579, valid loss: 18.84377\n",
      "[185/1000] train loss: 4.89444, valid loss: 18.70114\n",
      "[186/1000] train loss: 4.89184, valid loss: 18.76800\n",
      "[187/1000] train loss: 4.87832, valid loss: 19.04279\n",
      "[188/1000] train loss: 4.82895, valid loss: 18.91918\n",
      "[189/1000] train loss: 4.78907, valid loss: 18.98696\n",
      "[190/1000] train loss: 4.82937, valid loss: 18.61706\n",
      "[191/1000] train loss: 4.86687, valid loss: 18.91078\n",
      "[192/1000] train loss: 4.87750, valid loss: 19.11390\n",
      "[193/1000] train loss: 4.89448, valid loss: 18.89269\n",
      "[194/1000] train loss: 4.61617, valid loss: 19.34925\n",
      "[195/1000] train loss: 4.89340, valid loss: 18.75805\n",
      "[196/1000] train loss: 4.81171, valid loss: 19.48030\n",
      "[197/1000] train loss: 4.77039, valid loss: 19.03787\n",
      "[198/1000] train loss: 4.72789, valid loss: 18.49367\n",
      "[199/1000] train loss: 4.73670, valid loss: 19.23803\n",
      "[200/1000] train loss: 4.96921, valid loss: 19.80258\n",
      "[201/1000] train loss: 4.85298, valid loss: 18.20944\n",
      "[202/1000] train loss: 4.71439, valid loss: 19.73405\n",
      "[203/1000] train loss: 4.81347, valid loss: 18.85492\n",
      "[204/1000] train loss: 4.71031, valid loss: 18.64313\n",
      "[205/1000] train loss: 4.65679, valid loss: 19.38157\n",
      "[206/1000] train loss: 4.70927, valid loss: 18.41779\n",
      "[207/1000] train loss: 4.62865, valid loss: 18.93353\n",
      "[208/1000] train loss: 4.64620, valid loss: 19.25614\n",
      "[209/1000] train loss: 4.64373, valid loss: 18.66048\n",
      "[210/1000] train loss: 4.55554, valid loss: 19.26657\n",
      "[211/1000] train loss: 4.55900, valid loss: 18.74396\n",
      "[212/1000] train loss: 4.56554, valid loss: 18.87076\n",
      "[213/1000] train loss: 4.60605, valid loss: 18.83990\n",
      "[214/1000] train loss: 4.53835, valid loss: 18.95646\n",
      "[215/1000] train loss: 4.52625, valid loss: 18.49660\n",
      "[216/1000] train loss: 4.52614, valid loss: 18.66926\n",
      "[217/1000] train loss: 4.48249, valid loss: 18.87654\n",
      "[218/1000] train loss: 4.46547, valid loss: 19.06270\n",
      "[219/1000] train loss: 4.52785, valid loss: 18.82840\n",
      "[220/1000] train loss: 4.45374, valid loss: 18.82676\n",
      "[221/1000] train loss: 4.48192, valid loss: 18.98773\n",
      "[222/1000] train loss: 4.22043, valid loss: 18.45154\n",
      "[223/1000] train loss: 4.36164, valid loss: 19.29405\n",
      "[224/1000] train loss: 4.44319, valid loss: 19.22254\n",
      "[225/1000] train loss: 4.40263, valid loss: 18.94123\n",
      "[226/1000] train loss: 4.42086, valid loss: 18.93965\n",
      "[227/1000] train loss: 4.41233, valid loss: 19.58436\n",
      "[228/1000] train loss: 4.34139, valid loss: 18.85154\n",
      "[229/1000] train loss: 4.37530, valid loss: 18.98962\n",
      "[230/1000] train loss: 4.34977, valid loss: 19.17457\n",
      "[231/1000] train loss: 4.42129, valid loss: 19.14635\n",
      "[232/1000] train loss: 4.24318, valid loss: 19.33301\n",
      "[233/1000] train loss: 4.34462, valid loss: 18.93544\n",
      "[234/1000] train loss: 4.38237, valid loss: 19.88306\n",
      "[235/1000] train loss: 4.26726, valid loss: 19.06888\n",
      "[236/1000] train loss: 4.31200, valid loss: 19.40126\n",
      "[237/1000] train loss: 4.36223, valid loss: 20.08036\n",
      "[238/1000] train loss: 4.24291, valid loss: 18.93509\n",
      "[239/1000] train loss: 4.23726, valid loss: 19.68836\n",
      "[240/1000] train loss: 4.24809, valid loss: 19.40311\n",
      "[241/1000] train loss: 4.32649, valid loss: 19.77576\n",
      "[242/1000] train loss: 4.14369, valid loss: 19.07010\n",
      "[243/1000] train loss: 4.26646, valid loss: 19.50595\n",
      "[244/1000] train loss: 4.14571, valid loss: 19.80671\n",
      "[245/1000] train loss: 4.10687, valid loss: 19.38420\n",
      "[246/1000] train loss: 4.18336, valid loss: 19.85305\n",
      "[247/1000] train loss: 4.16229, valid loss: 19.18952\n",
      "[248/1000] train loss: 4.08712, valid loss: 19.83176\n",
      "[249/1000] train loss: 4.09249, valid loss: 19.47364\n",
      "[250/1000] train loss: 4.13610, valid loss: 19.56345\n",
      "[251/1000] train loss: 4.11222, valid loss: 20.07714\n",
      "[252/1000] train loss: 4.08150, valid loss: 19.58002\n",
      "[253/1000] train loss: 4.14919, valid loss: 20.14391\n",
      "[254/1000] train loss: 4.09831, valid loss: 19.51339\n",
      "[255/1000] train loss: 4.08912, valid loss: 19.93640\n",
      "[256/1000] train loss: 4.11669, valid loss: 19.85322\n",
      "[257/1000] train loss: 4.09199, valid loss: 19.51794\n",
      "[258/1000] train loss: 4.00346, valid loss: 20.65717\n",
      "[259/1000] train loss: 4.09671, valid loss: 19.98944\n",
      "[260/1000] train loss: 4.06998, valid loss: 19.76183\n",
      "[261/1000] train loss: 4.05881, valid loss: 20.54678\n",
      "[262/1000] train loss: 3.94379, valid loss: 19.64851\n",
      "[263/1000] train loss: 4.04493, valid loss: 20.31418\n",
      "[264/1000] train loss: 4.07894, valid loss: 19.68885\n",
      "[265/1000] train loss: 3.98420, valid loss: 20.28117\n",
      "[266/1000] train loss: 3.94157, valid loss: 20.30151\n",
      "[267/1000] train loss: 4.00076, valid loss: 19.86642\n",
      "[268/1000] train loss: 3.98263, valid loss: 20.41292\n",
      "[269/1000] train loss: 3.87420, valid loss: 20.22636\n",
      "[270/1000] train loss: 3.93895, valid loss: 20.35007\n",
      "[271/1000] train loss: 3.95429, valid loss: 19.94579\n",
      "[272/1000] train loss: 3.93218, valid loss: 20.93936\n",
      "[273/1000] train loss: 3.88168, valid loss: 19.99619\n",
      "[274/1000] train loss: 3.94908, valid loss: 20.46928\n",
      "[275/1000] train loss: 3.81795, valid loss: 20.03154\n",
      "[276/1000] train loss: 3.81719, valid loss: 20.50562\n",
      "[277/1000] train loss: 3.78281, valid loss: 20.10611\n",
      "[278/1000] train loss: 3.81309, valid loss: 20.32504\n",
      "[279/1000] train loss: 3.82901, valid loss: 20.49146\n",
      "[280/1000] train loss: 3.83864, valid loss: 20.22199\n",
      "[281/1000] train loss: 3.79326, valid loss: 20.19871\n",
      "[282/1000] train loss: 3.80752, valid loss: 20.87749\n",
      "[283/1000] train loss: 3.84714, valid loss: 20.37028\n",
      "[284/1000] train loss: 3.85393, valid loss: 20.82851\n",
      "[285/1000] train loss: 3.76696, valid loss: 20.32371\n",
      "[286/1000] train loss: 3.77796, valid loss: 20.38538\n",
      "[287/1000] train loss: 3.88027, valid loss: 20.62607\n",
      "[288/1000] train loss: 3.93763, valid loss: 20.49372\n",
      "[289/1000] train loss: 3.83786, valid loss: 20.53622\n",
      "[290/1000] train loss: 3.87988, valid loss: 20.93661\n",
      "[291/1000] train loss: 3.86945, valid loss: 20.33765\n",
      "[292/1000] train loss: 3.77960, valid loss: 20.73956\n",
      "[293/1000] train loss: 3.56371, valid loss: 20.62659\n",
      "[294/1000] train loss: 3.69422, valid loss: 21.05543\n",
      "[295/1000] train loss: 3.73217, valid loss: 20.65983\n",
      "[296/1000] train loss: 3.70648, valid loss: 20.75043\n",
      "[297/1000] train loss: 3.74118, valid loss: 20.61310\n",
      "[298/1000] train loss: 3.65519, valid loss: 20.72081\n",
      "[299/1000] train loss: 3.69280, valid loss: 20.90905\n",
      "[300/1000] train loss: 3.63022, valid loss: 20.89741\n",
      "[301/1000] train loss: 3.71215, valid loss: 21.01600\n",
      "[302/1000] train loss: 3.68893, valid loss: 20.71662\n",
      "[303/1000] train loss: 3.66576, valid loss: 20.88046\n",
      "[304/1000] train loss: 3.64089, valid loss: 20.54458\n",
      "[305/1000] train loss: 3.66693, valid loss: 20.83331\n",
      "[306/1000] train loss: 3.60837, valid loss: 20.54029\n",
      "[307/1000] train loss: 3.68217, valid loss: 21.07445\n",
      "[308/1000] train loss: 3.65219, valid loss: 20.72067\n",
      "[309/1000] train loss: 3.64279, valid loss: 21.31379\n",
      "[310/1000] train loss: 3.76358, valid loss: 20.54354\n",
      "[311/1000] train loss: 3.57597, valid loss: 21.45302\n",
      "[312/1000] train loss: 3.68607, valid loss: 20.74775\n",
      "[313/1000] train loss: 3.60445, valid loss: 21.03574\n",
      "[314/1000] train loss: 3.66890, valid loss: 20.86804\n",
      "[315/1000] train loss: 3.88213, valid loss: 21.39977\n",
      "[316/1000] train loss: 3.59436, valid loss: 20.88732\n",
      "[317/1000] train loss: 3.55265, valid loss: 20.84577\n",
      "[318/1000] train loss: 3.65853, valid loss: 21.52415\n",
      "[319/1000] train loss: 3.60463, valid loss: 20.49102\n",
      "[320/1000] train loss: 3.53081, valid loss: 21.17156\n",
      "[321/1000] train loss: 3.56003, valid loss: 20.83403\n",
      "[322/1000] train loss: 3.53875, valid loss: 21.22469\n",
      "[323/1000] train loss: 3.52720, valid loss: 20.84172\n",
      "[324/1000] train loss: 3.58754, valid loss: 21.00405\n",
      "[325/1000] train loss: 3.59253, valid loss: 20.86175\n",
      "[326/1000] train loss: 3.63701, valid loss: 20.63836\n",
      "[327/1000] train loss: 3.61184, valid loss: 20.92237\n",
      "[328/1000] train loss: 3.46674, valid loss: 20.69478\n",
      "[329/1000] train loss: 3.51676, valid loss: 20.88660\n",
      "[330/1000] train loss: 3.51218, valid loss: 21.04026\n",
      "[331/1000] train loss: 3.41626, valid loss: 20.72538\n",
      "[332/1000] train loss: 3.50422, valid loss: 21.06258\n",
      "[333/1000] train loss: 3.54858, valid loss: 20.63542\n",
      "[334/1000] train loss: 3.48653, valid loss: 21.16406\n",
      "[335/1000] train loss: 3.48310, valid loss: 21.00141\n",
      "[336/1000] train loss: 3.46877, valid loss: 20.68731\n",
      "[337/1000] train loss: 3.40269, valid loss: 20.59562\n",
      "[338/1000] train loss: 3.52682, valid loss: 20.65552\n",
      "[339/1000] train loss: 3.52603, valid loss: 20.37299\n",
      "[340/1000] train loss: 3.53132, valid loss: 21.73316\n",
      "[341/1000] train loss: 3.50838, valid loss: 20.22895\n",
      "[342/1000] train loss: 3.56452, valid loss: 20.85486\n",
      "[343/1000] train loss: 3.53514, valid loss: 20.48213\n",
      "[344/1000] train loss: 3.50306, valid loss: 20.82123\n",
      "[345/1000] train loss: 3.54350, valid loss: 20.86858\n",
      "[346/1000] train loss: 3.56191, valid loss: 20.88862\n",
      "[347/1000] train loss: 3.45118, valid loss: 20.81476\n",
      "[348/1000] train loss: 3.38242, valid loss: 20.34482\n",
      "[349/1000] train loss: 3.52684, valid loss: 21.10720\n",
      "[350/1000] train loss: 3.51727, valid loss: 20.24573\n",
      "[351/1000] train loss: 3.55353, valid loss: 20.82520\n",
      "[352/1000] train loss: 3.49981, valid loss: 20.96825\n",
      "[353/1000] train loss: 3.66020, valid loss: 21.30065\n",
      "[354/1000] train loss: 3.62125, valid loss: 20.77414\n",
      "[355/1000] train loss: 3.79255, valid loss: 20.78797\n",
      "[356/1000] train loss: 3.79914, valid loss: 20.94855\n",
      "[357/1000] train loss: 3.90771, valid loss: 20.61045\n",
      "[358/1000] train loss: 3.68224, valid loss: 21.65259\n",
      "[359/1000] train loss: 3.51525, valid loss: 20.02946\n",
      "[360/1000] train loss: 3.55078, valid loss: 21.66681\n",
      "[361/1000] train loss: 3.55294, valid loss: 20.22102\n",
      "[362/1000] train loss: 3.50561, valid loss: 21.44756\n",
      "[363/1000] train loss: 3.44519, valid loss: 20.65273\n",
      "[364/1000] train loss: 3.46539, valid loss: 21.21378\n",
      "[365/1000] train loss: 3.42407, valid loss: 20.34134\n",
      "[366/1000] train loss: 3.30685, valid loss: 21.20396\n",
      "[367/1000] train loss: 3.35195, valid loss: 21.14786\n",
      "[368/1000] train loss: 3.31670, valid loss: 20.68453\n",
      "[369/1000] train loss: 3.35879, valid loss: 21.02060\n",
      "[370/1000] train loss: 3.29984, valid loss: 20.82542\n",
      "[371/1000] train loss: 3.45011, valid loss: 21.41704\n",
      "[372/1000] train loss: 3.33480, valid loss: 21.13308\n",
      "[373/1000] train loss: 3.28614, valid loss: 21.03065\n",
      "[374/1000] train loss: 3.30682, valid loss: 20.84802\n",
      "[375/1000] train loss: 3.33076, valid loss: 20.74594\n",
      "[376/1000] train loss: 3.39912, valid loss: 20.82027\n",
      "[377/1000] train loss: 3.35629, valid loss: 20.96336\n",
      "[378/1000] train loss: 3.33634, valid loss: 21.24808\n",
      "[379/1000] train loss: 3.46512, valid loss: 20.67167\n",
      "[380/1000] train loss: 3.26589, valid loss: 21.78580\n",
      "[381/1000] train loss: 3.33046, valid loss: 20.66202\n",
      "[382/1000] train loss: 3.32041, valid loss: 21.06558\n",
      "[383/1000] train loss: 3.29194, valid loss: 20.83925\n",
      "[384/1000] train loss: 3.35630, valid loss: 20.76470\n",
      "[385/1000] train loss: 3.30389, valid loss: 21.00271\n",
      "[386/1000] train loss: 3.32221, valid loss: 20.65386\n",
      "[387/1000] train loss: 3.45158, valid loss: 21.25126\n",
      "[388/1000] train loss: 3.41466, valid loss: 20.83620\n",
      "[389/1000] train loss: 3.35287, valid loss: 21.47818\n",
      "[390/1000] train loss: 3.31122, valid loss: 20.62856\n",
      "[391/1000] train loss: 3.24950, valid loss: 20.76471\n",
      "[392/1000] train loss: 3.25627, valid loss: 20.48907\n",
      "[393/1000] train loss: 3.24569, valid loss: 21.16715\n",
      "[394/1000] train loss: 3.29288, valid loss: 20.95134\n",
      "[395/1000] train loss: 3.21373, valid loss: 21.07279\n",
      "[396/1000] train loss: 3.22199, valid loss: 20.76047\n",
      "[397/1000] train loss: 3.24040, valid loss: 21.09180\n",
      "[398/1000] train loss: 3.30851, valid loss: 20.95231\n",
      "[399/1000] train loss: 3.27686, valid loss: 21.14158\n",
      "[400/1000] train loss: 3.25023, valid loss: 20.67671\n",
      "[401/1000] train loss: 3.30739, valid loss: 21.26224\n",
      "[402/1000] train loss: 3.31138, valid loss: 20.80598\n",
      "[403/1000] train loss: 3.19818, valid loss: 21.17123\n",
      "[404/1000] train loss: 3.20911, valid loss: 21.19560\n",
      "[405/1000] train loss: 3.24357, valid loss: 21.11811\n",
      "[406/1000] train loss: 3.30372, valid loss: 20.36624\n",
      "[407/1000] train loss: 3.22827, valid loss: 21.72324\n",
      "[408/1000] train loss: 3.28411, valid loss: 21.03695\n",
      "[409/1000] train loss: 3.14832, valid loss: 21.19277\n",
      "[410/1000] train loss: 3.24628, valid loss: 20.58107\n",
      "[411/1000] train loss: 3.14554, valid loss: 21.36246\n",
      "[412/1000] train loss: 3.26987, valid loss: 20.39307\n",
      "[413/1000] train loss: 3.33750, valid loss: 22.10026\n",
      "[414/1000] train loss: 3.34547, valid loss: 20.21602\n",
      "[415/1000] train loss: 3.37037, valid loss: 21.42484\n",
      "[416/1000] train loss: 3.33553, valid loss: 20.73647\n",
      "[417/1000] train loss: 3.41677, valid loss: 21.45125\n",
      "[418/1000] train loss: 3.26178, valid loss: 20.61203\n",
      "[419/1000] train loss: 3.11709, valid loss: 21.40158\n",
      "[420/1000] train loss: 3.15928, valid loss: 20.68817\n",
      "[421/1000] train loss: 3.01510, valid loss: 21.45521\n",
      "[422/1000] train loss: 3.12938, valid loss: 20.76862\n",
      "[423/1000] train loss: 3.17614, valid loss: 20.70910\n",
      "[424/1000] train loss: 3.19003, valid loss: 21.06325\n",
      "[425/1000] train loss: 3.18657, valid loss: 20.67872\n",
      "[426/1000] train loss: 3.14042, valid loss: 20.87194\n",
      "[427/1000] train loss: 3.15835, valid loss: 20.53046\n",
      "[428/1000] train loss: 3.12996, valid loss: 21.19955\n",
      "[429/1000] train loss: 3.15390, valid loss: 20.47972\n",
      "[430/1000] train loss: 3.14197, valid loss: 20.05190\n",
      "[431/1000] train loss: 3.12439, valid loss: 20.87471\n",
      "[432/1000] train loss: 3.06504, valid loss: 20.41351\n",
      "[433/1000] train loss: 3.07084, valid loss: 20.49640\n",
      "[434/1000] train loss: 3.05752, valid loss: 20.58664\n",
      "[435/1000] train loss: 3.12303, valid loss: 20.72945\n",
      "[436/1000] train loss: 3.06187, valid loss: 20.29866\n",
      "[437/1000] train loss: 3.00290, valid loss: 21.00904\n",
      "[438/1000] train loss: 3.14037, valid loss: 20.59272\n",
      "[439/1000] train loss: 3.08709, valid loss: 20.33738\n",
      "[440/1000] train loss: 3.07878, valid loss: 20.32195\n",
      "[441/1000] train loss: 3.08816, valid loss: 20.81110\n",
      "[442/1000] train loss: 3.01883, valid loss: 20.56481\n",
      "[443/1000] train loss: 3.05691, valid loss: 20.51895\n",
      "[444/1000] train loss: 3.11131, valid loss: 20.54447\n",
      "[445/1000] train loss: 3.10104, valid loss: 19.98143\n",
      "[446/1000] train loss: 3.02930, valid loss: 20.53103\n",
      "[447/1000] train loss: 3.01037, valid loss: 19.81643\n",
      "[448/1000] train loss: 3.18101, valid loss: 20.98501\n",
      "[449/1000] train loss: 3.14733, valid loss: 20.43841\n",
      "[450/1000] train loss: 3.00044, valid loss: 20.54715\n",
      "[451/1000] train loss: 3.03966, valid loss: 20.56754\n",
      "[452/1000] train loss: 3.15228, valid loss: 20.51968\n",
      "[453/1000] train loss: 2.95364, valid loss: 20.61883\n",
      "[454/1000] train loss: 3.02146, valid loss: 20.47619\n",
      "[455/1000] train loss: 3.02641, valid loss: 20.18426\n",
      "[456/1000] train loss: 2.99722, valid loss: 20.68792\n",
      "[457/1000] train loss: 3.01749, valid loss: 20.53866\n",
      "[458/1000] train loss: 2.96918, valid loss: 20.38725\n",
      "[459/1000] train loss: 3.02460, valid loss: 20.41341\n",
      "[460/1000] train loss: 3.02450, valid loss: 19.73471\n",
      "[461/1000] train loss: 2.90392, valid loss: 20.53707\n",
      "[462/1000] train loss: 2.94571, valid loss: 20.14232\n",
      "[463/1000] train loss: 2.82509, valid loss: 20.13401\n",
      "[464/1000] train loss: 2.91868, valid loss: 19.92019\n",
      "[465/1000] train loss: 2.97071, valid loss: 20.15488\n",
      "[466/1000] train loss: 3.02010, valid loss: 20.41049\n",
      "[467/1000] train loss: 2.98738, valid loss: 19.81005\n",
      "[468/1000] train loss: 2.88451, valid loss: 20.83475\n",
      "[469/1000] train loss: 3.02014, valid loss: 19.79081\n",
      "[470/1000] train loss: 3.12894, valid loss: 21.00276\n",
      "[471/1000] train loss: 3.12630, valid loss: 19.77512\n",
      "[472/1000] train loss: 2.96450, valid loss: 20.58827\n",
      "[473/1000] train loss: 2.98748, valid loss: 20.00557\n",
      "[474/1000] train loss: 2.86056, valid loss: 20.59559\n",
      "[475/1000] train loss: 2.90828, valid loss: 20.27454\n",
      "[476/1000] train loss: 2.80506, valid loss: 20.21754\n",
      "[477/1000] train loss: 2.85878, valid loss: 19.73726\n",
      "[478/1000] train loss: 2.89421, valid loss: 20.44692\n",
      "[479/1000] train loss: 2.88660, valid loss: 19.79300\n",
      "[480/1000] train loss: 2.91646, valid loss: 20.30423\n",
      "[481/1000] train loss: 2.90328, valid loss: 20.30062\n",
      "[482/1000] train loss: 2.82846, valid loss: 20.15604\n",
      "[483/1000] train loss: 2.81139, valid loss: 20.17554\n",
      "[484/1000] train loss: 2.83079, valid loss: 20.38069\n",
      "[485/1000] train loss: 2.82624, valid loss: 20.08066\n",
      "[486/1000] train loss: 2.83644, valid loss: 19.64122\n",
      "[487/1000] train loss: 2.85113, valid loss: 20.57908\n",
      "[488/1000] train loss: 3.23232, valid loss: 19.66387\n",
      "[489/1000] train loss: 3.13507, valid loss: 21.48773\n",
      "[490/1000] train loss: 2.89920, valid loss: 19.47149\n",
      "[491/1000] train loss: 2.98089, valid loss: 20.71172\n",
      "[492/1000] train loss: 2.94702, valid loss: 19.55781\n",
      "[493/1000] train loss: 2.76532, valid loss: 20.91162\n",
      "[494/1000] train loss: 2.94998, valid loss: 19.51815\n",
      "[495/1000] train loss: 2.95474, valid loss: 20.65143\n",
      "[496/1000] train loss: 2.82726, valid loss: 19.92495\n",
      "[497/1000] train loss: 3.00791, valid loss: 19.98571\n",
      "[498/1000] train loss: 2.90916, valid loss: 20.20756\n",
      "[499/1000] train loss: 2.80366, valid loss: 20.31387\n",
      "[500/1000] train loss: 2.76777, valid loss: 19.51726\n",
      "[501/1000] train loss: 2.80717, valid loss: 20.38274\n",
      "[502/1000] train loss: 2.93772, valid loss: 20.47741\n",
      "[503/1000] train loss: 2.83901, valid loss: 19.55668\n",
      "[504/1000] train loss: 2.79875, valid loss: 20.29493\n",
      "[505/1000] train loss: 2.73475, valid loss: 20.68394\n",
      "[506/1000] train loss: 2.77150, valid loss: 20.07721\n",
      "[507/1000] train loss: 2.80408, valid loss: 19.24457\n",
      "[508/1000] train loss: 2.82598, valid loss: 20.00176\n",
      "[509/1000] train loss: 2.90391, valid loss: 20.55608\n",
      "[510/1000] train loss: 2.78012, valid loss: 19.35408\n",
      "[511/1000] train loss: 2.64468, valid loss: 20.85563\n",
      "[512/1000] train loss: 2.71855, valid loss: 19.76480\n",
      "[513/1000] train loss: 2.81114, valid loss: 20.10149\n",
      "[514/1000] train loss: 2.76515, valid loss: 20.46359\n",
      "[515/1000] train loss: 2.64926, valid loss: 20.25335\n",
      "[516/1000] train loss: 2.71463, valid loss: 20.29309\n",
      "[517/1000] train loss: 2.68906, valid loss: 19.78506\n",
      "[518/1000] train loss: 2.75866, valid loss: 20.42479\n",
      "[519/1000] train loss: 2.73169, valid loss: 19.36009\n",
      "[520/1000] train loss: 2.75622, valid loss: 19.87667\n",
      "[521/1000] train loss: 2.62357, valid loss: 20.11311\n",
      "[522/1000] train loss: 2.58716, valid loss: 20.24678\n",
      "[523/1000] train loss: 2.59976, valid loss: 20.18339\n",
      "[524/1000] train loss: 2.64180, valid loss: 19.90325\n",
      "[525/1000] train loss: 2.58650, valid loss: 20.21634\n",
      "[526/1000] train loss: 2.55633, valid loss: 20.01552\n",
      "[527/1000] train loss: 2.55359, valid loss: 20.09745\n",
      "[528/1000] train loss: 2.53788, valid loss: 19.86682\n",
      "[529/1000] train loss: 2.48903, valid loss: 20.03074\n",
      "[530/1000] train loss: 2.56645, valid loss: 20.52979\n",
      "[531/1000] train loss: 2.44627, valid loss: 19.68321\n",
      "[532/1000] train loss: 2.47402, valid loss: 20.70210\n",
      "[533/1000] train loss: 2.54630, valid loss: 19.67731\n",
      "[534/1000] train loss: 2.56698, valid loss: 20.90034\n",
      "[535/1000] train loss: 2.52082, valid loss: 19.63883\n",
      "[536/1000] train loss: 2.53189, valid loss: 20.43174\n",
      "[537/1000] train loss: 2.59619, valid loss: 20.33299\n",
      "[538/1000] train loss: 2.36375, valid loss: 20.27050\n",
      "[539/1000] train loss: 2.51746, valid loss: 19.75548\n",
      "[540/1000] train loss: 2.59179, valid loss: 20.12678\n",
      "[541/1000] train loss: 2.44766, valid loss: 20.65218\n",
      "[542/1000] train loss: 2.48383, valid loss: 20.15721\n",
      "[543/1000] train loss: 2.42011, valid loss: 19.86520\n",
      "[544/1000] train loss: 2.42956, valid loss: 20.49310\n",
      "[545/1000] train loss: 2.30506, valid loss: 19.45909\n",
      "[546/1000] train loss: 2.44412, valid loss: 20.17907\n",
      "[547/1000] train loss: 2.42829, valid loss: 19.84222\n",
      "[548/1000] train loss: 2.43013, valid loss: 20.49447\n",
      "[549/1000] train loss: 2.38486, valid loss: 19.80387\n",
      "[550/1000] train loss: 2.35689, valid loss: 20.30296\n",
      "[551/1000] train loss: 2.46837, valid loss: 20.79368\n",
      "[552/1000] train loss: 2.43383, valid loss: 19.51879\n",
      "[553/1000] train loss: 2.51124, valid loss: 21.42004\n",
      "[554/1000] train loss: 2.40135, valid loss: 19.95704\n",
      "[555/1000] train loss: 2.37148, valid loss: 19.66943\n",
      "[556/1000] train loss: 2.49453, valid loss: 20.57900\n",
      "[557/1000] train loss: 2.34911, valid loss: 19.82640\n",
      "[558/1000] train loss: 2.44522, valid loss: 19.93867\n",
      "[559/1000] train loss: 2.43232, valid loss: 20.70352\n",
      "[560/1000] train loss: 2.33449, valid loss: 19.89236\n",
      "[561/1000] train loss: 2.45472, valid loss: 20.45551\n",
      "[562/1000] train loss: 2.37299, valid loss: 20.97359\n",
      "[563/1000] train loss: 2.29844, valid loss: 20.59897\n",
      "[564/1000] train loss: 2.45920, valid loss: 19.54799\n",
      "[565/1000] train loss: 2.36775, valid loss: 20.70404\n",
      "[566/1000] train loss: 2.37511, valid loss: 19.38610\n",
      "[567/1000] train loss: 2.43312, valid loss: 21.02046\n",
      "[568/1000] train loss: 2.50694, valid loss: 20.52980\n",
      "[569/1000] train loss: 2.44893, valid loss: 19.13684\n",
      "[570/1000] train loss: 2.49821, valid loss: 20.15228\n",
      "[571/1000] train loss: 2.37174, valid loss: 21.08735\n",
      "[572/1000] train loss: 2.45289, valid loss: 20.78886\n",
      "[573/1000] train loss: 2.49687, valid loss: 20.77334\n",
      "[574/1000] train loss: 2.49247, valid loss: 20.67746\n",
      "[575/1000] train loss: 2.67242, valid loss: 20.32571\n",
      "[576/1000] train loss: 2.46682, valid loss: 19.62358\n",
      "[577/1000] train loss: 2.25728, valid loss: 20.95889\n",
      "[578/1000] train loss: 2.44674, valid loss: 19.74889\n",
      "[579/1000] train loss: 2.62102, valid loss: 20.03729\n",
      "[580/1000] train loss: 2.41910, valid loss: 20.87292\n",
      "[581/1000] train loss: 2.38952, valid loss: 20.50539\n",
      "[582/1000] train loss: 2.59440, valid loss: 20.57413\n",
      "[583/1000] train loss: 2.37122, valid loss: 20.60904\n",
      "[584/1000] train loss: 2.46695, valid loss: 21.25723\n",
      "[585/1000] train loss: 2.41475, valid loss: 20.38996\n",
      "[586/1000] train loss: 2.59568, valid loss: 22.49365\n",
      "[587/1000] train loss: 2.65033, valid loss: 19.74273\n",
      "[588/1000] train loss: 2.39894, valid loss: 20.63661\n",
      "[589/1000] train loss: 2.20893, valid loss: 21.09378\n",
      "[590/1000] train loss: 2.24340, valid loss: 20.94009\n",
      "[591/1000] train loss: 2.29352, valid loss: 20.89710\n",
      "[592/1000] train loss: 2.18564, valid loss: 20.96268\n",
      "[593/1000] train loss: 2.31108, valid loss: 21.35340\n",
      "[594/1000] train loss: 2.14525, valid loss: 21.91022\n",
      "[595/1000] train loss: 2.18601, valid loss: 20.59317\n",
      "[596/1000] train loss: 2.23709, valid loss: 21.66315\n",
      "[597/1000] train loss: 2.59718, valid loss: 20.71106\n",
      "[598/1000] train loss: 2.40928, valid loss: 20.10732\n",
      "[599/1000] train loss: 2.64999, valid loss: 21.16704\n",
      "[600/1000] train loss: 2.26294, valid loss: 20.85603\n",
      "[601/1000] train loss: 2.21425, valid loss: 21.33409\n",
      "[602/1000] train loss: 2.16231, valid loss: 21.45517\n",
      "[603/1000] train loss: 2.10960, valid loss: 20.93472\n",
      "[604/1000] train loss: 2.25299, valid loss: 20.99810\n",
      "[605/1000] train loss: 2.23345, valid loss: 21.10370\n",
      "[606/1000] train loss: 2.25074, valid loss: 21.24296\n",
      "[607/1000] train loss: 2.07998, valid loss: 21.37470\n",
      "[608/1000] train loss: 2.23412, valid loss: 21.37447\n",
      "[609/1000] train loss: 2.20102, valid loss: 22.18668\n",
      "[610/1000] train loss: 2.44391, valid loss: 20.99935\n",
      "[611/1000] train loss: 2.37948, valid loss: 21.83600\n",
      "[612/1000] train loss: 2.27421, valid loss: 21.25641\n",
      "[613/1000] train loss: 2.22968, valid loss: 20.91012\n",
      "[614/1000] train loss: 2.24138, valid loss: 22.35481\n",
      "[615/1000] train loss: 2.41839, valid loss: 21.47159\n",
      "[616/1000] train loss: 2.28358, valid loss: 21.06899\n",
      "[617/1000] train loss: 2.40822, valid loss: 21.79582\n",
      "[618/1000] train loss: 2.24082, valid loss: 21.93380\n",
      "[619/1000] train loss: 2.16656, valid loss: 20.92677\n",
      "[620/1000] train loss: 2.20925, valid loss: 22.12370\n",
      "[621/1000] train loss: 2.16660, valid loss: 20.92163\n",
      "[622/1000] train loss: 2.32717, valid loss: 20.89945\n",
      "[623/1000] train loss: 2.06981, valid loss: 21.96695\n",
      "[624/1000] train loss: 2.09425, valid loss: 20.73512\n",
      "[625/1000] train loss: 2.15921, valid loss: 21.44908\n",
      "[626/1000] train loss: 1.99994, valid loss: 22.51297\n",
      "[627/1000] train loss: 2.04427, valid loss: 21.00536\n",
      "[628/1000] train loss: 2.05561, valid loss: 21.91370\n",
      "[629/1000] train loss: 2.13908, valid loss: 20.83628\n",
      "[630/1000] train loss: 1.99818, valid loss: 20.99859\n",
      "[631/1000] train loss: 2.03184, valid loss: 21.02223\n",
      "[632/1000] train loss: 2.08784, valid loss: 22.77036\n",
      "[633/1000] train loss: 2.17010, valid loss: 20.57773\n",
      "[634/1000] train loss: 1.99033, valid loss: 21.75657\n",
      "[635/1000] train loss: 2.21559, valid loss: 21.25050\n",
      "[636/1000] train loss: 2.23922, valid loss: 20.89984\n",
      "[637/1000] train loss: 2.67348, valid loss: 20.85850\n",
      "[638/1000] train loss: 2.22104, valid loss: 22.88483\n",
      "[639/1000] train loss: 2.16658, valid loss: 21.51386\n",
      "[640/1000] train loss: 2.47022, valid loss: 21.46836\n",
      "[641/1000] train loss: 2.47833, valid loss: 24.98471\n",
      "[642/1000] train loss: 3.10354, valid loss: 21.07547\n",
      "[643/1000] train loss: 2.60144, valid loss: 21.29523\n",
      "[644/1000] train loss: 2.30772, valid loss: 23.00484\n",
      "[645/1000] train loss: 2.13699, valid loss: 20.98448\n",
      "[646/1000] train loss: 2.13302, valid loss: 21.82180\n",
      "[647/1000] train loss: 1.97864, valid loss: 21.31256\n",
      "[648/1000] train loss: 2.03448, valid loss: 21.07275\n",
      "[649/1000] train loss: 2.02947, valid loss: 21.77046\n",
      "[650/1000] train loss: 2.03295, valid loss: 21.37950\n",
      "[651/1000] train loss: 1.97759, valid loss: 21.25605\n",
      "[652/1000] train loss: 1.94155, valid loss: 21.50328\n",
      "[653/1000] train loss: 1.99894, valid loss: 20.93129\n",
      "[654/1000] train loss: 2.09170, valid loss: 22.85846\n",
      "[655/1000] train loss: 1.98723, valid loss: 21.28527\n",
      "[656/1000] train loss: 1.92147, valid loss: 21.89687\n",
      "[657/1000] train loss: 1.94529, valid loss: 22.13563\n",
      "[658/1000] train loss: 1.98138, valid loss: 21.64917\n",
      "[659/1000] train loss: 1.97457, valid loss: 21.83037\n",
      "[660/1000] train loss: 2.00449, valid loss: 21.12649\n",
      "[661/1000] train loss: 2.25710, valid loss: 21.66778\n",
      "[662/1000] train loss: 2.44896, valid loss: 24.29708\n",
      "[663/1000] train loss: 2.57931, valid loss: 21.34787\n",
      "[664/1000] train loss: 2.32767, valid loss: 21.25092\n",
      "[665/1000] train loss: 1.95519, valid loss: 21.34577\n",
      "[666/1000] train loss: 2.01537, valid loss: 21.83757\n",
      "[667/1000] train loss: 2.10175, valid loss: 21.64321\n",
      "[668/1000] train loss: 1.95707, valid loss: 21.16179\n",
      "[669/1000] train loss: 1.93342, valid loss: 21.11094\n",
      "[670/1000] train loss: 1.95864, valid loss: 21.23576\n",
      "[671/1000] train loss: 2.15152, valid loss: 21.23268\n",
      "[672/1000] train loss: 2.16133, valid loss: 21.03603\n",
      "[673/1000] train loss: 2.51552, valid loss: 21.45533\n",
      "[674/1000] train loss: 2.01975, valid loss: 22.47079\n",
      "[675/1000] train loss: 1.93735, valid loss: 20.34910\n",
      "[676/1000] train loss: 1.97985, valid loss: 22.23915\n",
      "[677/1000] train loss: 1.99132, valid loss: 21.74604\n",
      "[678/1000] train loss: 1.99672, valid loss: 21.48715\n",
      "[679/1000] train loss: 1.86666, valid loss: 21.07175\n",
      "[680/1000] train loss: 1.83165, valid loss: 21.20576\n",
      "[681/1000] train loss: 1.85261, valid loss: 21.23367\n",
      "[682/1000] train loss: 1.80998, valid loss: 21.37197\n",
      "[683/1000] train loss: 1.80179, valid loss: 21.17795\n",
      "[684/1000] train loss: 1.93475, valid loss: 21.84451\n",
      "[685/1000] train loss: 1.89717, valid loss: 21.78416\n",
      "[686/1000] train loss: 1.83011, valid loss: 21.14622\n",
      "[687/1000] train loss: 2.04229, valid loss: 22.35104\n",
      "[688/1000] train loss: 2.10273, valid loss: 21.70847\n",
      "[689/1000] train loss: 2.01217, valid loss: 21.63159\n",
      "[690/1000] train loss: 2.15910, valid loss: 22.93935\n",
      "[691/1000] train loss: 2.54407, valid loss: 21.05787\n",
      "[692/1000] train loss: 1.85992, valid loss: 21.21776\n",
      "[693/1000] train loss: 1.91574, valid loss: 22.55886\n",
      "[694/1000] train loss: 2.04457, valid loss: 20.75408\n",
      "[695/1000] train loss: 1.83465, valid loss: 20.87343\n",
      "[696/1000] train loss: 1.88708, valid loss: 22.54297\n",
      "[697/1000] train loss: 2.05471, valid loss: 21.07470\n",
      "[698/1000] train loss: 2.15264, valid loss: 22.10349\n",
      "[699/1000] train loss: 2.06079, valid loss: 22.41867\n",
      "[700/1000] train loss: 2.37453, valid loss: 19.27403\n",
      "[701/1000] train loss: 2.38096, valid loss: 20.91974\n",
      "[702/1000] train loss: 1.98750, valid loss: 20.80301\n",
      "[703/1000] train loss: 1.99199, valid loss: 20.09398\n",
      "[704/1000] train loss: 2.21611, valid loss: 21.19242\n",
      "[705/1000] train loss: 2.11898, valid loss: 20.16799\n",
      "[706/1000] train loss: 1.87634, valid loss: 20.73061\n",
      "[707/1000] train loss: 1.82732, valid loss: 20.14687\n",
      "[708/1000] train loss: 1.83938, valid loss: 20.51920\n",
      "[709/1000] train loss: 1.97026, valid loss: 21.06628\n",
      "[710/1000] train loss: 1.91710, valid loss: 20.54007\n",
      "[711/1000] train loss: 1.93094, valid loss: 20.35375\n",
      "[712/1000] train loss: 1.93310, valid loss: 21.29461\n",
      "[713/1000] train loss: 1.95503, valid loss: 19.57320\n",
      "[714/1000] train loss: 1.93756, valid loss: 20.34786\n",
      "[715/1000] train loss: 1.84161, valid loss: 20.73685\n",
      "[716/1000] train loss: 1.82245, valid loss: 20.30896\n",
      "[717/1000] train loss: 1.72790, valid loss: 20.93488\n",
      "[718/1000] train loss: 1.73799, valid loss: 21.07462\n",
      "[719/1000] train loss: 1.86902, valid loss: 20.50409\n",
      "[720/1000] train loss: 1.93070, valid loss: 20.55586\n",
      "[721/1000] train loss: 1.83416, valid loss: 21.30620\n",
      "[722/1000] train loss: 2.01028, valid loss: 21.71069\n",
      "[723/1000] train loss: 1.81223, valid loss: 20.38631\n",
      "[724/1000] train loss: 1.80134, valid loss: 21.26166\n",
      "[725/1000] train loss: 1.78849, valid loss: 21.15326\n",
      "[726/1000] train loss: 1.77782, valid loss: 21.07517\n",
      "[727/1000] train loss: 1.78314, valid loss: 20.87530\n",
      "[728/1000] train loss: 1.78560, valid loss: 21.28238\n",
      "[729/1000] train loss: 1.82187, valid loss: 21.14162\n",
      "[730/1000] train loss: 1.69133, valid loss: 21.37364\n",
      "[731/1000] train loss: 1.67313, valid loss: 21.54041\n",
      "[732/1000] train loss: 1.70733, valid loss: 21.66152\n",
      "[733/1000] train loss: 1.71540, valid loss: 21.02999\n",
      "[734/1000] train loss: 1.75710, valid loss: 21.05427\n",
      "[735/1000] train loss: 1.73324, valid loss: 21.97425\n",
      "[736/1000] train loss: 1.80124, valid loss: 21.37992\n",
      "[737/1000] train loss: 1.75112, valid loss: 20.81987\n",
      "[738/1000] train loss: 1.76179, valid loss: 21.44631\n",
      "[739/1000] train loss: 1.75407, valid loss: 20.95832\n",
      "[740/1000] train loss: 1.71176, valid loss: 21.84618\n",
      "[741/1000] train loss: 1.85991, valid loss: 21.82221\n",
      "[742/1000] train loss: 1.74148, valid loss: 20.67704\n",
      "[743/1000] train loss: 1.85644, valid loss: 22.15041\n",
      "[744/1000] train loss: 1.90711, valid loss: 20.65154\n",
      "[745/1000] train loss: 1.88516, valid loss: 20.93377\n",
      "[746/1000] train loss: 1.84712, valid loss: 21.85194\n",
      "[747/1000] train loss: 1.71676, valid loss: 22.05715\n",
      "[748/1000] train loss: 1.82875, valid loss: 20.79803\n",
      "[749/1000] train loss: 1.80682, valid loss: 21.15011\n",
      "[750/1000] train loss: 1.66020, valid loss: 21.08882\n",
      "[751/1000] train loss: 1.89171, valid loss: 21.02877\n",
      "[752/1000] train loss: 2.04014, valid loss: 22.63186\n",
      "[753/1000] train loss: 2.49009, valid loss: 21.19697\n",
      "[754/1000] train loss: 2.12473, valid loss: 21.13195\n",
      "[755/1000] train loss: 1.85149, valid loss: 22.00290\n",
      "[756/1000] train loss: 2.05304, valid loss: 21.43061\n",
      "[757/1000] train loss: 1.92945, valid loss: 21.42445\n",
      "[758/1000] train loss: 1.96205, valid loss: 21.60548\n",
      "[759/1000] train loss: 1.73055, valid loss: 21.47180\n",
      "[760/1000] train loss: 1.63855, valid loss: 20.91411\n",
      "[761/1000] train loss: 1.66324, valid loss: 21.32628\n",
      "[762/1000] train loss: 1.76818, valid loss: 21.43635\n",
      "[763/1000] train loss: 1.60946, valid loss: 21.50781\n",
      "[764/1000] train loss: 1.66584, valid loss: 22.21775\n",
      "[765/1000] train loss: 1.91205, valid loss: 21.78605\n",
      "[766/1000] train loss: 1.88873, valid loss: 20.56116\n",
      "[767/1000] train loss: 1.88828, valid loss: 21.86455\n",
      "[768/1000] train loss: 1.87746, valid loss: 21.14468\n",
      "[769/1000] train loss: 1.69433, valid loss: 21.00820\n",
      "[770/1000] train loss: 1.64818, valid loss: 22.58074\n",
      "[771/1000] train loss: 1.75312, valid loss: 21.10443\n",
      "[772/1000] train loss: 1.75765, valid loss: 21.24893\n",
      "[773/1000] train loss: 1.79152, valid loss: 22.85795\n",
      "[774/1000] train loss: 2.03403, valid loss: 21.73794\n",
      "[775/1000] train loss: 2.08636, valid loss: 20.74718\n",
      "[776/1000] train loss: 2.24098, valid loss: 22.64991\n",
      "[777/1000] train loss: 2.22058, valid loss: 22.76409\n",
      "[778/1000] train loss: 1.96646, valid loss: 20.77725\n",
      "[779/1000] train loss: 2.44310, valid loss: 23.64932\n",
      "[780/1000] train loss: 2.09525, valid loss: 22.56220\n",
      "[781/1000] train loss: 1.80794, valid loss: 21.16450\n",
      "[782/1000] train loss: 1.99125, valid loss: 21.82313\n",
      "[783/1000] train loss: 1.92159, valid loss: 20.52194\n",
      "[784/1000] train loss: 1.79952, valid loss: 20.61559\n",
      "[785/1000] train loss: 2.14230, valid loss: 22.59358\n",
      "[786/1000] train loss: 1.88898, valid loss: 21.43782\n",
      "[787/1000] train loss: 1.83577, valid loss: 21.12038\n",
      "[788/1000] train loss: 1.74023, valid loss: 21.79415\n",
      "[789/1000] train loss: 1.68786, valid loss: 21.20004\n",
      "[790/1000] train loss: 1.85354, valid loss: 20.96328\n",
      "[791/1000] train loss: 1.71628, valid loss: 21.38454\n",
      "[792/1000] train loss: 1.74841, valid loss: 21.53918\n",
      "[793/1000] train loss: 1.77356, valid loss: 20.62206\n",
      "[794/1000] train loss: 1.63460, valid loss: 22.01368\n",
      "[795/1000] train loss: 1.62053, valid loss: 21.41745\n",
      "[796/1000] train loss: 1.62814, valid loss: 21.99778\n",
      "[797/1000] train loss: 1.81373, valid loss: 22.71226\n",
      "[798/1000] train loss: 1.76168, valid loss: 21.35465\n",
      "[799/1000] train loss: 1.64906, valid loss: 22.16143\n",
      "[800/1000] train loss: 1.64852, valid loss: 21.64845\n",
      "[801/1000] train loss: 1.62341, valid loss: 20.90620\n",
      "[802/1000] train loss: 1.62828, valid loss: 22.01821\n",
      "[803/1000] train loss: 1.75428, valid loss: 21.98348\n",
      "[804/1000] train loss: 1.66726, valid loss: 21.69908\n",
      "[805/1000] train loss: 1.59245, valid loss: 21.81782\n",
      "[806/1000] train loss: 1.92668, valid loss: 21.00993\n",
      "[807/1000] train loss: 1.72846, valid loss: 22.37018\n",
      "[808/1000] train loss: 1.79306, valid loss: 21.20573\n",
      "[809/1000] train loss: 2.06702, valid loss: 22.13781\n",
      "[810/1000] train loss: 1.77580, valid loss: 22.57727\n",
      "[811/1000] train loss: 1.85717, valid loss: 21.02273\n",
      "[812/1000] train loss: 1.70342, valid loss: 21.64171\n",
      "[813/1000] train loss: 1.61389, valid loss: 21.46399\n",
      "[814/1000] train loss: 1.60418, valid loss: 21.15901\n",
      "[815/1000] train loss: 1.52165, valid loss: 21.62839\n",
      "[816/1000] train loss: 1.58870, valid loss: 21.49038\n",
      "[817/1000] train loss: 1.54407, valid loss: 21.30636\n",
      "[818/1000] train loss: 1.58505, valid loss: 21.57056\n",
      "[819/1000] train loss: 1.59346, valid loss: 21.68266\n",
      "[820/1000] train loss: 1.64306, valid loss: 21.52879\n",
      "[821/1000] train loss: 1.69358, valid loss: 22.12677\n",
      "[822/1000] train loss: 1.64636, valid loss: 22.46984\n",
      "[823/1000] train loss: 1.69489, valid loss: 21.22542\n",
      "[824/1000] train loss: 1.63000, valid loss: 22.80711\n",
      "[825/1000] train loss: 1.81128, valid loss: 22.16395\n",
      "[826/1000] train loss: 2.29052, valid loss: 21.64332\n",
      "[827/1000] train loss: 1.73399, valid loss: 22.13679\n",
      "[828/1000] train loss: 1.69455, valid loss: 21.14017\n",
      "[829/1000] train loss: 1.52569, valid loss: 21.63632\n",
      "[830/1000] train loss: 1.60558, valid loss: 21.36790\n",
      "[831/1000] train loss: 1.60857, valid loss: 21.19221\n",
      "[832/1000] train loss: 1.61424, valid loss: 21.91544\n",
      "[833/1000] train loss: 1.58682, valid loss: 21.77350\n",
      "[834/1000] train loss: 1.61980, valid loss: 21.89736\n",
      "[835/1000] train loss: 1.71227, valid loss: 20.90555\n",
      "[836/1000] train loss: 1.52025, valid loss: 22.33204\n",
      "[837/1000] train loss: 1.54873, valid loss: 21.89282\n",
      "[838/1000] train loss: 1.45309, valid loss: 21.91612\n",
      "[839/1000] train loss: 1.53400, valid loss: 22.02974\n",
      "[840/1000] train loss: 1.63827, valid loss: 22.11426\n",
      "[841/1000] train loss: 1.59510, valid loss: 21.14987\n",
      "[842/1000] train loss: 1.59966, valid loss: 21.72289\n",
      "[843/1000] train loss: 1.62479, valid loss: 22.61651\n",
      "[844/1000] train loss: 1.54072, valid loss: 21.54705\n",
      "[845/1000] train loss: 1.46154, valid loss: 22.50487\n",
      "[846/1000] train loss: 1.78825, valid loss: 21.44756\n",
      "[847/1000] train loss: 2.00860, valid loss: 20.70082\n",
      "[848/1000] train loss: 1.91474, valid loss: 23.22181\n",
      "[849/1000] train loss: 1.99220, valid loss: 21.65413\n",
      "[850/1000] train loss: 1.64167, valid loss: 21.62471\n",
      "[851/1000] train loss: 1.69785, valid loss: 22.22970\n",
      "[852/1000] train loss: 1.78084, valid loss: 20.77353\n",
      "[853/1000] train loss: 1.70767, valid loss: 21.93643\n",
      "[854/1000] train loss: 1.58260, valid loss: 22.00212\n",
      "[855/1000] train loss: 1.53261, valid loss: 20.73828\n",
      "[856/1000] train loss: 1.57489, valid loss: 21.35909\n",
      "[857/1000] train loss: 1.64492, valid loss: 22.05332\n",
      "[858/1000] train loss: 1.71241, valid loss: 21.95852\n",
      "[859/1000] train loss: 1.54065, valid loss: 20.70778\n",
      "[860/1000] train loss: 1.46733, valid loss: 22.18760\n",
      "[861/1000] train loss: 1.46047, valid loss: 21.88506\n",
      "[862/1000] train loss: 1.40982, valid loss: 22.24024\n",
      "[863/1000] train loss: 1.43600, valid loss: 21.73586\n",
      "[864/1000] train loss: 1.47531, valid loss: 21.79377\n",
      "[865/1000] train loss: 1.45211, valid loss: 22.22382\n",
      "[866/1000] train loss: 1.46299, valid loss: 21.41073\n",
      "[867/1000] train loss: 1.51511, valid loss: 20.99507\n",
      "[868/1000] train loss: 1.50973, valid loss: 21.80116\n",
      "[869/1000] train loss: 1.47905, valid loss: 22.47325\n",
      "[870/1000] train loss: 1.51269, valid loss: 22.53964\n",
      "[871/1000] train loss: 1.66814, valid loss: 21.55806\n",
      "[872/1000] train loss: 1.89317, valid loss: 21.71727\n",
      "[873/1000] train loss: 1.87752, valid loss: 22.00691\n",
      "[874/1000] train loss: 1.48066, valid loss: 21.63634\n",
      "[875/1000] train loss: 1.82634, valid loss: 21.41784\n",
      "[876/1000] train loss: 1.71285, valid loss: 22.90042\n",
      "[877/1000] train loss: 2.64542, valid loss: 21.48719\n",
      "[878/1000] train loss: 2.71315, valid loss: 21.45970\n",
      "[879/1000] train loss: 2.55032, valid loss: 23.05799\n",
      "[880/1000] train loss: 1.96557, valid loss: 20.43527\n",
      "[881/1000] train loss: 1.70728, valid loss: 20.48908\n",
      "[882/1000] train loss: 1.83088, valid loss: 21.57048\n",
      "[883/1000] train loss: 1.62934, valid loss: 21.22843\n",
      "[884/1000] train loss: 1.53970, valid loss: 20.82473\n",
      "[885/1000] train loss: 1.66285, valid loss: 21.89788\n",
      "[886/1000] train loss: 1.59998, valid loss: 21.01932\n",
      "[887/1000] train loss: 1.78121, valid loss: 21.42423\n",
      "[888/1000] train loss: 1.67711, valid loss: 22.44182\n",
      "[889/1000] train loss: 1.91317, valid loss: 21.64224\n",
      "[890/1000] train loss: 1.72628, valid loss: 21.47470\n",
      "[891/1000] train loss: 1.72450, valid loss: 23.00517\n",
      "[892/1000] train loss: 1.57171, valid loss: 21.26711\n",
      "[893/1000] train loss: 1.56058, valid loss: 21.76690\n",
      "[894/1000] train loss: 1.80056, valid loss: 22.48346\n",
      "[895/1000] train loss: 1.94409, valid loss: 21.53216\n",
      "[896/1000] train loss: 1.61194, valid loss: 21.42151\n",
      "[897/1000] train loss: 1.52870, valid loss: 22.57019\n",
      "[898/1000] train loss: 1.56601, valid loss: 21.22749\n",
      "[899/1000] train loss: 1.56852, valid loss: 22.23782\n",
      "[900/1000] train loss: 1.49173, valid loss: 21.86337\n",
      "[901/1000] train loss: 1.56129, valid loss: 21.68727\n",
      "[902/1000] train loss: 1.38105, valid loss: 22.94150\n",
      "[903/1000] train loss: 1.41197, valid loss: 22.35391\n",
      "[904/1000] train loss: 1.38673, valid loss: 22.04556\n",
      "[905/1000] train loss: 1.39310, valid loss: 22.29677\n",
      "[906/1000] train loss: 1.36520, valid loss: 22.93234\n",
      "[907/1000] train loss: 1.37390, valid loss: 22.05535\n",
      "[908/1000] train loss: 1.35858, valid loss: 22.43977\n",
      "[909/1000] train loss: 1.38717, valid loss: 22.66136\n",
      "[910/1000] train loss: 1.46308, valid loss: 22.63544\n",
      "[911/1000] train loss: 1.47798, valid loss: 21.95597\n",
      "[912/1000] train loss: 1.46215, valid loss: 22.25157\n",
      "[913/1000] train loss: 1.53417, valid loss: 23.76998\n",
      "[914/1000] train loss: 1.53205, valid loss: 22.49653\n",
      "[915/1000] train loss: 1.39535, valid loss: 22.11288\n",
      "[916/1000] train loss: 1.46799, valid loss: 23.06545\n",
      "[917/1000] train loss: 1.38230, valid loss: 22.89153\n",
      "[918/1000] train loss: 1.40795, valid loss: 22.25599\n",
      "[919/1000] train loss: 1.38037, valid loss: 22.69834\n",
      "[920/1000] train loss: 1.44054, valid loss: 22.47169\n",
      "[921/1000] train loss: 1.47841, valid loss: 22.38974\n",
      "[922/1000] train loss: 1.45592, valid loss: 22.01633\n",
      "[923/1000] train loss: 1.82406, valid loss: 23.81117\n",
      "[924/1000] train loss: 1.55886, valid loss: 23.17259\n",
      "[925/1000] train loss: 1.72947, valid loss: 22.11993\n",
      "[926/1000] train loss: 1.61098, valid loss: 21.53272\n",
      "[927/1000] train loss: 1.47663, valid loss: 22.16525\n",
      "[928/1000] train loss: 1.46328, valid loss: 22.83459\n",
      "[929/1000] train loss: 1.48728, valid loss: 22.97919\n",
      "[930/1000] train loss: 1.42025, valid loss: 22.30396\n",
      "[931/1000] train loss: 1.44996, valid loss: 21.98971\n",
      "[932/1000] train loss: 1.40668, valid loss: 23.04277\n",
      "[933/1000] train loss: 1.33612, valid loss: 22.64645\n",
      "[934/1000] train loss: 1.38810, valid loss: 22.68379\n",
      "[935/1000] train loss: 1.35269, valid loss: 22.68572\n",
      "[936/1000] train loss: 1.34900, valid loss: 21.91702\n",
      "[937/1000] train loss: 1.33726, valid loss: 22.91548\n",
      "[938/1000] train loss: 1.32425, valid loss: 22.78810\n",
      "[939/1000] train loss: 1.42608, valid loss: 22.44307\n",
      "[940/1000] train loss: 1.30939, valid loss: 23.06510\n",
      "[941/1000] train loss: 1.33532, valid loss: 21.98613\n",
      "[942/1000] train loss: 1.45791, valid loss: 22.45801\n",
      "[943/1000] train loss: 1.58414, valid loss: 22.73320\n",
      "[944/1000] train loss: 1.51805, valid loss: 23.04840\n",
      "[945/1000] train loss: 1.59044, valid loss: 22.79859\n",
      "[946/1000] train loss: 1.57852, valid loss: 21.61192\n",
      "[947/1000] train loss: 1.75854, valid loss: 23.39858\n",
      "[948/1000] train loss: 1.49338, valid loss: 24.01236\n",
      "[949/1000] train loss: 1.89567, valid loss: 22.38142\n",
      "[950/1000] train loss: 1.94638, valid loss: 22.40685\n",
      "[951/1000] train loss: 1.58354, valid loss: 23.91003\n",
      "[952/1000] train loss: 1.78155, valid loss: 22.40597\n",
      "[953/1000] train loss: 1.51604, valid loss: 22.36494\n",
      "[954/1000] train loss: 1.41288, valid loss: 23.34631\n",
      "[955/1000] train loss: 1.69055, valid loss: 23.25724\n",
      "[956/1000] train loss: 1.67249, valid loss: 22.53414\n",
      "[957/1000] train loss: 1.61086, valid loss: 22.57567\n",
      "[958/1000] train loss: 1.52895, valid loss: 22.08373\n",
      "[959/1000] train loss: 1.96805, valid loss: 22.36090\n",
      "[960/1000] train loss: 1.56725, valid loss: 23.79234\n",
      "[961/1000] train loss: 1.54048, valid loss: 22.06565\n",
      "[962/1000] train loss: 1.43616, valid loss: 22.03259\n",
      "[963/1000] train loss: 1.51297, valid loss: 23.22712\n",
      "[964/1000] train loss: 1.44415, valid loss: 21.87321\n",
      "[965/1000] train loss: 1.44258, valid loss: 21.90109\n",
      "[966/1000] train loss: 1.32283, valid loss: 22.18289\n",
      "[967/1000] train loss: 1.29517, valid loss: 22.03617\n",
      "[968/1000] train loss: 1.33007, valid loss: 22.61746\n",
      "[969/1000] train loss: 1.40024, valid loss: 22.50811\n",
      "[970/1000] train loss: 1.38915, valid loss: 21.96726\n",
      "[971/1000] train loss: 1.50088, valid loss: 22.21892\n",
      "[972/1000] train loss: 1.59352, valid loss: 23.91889\n",
      "[973/1000] train loss: 2.36886, valid loss: 22.77197\n",
      "[974/1000] train loss: 1.54878, valid loss: 21.83103\n",
      "[975/1000] train loss: 1.56773, valid loss: 23.50372\n",
      "[976/1000] train loss: 1.54361, valid loss: 21.57554\n",
      "[977/1000] train loss: 1.29959, valid loss: 22.58092\n",
      "[978/1000] train loss: 1.34593, valid loss: 22.76232\n",
      "[979/1000] train loss: 1.37917, valid loss: 22.07875\n",
      "[980/1000] train loss: 1.33089, valid loss: 22.27490\n",
      "[981/1000] train loss: 1.33865, valid loss: 21.81861\n",
      "[982/1000] train loss: 1.28283, valid loss: 22.81263\n",
      "[983/1000] train loss: 1.30898, valid loss: 22.33440\n",
      "[984/1000] train loss: 1.45852, valid loss: 21.80176\n",
      "[985/1000] train loss: 1.52212, valid loss: 23.85825\n",
      "[986/1000] train loss: 1.60595, valid loss: 21.99293\n",
      "[987/1000] train loss: 1.52362, valid loss: 21.45008\n",
      "[988/1000] train loss: 1.63053, valid loss: 22.74099\n",
      "[989/1000] train loss: 1.55070, valid loss: 21.81020\n",
      "[990/1000] train loss: 1.51305, valid loss: 22.16331\n",
      "[991/1000] train loss: 1.45029, valid loss: 22.22356\n",
      "[992/1000] train loss: 1.52185, valid loss: 22.80221\n",
      "[993/1000] train loss: 1.59242, valid loss: 22.31540\n",
      "[994/1000] train loss: 1.46628, valid loss: 21.75662\n",
      "[995/1000] train loss: 1.46736, valid loss: 23.80196\n",
      "[996/1000] train loss: 1.52074, valid loss: 22.35338\n",
      "[997/1000] train loss: 1.47025, valid loss: 21.13255\n",
      "[998/1000] train loss: 1.83166, valid loss: 22.59885\n",
      "[999/1000] train loss: 1.57814, valid loss: 22.26113\n",
      "[1000/1000] train loss: 1.71750, valid loss: 20.93136\n"
     ]
    }
   ],
   "source": [
    "# 학습:  train + validation\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "for epoch in range(1000):  # 1000에폭 학습.\n",
    "    ######################\n",
    "    # train\n",
    "    ######################\n",
    "    boston_model.train()\n",
    "    train_loss = 0.0 # 현재 epoch loss를 저장할 함수.\n",
    "    # step단위로 학습.\n",
    "    for X_train, y_train in train_loader:\n",
    "        # 1 batch 학습 == 1 Step\n",
    "        # 1. X, y를 device로 옮기\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "        # 2. 모델을 이용해서 추정\n",
    "        pred = boston_model(X_train)\n",
    "\n",
    "        # 3. Loss 계산\n",
    "        loss = loss_fn(pred, y_train) # (모델추정값, 정답)\n",
    "\n",
    "        # 4. gradient 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. weight/bias update\n",
    "        optimizer.step()\n",
    "\n",
    "        # 6. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader) # 1 에폭 loss 결과 저장.\n",
    "\n",
    "    ######################\n",
    "    # validation\n",
    "    ######################\n",
    "    boston_model.eval()\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            # 1. X, y를 device로 옮기기.\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "            # 2. 추정\n",
    "            pred_test = boston_model(X_test)\n",
    "\n",
    "            # 3. 검증 - loss 계산.\n",
    "            valid_loss += loss_fn(pred_test, y_test).item()\n",
    "\n",
    "        valid_loss /= len(test_loader)\n",
    "\n",
    "    # 현재 epoch 학습 결과를 출력\n",
    "    print(f\"[{epoch+1}/1000] train loss: {train_loss:.5f}, valid loss: {valid_loss:.5f}\")\n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "executionInfo": {
     "elapsed": 677,
     "status": "ok",
     "timestamp": 1747026583736,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "aWd83c66W74U",
    "outputId": "3c6ee1ee-7c51-4ed7-b058-48850ea62969"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# epoch별 loss 변화를 시각화\n",
    "plt.plot(range(1000), train_loss_list, label=\"train loss\")\n",
    "plt.plot(range(1000), valid_loss_list, label=\"valid loss\")\n",
    "plt.ylim(0, 25)\n",
    "# plt.xlim(200, 250)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1747026924520,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "Al50G8uyW71m"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# 모델 저장 - 불러오기\n",
    "########################\n",
    "import os\n",
    "# 코랩-구글드라이브에 저장.\n",
    "os.makedirs('saved_model', exist_ok=True)\n",
    "torch.save(boston_model, \"saved_model/boston_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747027074886,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "GqsWH7HNIUqi"
   },
   "outputs": [],
   "source": [
    "load_boston_model = torch.load(\n",
    "    \"saved_model/boston_model.pt\", weights_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1747027179239,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "XdMfhO2V7can"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 파라미터만 저장 - 불러오기\n",
    "############################\n",
    "# state_dict를 조회\n",
    "torch.save(\n",
    "    boston_model.state_dict(), \"saved_model/boston_model_state_dict.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1747027238622,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "i0o4HPXd7cYZ",
    "outputId": "4ce11a8d-cd44-4d30-c86a-597e91b84c5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_sd = torch.load(\"saved_model/boston_model_state_dict.pt\")\n",
    "load_boston_model2 = BostonModel()\n",
    "load_boston_model2.load_state_dict(load_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1747027676569,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "xEfyel5c7cV2",
    "outputId": "88e0776e-5a22-491d-b309-7bb9fe82bd6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.931360244750977\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# 최종 평가\n",
    "####################\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_loader:\n",
    "        # device로 이동\n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        # 추론\n",
    "        # pred_test = boston_model(X_test)\n",
    "        pred_test = load_boston_model2(X_test)\n",
    "        # loss 계산\n",
    "        loss_test = loss_fn(pred_test, y_test)\n",
    "        test_loss += loss_test.item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1747027905604,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "N0Mi1qDp7cSy"
   },
   "outputs": [],
   "source": [
    "# 추론\n",
    "new_X, _ = trainset[:5]\n",
    "load_boston_model.eval()\n",
    "with torch.no_grad():\n",
    "    new_y = load_boston_model(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1747027912356,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "meH3hw9qIUqi",
    "outputId": "ad29d979-d691-4bd5-d3fb-43930d3a990f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26.5098],\n",
       "        [21.0406],\n",
       "        [23.0117],\n",
       "        [21.2224],\n",
       "        [10.2827]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y\n",
    "\n",
    "# tensor([[26.5098],\n",
    "#         [21.0406],\n",
    "#         [23.0117],\n",
    "#         [21.2224],\n",
    "#         [10.2827]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtAC6rMaIUqi"
   },
   "source": [
    "## 분류 (Classification)\n",
    "\n",
    "### Fashion MNIST Dataset - **다중분류(Multi-Class Classification) 문제**\n",
    "\n",
    "10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋.\n",
    "이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 의류 품목을 나타낸다:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>그림</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnqTZSDWIUqi"
   },
   "source": [
    "- **Feature**이미지는 28x28 크기이며 Gray scale이다.\n",
    "- **Target**은 총 10개의 class로 구성되어 있으며 각 class의 class 이름은 다음과 같다.\n",
    "\n",
    "| 레이블 | 클래스       |\n",
    "|--------|--------------|\n",
    "| 0      | T-shirt/top |\n",
    "| 1      | Trousers    |\n",
    "| 2      | Pullover    |\n",
    "| 3      | Dress       |\n",
    "| 4      | Coat        |\n",
    "| 5      | Sandal      |\n",
    "| 6      | Shirt       |\n",
    "| 7      | Sneaker     |\n",
    "| 8      | Bag         |\n",
    "| 9      | Ankle boot  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4tcn2xsIUqi"
   },
   "source": [
    "> #### 학습 도중 모델 저장\n",
    ">\n",
    "> - 학습 도중 가장 좋은 성능을 보이는 모델이 나올 수 있다.\n",
    "> - 학습 도중 모델을 저장하는 방법\n",
    ">   1. 각 에폭이 끝날 때 마다 모델을 저장한다.\n",
    ">   2. 한 에폭 학습 후 성능 개선이 있으면 모델을 저장하여 가장 성능 좋은 모델만 저장되도록 한다.\n",
    ">      - 최고 성능 점수(best score)와 현재 에폭의 성능을 비교하여, 성능이 개선되었을 경우 모델을 저장(덮어쓰기)한다.\n",
    ">\n",
    "> #### 조기 종료(Early Stopping)\n",
    ">\n",
    "> - 학습 도중 성능 개선이 나타나지 않으면, 중간에 학습을 종료하도록 구현한다.\n",
    "> - 에폭 수를 충분히 길게 설정한 뒤, 특정 횟수 동안 성능 개선이 없으면 학습을 조기 종료하도록 구현한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1795,
     "status": "ok",
     "timestamp": 1747030235465,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "lcAcTsMkHIZO",
    "outputId": "582b48e3-b371-4ba4-a140-e109e5fda68b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1747030615756,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "L55N5f1xHIWo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Dataset loading\n",
    "fmnist_trainset = datasets.FashionMNIST(\n",
    "    root=\"data\", train=True, download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "fmnist_testset = datasets.FashionMNIST(\n",
    "    root=\"data\", train=False, download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1747030618095,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "uYrbdp7JHITo",
    "outputId": "d5b9102d-7603-4906-eb22-a7ca0b4c9588"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747030624352,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "48f0-papHIRO",
    "outputId": "45635a67-fccb-4c23-d616-3ad0772c503c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1747030626726,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "UnhAMiKPHIOx",
    "outputId": "dd8de1b7-b148-448e-b5f3-022ab07f0bb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
       "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
       "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
       "           0.0157, 0.0000, 0.0000, 0.0118],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
       "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0471, 0.0392, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
       "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
       "           0.3020, 0.5098, 0.2824, 0.0588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
       "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
       "           0.5529, 0.3451, 0.6745, 0.2588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
       "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
       "           0.4824, 0.7686, 0.8980, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
       "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
       "           0.8745, 0.9608, 0.6784, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
       "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
       "           0.8627, 0.9529, 0.7922, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
       "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
       "           0.8863, 0.7725, 0.8196, 0.2039],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
       "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
       "           0.9608, 0.4667, 0.6549, 0.2196],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
       "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
       "           0.8510, 0.8196, 0.3608, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
       "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
       "           0.8549, 1.0000, 0.3020, 0.0000],\n",
       "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
       "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
       "           0.8784, 0.9569, 0.6235, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
       "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
       "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
       "           0.9137, 0.9333, 0.8431, 0.0000],\n",
       "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
       "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
       "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
       "           0.8627, 0.9098, 0.9647, 0.0000],\n",
       "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
       "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
       "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
       "           0.8706, 0.8941, 0.8824, 0.0000],\n",
       "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
       "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
       "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
       "           0.8745, 0.8784, 0.8980, 0.1137],\n",
       "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
       "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
       "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
       "           0.8627, 0.8667, 0.9020, 0.2627],\n",
       "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
       "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
       "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
       "           0.7098, 0.8039, 0.8078, 0.4510],\n",
       "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
       "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
       "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
       "           0.6549, 0.6941, 0.8235, 0.3608],\n",
       "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
       "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
       "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
       "           0.7529, 0.8471, 0.6667, 0.0000],\n",
       "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
       "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
       "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
       "           0.3882, 0.2275, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
       "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 9)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_trainset[0]\n",
    "# (<PIL.Image.Image image mode=L size=28x28>, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 45
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1747030469158,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "kuslHrFAHIL0",
    "outputId": "7fb2d252-0141-4005-9365-84a9fc830fad"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APMfhppltqHiWSW8to7iG1t2l8uVAyFshRlTweGJ/CvcbzStE1lEmvdK025kAGJXhTftHRc46VV17w/pGo6DJpKWOnwxSBjCsMKJ5TkcOu0DBzjJ79+K+ZiMHB61s+HPE+p+Fr17vTHiWV0KnzIlcdDg4PcE5HuOc177pkn2nTLScps82FJGX0ZlDH9TXNfELxZqfhiDT/7MaFWu45opvMhDgbdhRhnow3N/UGvEZZGmmeVyC7sWYgAcn2FSWdu13ewWyfelkWMfUkD+tfUSWwWPYoACoSAPQED+hrgPi/Z58N2dxt+ZLlSD6K6H/AV4rWhoLbPEGmtgHF1EcHv84r3g+LLyC6uIltrUj7O4yQ+cc/7Vct8SNZn1PwhF5kUMYE8AxGG6bH9SfSvIK//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACJElEQVR4AVWSzW4TMRDHx17bu5vNJqFK0tKAooiqrSqkUqkHPgQSlThwBAkegAMCDghx4S3gyqvwAogDcOHWQwUqNE3bJUqaull7beNdbyLqg3c8vxnP37MD4Fb0/sur0nzy6U1pIff9eM8bbJzs744XbrPaQXX/+V7ud3DnXVJDYaty+G07GA3afxunj3KI8w0e/BTSnPT30o1pMuxI/Afu5G4HlwVI7YtkOE50hZ8qz9zNISkiaqNRAIRUiaCYYHoOerXw51sPh3IoKph5QSABacPPOrm/yFxKNfrlTVCUaUm4Fv1KnLSOy5rNvv/QO0jNlJ+nXMvBzesc1m1mIahVvd/dHrW0wJqFwaLgm1ts00L3zu6H18868ZgBRlm0qJ6+7bxI59Aaj1/+FgQRDLzn7dhzvoprkUfhx8RgClojw+sAnjeHRmUwAUEzD7AJziSAVnNoI4FmkiiCkA717kyKa58N68gAogwMlQiulDpLaOCWZDj1QVEFYbtseQk1XMsYA2J12AaugclLzkIMafsGgY8NVliugf4PIqgloWASA0EeISugi+a4axFcjY3PDPORCSET1N04U7te51VBSAgi8k1Qd3JncMGX9YzYJvE68ehSUbIUhKAnUDRiUFMiDjGKL2aqNLQzFrW0/TN2hC7WFIoe6ZSqoZlgDVP3lmJMbIXVhmxcYs3aSnvrc4yEq+mghq/No/RYLV/+7ncNv3HoMv8Bp1jNMx1RU5AAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 10\n",
    "fmnist_trainset[idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1747030736998,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "HWC-mAB3JXYA",
    "outputId": "324e917c-a496-4541-9616-53365a37a8da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fmnist_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1747030781842,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "KGU-59ztIUqi"
   },
   "outputs": [],
   "source": [
    "# trainset -> trainset/validationset으로 분리\n",
    "fmnist_trainset, fmnist_validset = torch.utils.data.random_split(\n",
    "    fmnist_trainset, [50000, 10000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1747030827561,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "0aJvonUtIUqj",
    "outputId": "555c8591-9aaf-44a3-fcda-92f8ee48fa3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 10000)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fmnist_trainset), len(fmnist_validset), len(fmnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1747030891208,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "gUefQlhRJ2G-",
    "outputId": "a780e276-3feb-464d-bfc9-ede57804f0d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_testset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1747030905327,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "f3kh8vhzJ2Eu",
    "outputId": "087fd8ac-d52a-45f0-b8e2-35d522fa4396"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_testset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1747031155021,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "1RVZUHulKXaL"
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# DataLoader\n",
    "##############\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    fmnist_trainset, batch_size=200, shuffle=True, drop_last=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    fmnist_validset, batch_size=200\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    fmnist_testset, batch_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1747032143441,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "BlkgCvrKJ2B1"
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# 모델 정의\n",
    "#########################\n",
    "class FMNISTModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(28*28, 512)\n",
    "        self.lr2 = nn.Linear(512, 256)\n",
    "        self.lr3 = nn.Linear(256, 128)\n",
    "        self.lr4 = nn.Linear(128, 64)\n",
    "        self.lr5 = nn.Linear(64, 10) #출력->class개수\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: 입력 tensor - shape: [batch_size, 1, 28, 28]\n",
    "        \"\"\"\n",
    "        # 1. flatten -> 3차원 입력을 1차원으로 변환.\n",
    "        # torch.flatten(X, start_dim=1)\n",
    "        X = nn.Flatten()(X) # 1축 이후를 flatten\n",
    "\n",
    "        X = self.lr1(X)\n",
    "        X = self.relu(X)\n",
    "\n",
    "        X = self.lr2(X)\n",
    "        X = self.relu(X)\n",
    "\n",
    "        X = self.lr3(X)\n",
    "        X = self.relu(X)\n",
    "\n",
    "        X = self.lr4(X)\n",
    "        X = self.relu(X)\n",
    "\n",
    "        out = self.lr5(X)  # CrossEntropyLoss() -> Linear 출력결과를 그대로 반환.(Softmax를 적용안함.)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1747032171776,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "T2ogJRZZOyZu",
    "outputId": "1ae91ab7-4fda-4645-ceab-5473873ae793"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FMNISTModel(\n",
       "  (lr1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (lr2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (lr3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lr4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (lr5): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 구조 확인\n",
    "m = FMNISTModel()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1747032274457,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "rppit0M-J15-",
    "outputId": "a8f29d2f-d5a9-405b-8d96-f01978ebfd0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FMNISTModel                              [200, 10]                 --\n",
       "├─Linear: 1-1                            [200, 512]                401,920\n",
       "├─ReLU: 1-2                              [200, 512]                --\n",
       "├─Linear: 1-3                            [200, 256]                131,328\n",
       "├─ReLU: 1-4                              [200, 256]                --\n",
       "├─Linear: 1-5                            [200, 128]                32,896\n",
       "├─ReLU: 1-6                              [200, 128]                --\n",
       "├─Linear: 1-7                            [200, 64]                 8,256\n",
       "├─ReLU: 1-8                              [200, 64]                 --\n",
       "├─Linear: 1-9                            [200, 10]                 650\n",
       "==========================================================================================\n",
       "Total params: 575,050\n",
       "Trainable params: 575,050\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 115.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.63\n",
       "Forward/backward pass size (MB): 1.55\n",
       "Params size (MB): 2.30\n",
       "Estimated Total Size (MB): 4.48\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(m, (200, 1, 28, 28), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747038640225,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "FCt6SbsvJ13Z"
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# 학습\n",
    "##################\n",
    "LR = 0.001\n",
    "EPOCHS = 20\n",
    "\n",
    "fmnist_model = FMNISTModel().to(device)\n",
    "optimizer = torch.optim.Adam(fmnist_model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# 다중 분류 loss: categorical crossentropy\n",
    "## CrossEntropyLoss:\n",
    "#1. y정답을 one hot encoding\n",
    "#2. pred 예측값에 softmax를 적용\n",
    "#3. categorical crossentrpy 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 208692,
     "status": "ok",
     "timestamp": 1747038850343,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "sElFy1e3J10v",
    "outputId": "5a409454-c03d-4ea6-965e-32cc5aad2353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] train loss: 0.6909856842756271, valid loss: 0.4435706722736359, valid acc: 0.842\n",
      ">>>>>>>>>>1 에폭에서 저장. 0.4435706722736359에서 0.4435706722736359만큼 validation loss가 개선됨.\n",
      "[2/100] train loss: 0.41600893962383273, valid loss: 0.3957969707250595, valid acc: 0.8572\n",
      ">>>>>>>>>>2 에폭에서 저장. 0.3957969707250595에서 0.3957969707250595만큼 validation loss가 개선됨.\n",
      "[3/100] train loss: 0.36363321864604947, valid loss: 0.37852869093418123, valid acc: 0.8629\n",
      ">>>>>>>>>>3 에폭에서 저장. 0.37852869093418123에서 0.37852869093418123만큼 validation loss가 개선됨.\n",
      "[4/100] train loss: 0.3358773491382599, valid loss: 0.3475519070029259, valid acc: 0.8766\n",
      ">>>>>>>>>>4 에폭에서 저장. 0.3475519070029259에서 0.3475519070029259만큼 validation loss가 개선됨.\n",
      "[5/100] train loss: 0.3129865292906761, valid loss: 0.32284657537937167, valid acc: 0.8831\n",
      ">>>>>>>>>>5 에폭에서 저장. 0.32284657537937167에서 0.32284657537937167만큼 validation loss가 개선됨.\n",
      "[6/100] train loss: 0.29860647362470627, valid loss: 0.35946077823638917, valid acc: 0.8712\n",
      "[7/100] train loss: 0.2894183790385723, valid loss: 0.32858614772558215, valid acc: 0.8807\n",
      "[8/100] train loss: 0.2750513117313385, valid loss: 0.337883303463459, valid acc: 0.8755\n",
      "[9/100] train loss: 0.2614323979020119, valid loss: 0.30635296493768693, valid acc: 0.8923\n",
      ">>>>>>>>>>9 에폭에서 저장. 0.30635296493768693에서 0.30635296493768693만큼 validation loss가 개선됨.\n",
      "[10/100] train loss: 0.25113477373123166, valid loss: 0.3135702630877495, valid acc: 0.8894\n",
      "[11/100] train loss: 0.2441569816172123, valid loss: 0.3541478282213211, valid acc: 0.8764\n",
      "[12/100] train loss: 0.2335461930334568, valid loss: 0.2962196233868599, valid acc: 0.8959\n",
      ">>>>>>>>>>12 에폭에서 저장. 0.2962196233868599에서 0.2962196233868599만큼 validation loss가 개선됨.\n",
      "[13/100] train loss: 0.22428283441066743, valid loss: 0.30650457948446275, valid acc: 0.8933\n",
      "[14/100] train loss: 0.21382410761713982, valid loss: 0.31456234395503996, valid acc: 0.8951\n",
      "[15/100] train loss: 0.2075494408905506, valid loss: 0.31260656923055646, valid acc: 0.8973\n",
      "[16/100] train loss: 0.2012068432867527, valid loss: 0.30042265623807907, valid acc: 0.8981\n",
      "[17/100] train loss: 0.19669565337896347, valid loss: 0.31370237439870835, valid acc: 0.893\n",
      "============17에폭에서 조기 종료\n",
      "학습에 걸린 총 시간: 208.6844425201416초\n"
     ]
    }
   ],
   "source": [
    "########### 학습\n",
    "\n",
    "import time\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "valid_acc_list = []\n",
    "\n",
    "#######################################\n",
    "# 1. 학습 도중 이전 epoch들 보다 성능이 개선되면 모델을 저장\n",
    "# 2. 특정 epoch 동안 성능 개선이 없으면 학습을 중지\n",
    "#######################################\n",
    "# 변수들 선언\n",
    "import os\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "saved_path = \"saved_models/fashion_mnist_model.pt\"\n",
    "best_score = torch.inf # 현재 가장 좋은 평가지표(loss).\n",
    "#  best_score > 검증loss (성능개선) -> 모델 저장.\n",
    "\n",
    "## 조기종료 관련 변수\n",
    "patience = 5 # 성능이 개선될지 몇 에폭을 기다릴지.\n",
    "trigger_cnt = 0 # 몇 에폭째 기다리는지 저장할 변수.\n",
    "\n",
    "\n",
    "s = time.time()\n",
    "\n",
    "####### train 코드 작성.\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    # 학습\n",
    "    fmnist_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_train, y_train in train_loader:\n",
    "        # device로 이동\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "        # 추론\n",
    "        pred_train = fmnist_model(X_train)\n",
    "\n",
    "        # loss 계산\n",
    "        loss = loss_fn(pred_train, y_train)\n",
    "\n",
    "        # gradient 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    # 검증\n",
    "    fmnist_model.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_valid, y_valid in valid_loader:\n",
    "            # 이동\n",
    "            X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n",
    "\n",
    "            # 추론\n",
    "            pred_valid = fmnist_model(X_valid)\n",
    "\n",
    "            # 검증\n",
    "            ## loss\n",
    "            valid_loss += loss_fn(pred_valid, y_valid).item()\n",
    "            ## 정확도\n",
    "            # 모델이 예측한 정답 class -> 10개 출력값 중 가장 큰 값이 있는 index\n",
    "            pred_class = pred_valid.argmax(dim=-1)\n",
    "            valid_acc += torch.sum(pred_class == y_valid).item()\n",
    "\n",
    "        valid_loss /= len(valid_loader) # step수로 나눠서 평균계산.\n",
    "        valid_acc /= len(fmnist_validset) # 총 데이터개수\n",
    "        valid_loss_list.append(valid_loss)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "\n",
    "        print(f\"[{epoch+1}/{EPOCHS}] train loss: {train_loss}, valid loss: {valid_loss}, valid acc: {valid_acc}\")\n",
    "\n",
    "    ## 한 에폭 학습/검증 종료 후 모델 저장(성능개선시),\n",
    "    #                                        조기종료(성능개선이 안된 경우)\n",
    "    if valid_loss < best_score: # 이전 에폭의 결과보다 성능 개선됨.\n",
    "        # 모델 저장\n",
    "        torch.save(fmnist_model, saved_path)\n",
    "        best_score = valid_loss\n",
    "        print(f\">>>>>>>>>>{epoch+1} 에폭에서 저장. {best_score}에서 {valid_loss}만큼 validation loss가 개선됨.\")\n",
    "        trigger_cnt = 0\n",
    "        # 조기종료를 위해서 성능개선을 기다리는 에폭수. 성능이 개선됐으므로 0으로 초기화\n",
    "    else:\n",
    "        # 조기종료 처리.\n",
    "        trigger_cnt += 1\n",
    "        if patience == trigger_cnt:\n",
    "            print(f\"============{epoch+1}에폭에서 조기 종료\")\n",
    "            break\n",
    "\n",
    "e = time.time()\n",
    "print(f'학습에 걸린 총 시간: {e-s}초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 1997,
     "status": "ok",
     "timestamp": 1747037343922,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "3bCCkCu0J1x9",
    "outputId": "76d4fd1d-c3fb-4475-a5ef-db5415bfcaae"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 결과 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(EPOCHS), train_loss_list, label=\"train loss\")\n",
    "plt.plot(range(EPOCHS), valid_loss_list, label=\"valid loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(EPOCHS), valid_acc_list, label='valid loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747037482544,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "Z0EbqU67J1vX",
    "outputId": "4519e4a8-00c0-49df-8754-dd5869e972b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.2961127930879593), np.int64(8))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.min(valid_loss_list), np.argmin(valid_loss_list)\n",
    "\n",
    "(np.float64(0.2961127930879593), np.int64(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "executionInfo": {
     "elapsed": 3209,
     "status": "ok",
     "timestamp": 1747039130639,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "nZFkUKrSJ1s7"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# 최종 평가\n",
    "###########################\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    for X_test, y_test in test_loader:\n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        pred_test = fmnist_model(X_test)\n",
    "        test_loss += loss_fn(pred_test, y_test).item()\n",
    "        test_acc += torch.sum(pred_test.argmax(dim=-1) == y_test).item()\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc /= len(fmnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1747039136926,
     "user": {
      "displayName": "Sunghwan KIM",
      "userId": "06010856989212311727"
     },
     "user_tz": -540
    },
    "id": "Vp6Q9gD2J1qL",
    "outputId": "450304eb-86ab-4ca1-e5d2-b19efa3c663d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.349494885802269\n",
      "0.8825\n"
     ]
    }
   ],
   "source": [
    "print(test_loss)\n",
    "print(test_acc)\n",
    "# 0.349494885802269\n",
    "# 0.8825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dYDW-wFQJ1lg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FMNISTModel(\n",
       "  (lr1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (lr2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (lr3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lr4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (lr5): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_fmodel = torch.load(\"fashion_mnist_model.pt\", weights_only=False)\n",
    "load_fmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 1, 28, 28]), torch.Size([50]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(fmnist_testset, batch_size=50)\n",
    "x, y = next(iter(dataloader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred_logit = load_fmodel(x)\n",
    "    result = torch.max(pred_logit, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.6165, 10.2547, 17.5566, 15.8298,  5.5867, 12.9642, 12.3320, 12.8852,\n",
      "        12.3735,  7.2927,  5.8394,  8.4934,  5.0595,  6.3468,  5.9614,  9.6942,\n",
      "         8.1569,  6.4463, 11.1379,  5.2048,  3.2624,  4.3939,  7.9788,  5.0826,\n",
      "        16.5399,  5.5271,  7.8283,  4.4728,  5.8460,  2.2570, 18.4551, 14.1004,\n",
      "         4.2600,  3.6490, 22.8172,  8.0234,  6.0692,  9.5096,  7.7339, 10.2880,\n",
      "         6.9833, 14.9835,  5.2897,  5.6794,  8.4235,  5.0249,  3.5945,  8.6214,\n",
      "         5.4602,  4.6217])\n"
     ]
    }
   ],
   "source": [
    "print(result.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.9906), tensor(9))\n",
      "(tensor(0.9958), tensor(2))\n",
      "(tensor(1.), tensor(1))\n",
      "(tensor(1.), tensor(1))\n",
      "(tensor(0.7736), tensor(6))\n",
      "(tensor(1.0000), tensor(1))\n",
      "(tensor(0.9996), tensor(4))\n",
      "(tensor(0.9994), tensor(6))\n",
      "(tensor(1.0000), tensor(5))\n",
      "(tensor(0.9984), tensor(7))\n",
      "(tensor(0.9165), tensor(4))\n",
      "(tensor(0.9999), tensor(5))\n",
      "(tensor(0.5519), tensor(7))\n",
      "(tensor(0.9979), tensor(3))\n",
      "(tensor(0.8424), tensor(4))\n",
      "(tensor(1.0000), tensor(1))\n",
      "(tensor(0.9948), tensor(2))\n",
      "(tensor(0.8746), tensor(2))\n",
      "(tensor(1.0000), tensor(8))\n",
      "(tensor(0.9471), tensor(0))\n",
      "(tensor(0.7399), tensor(2))\n",
      "(tensor(0.6761), tensor(5))\n",
      "(tensor(0.9997), tensor(7))\n",
      "(tensor(0.9855), tensor(5))\n",
      "(tensor(1.), tensor(1))\n",
      "(tensor(0.7187), tensor(2))\n",
      "(tensor(0.9264), tensor(6))\n",
      "(tensor(0.8197), tensor(0))\n",
      "(tensor(0.9780), tensor(9))\n",
      "(tensor(0.4040), tensor(3))\n",
      "(tensor(1.), tensor(8))\n",
      "(tensor(1.0000), tensor(8))\n",
      "(tensor(0.9744), tensor(3))\n",
      "(tensor(0.8790), tensor(3))\n",
      "(tensor(1.), tensor(8))\n",
      "(tensor(0.9943), tensor(0))\n",
      "(tensor(0.9982), tensor(7))\n",
      "(tensor(0.9999), tensor(5))\n",
      "(tensor(0.9995), tensor(7))\n",
      "(tensor(0.9999), tensor(9))\n",
      "(tensor(0.9350), tensor(0))\n",
      "(tensor(1.), tensor(1))\n",
      "(tensor(0.8963), tensor(6))\n",
      "(tensor(0.9547), tensor(7))\n",
      "(tensor(0.9171), tensor(6))\n",
      "(tensor(0.9510), tensor(7))\n",
      "(tensor(0.7934), tensor(2))\n",
      "(tensor(0.9999), tensor(1))\n",
      "(tensor(0.4417), tensor(2))\n",
      "(tensor(0.8384), tensor(6))\n"
     ]
    }
   ],
   "source": [
    "# 확률값으로 변환 - softmax\n",
    "pred_proba = nn.Softmax(dim=-1)(pred_logit)\n",
    "r = torch.max(pred_proba, dim=-1)\n",
    "for a in zip(r.values, r.indices):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "        pred_proba = nn.Softmax(dim=-1)(pred)\n",
    "        result = torch.max(pred_proba, dim=-1)\n",
    "        return [(idx.item(), proba.item()) for idx, proba in zip(result.indices, result.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(load_fmodel, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 0.9905555844306946),\n",
       " (2, 0.9958310723304749),\n",
       " (1, 1.0),\n",
       " (1, 1.0),\n",
       " (6, 0.7735845446586609),\n",
       " (1, 0.9999996423721313),\n",
       " (4, 0.9995827078819275),\n",
       " (6, 0.9994454979896545),\n",
       " (5, 0.9999996423721313),\n",
       " (7, 0.9984103441238403),\n",
       " (4, 0.9164848923683167),\n",
       " (5, 0.9999071359634399),\n",
       " (7, 0.5518661737442017),\n",
       " (3, 0.9979277849197388),\n",
       " (4, 0.8423774838447571),\n",
       " (1, 0.9999840259552002),\n",
       " (2, 0.994787335395813),\n",
       " (2, 0.8745715022087097),\n",
       " (8, 0.9999815225601196),\n",
       " (0, 0.9470783472061157),\n",
       " (2, 0.7399144768714905),\n",
       " (5, 0.6761211156845093),\n",
       " (7, 0.9997068047523499),\n",
       " (5, 0.9854647517204285),\n",
       " (1, 1.0),\n",
       " (2, 0.7186679840087891),\n",
       " (6, 0.9263898134231567),\n",
       " (0, 0.819709300994873),\n",
       " (9, 0.9779600501060486),\n",
       " (3, 0.4039537310600281),\n",
       " (8, 1.0),\n",
       " (8, 0.9999992847442627),\n",
       " (3, 0.9743779897689819),\n",
       " (3, 0.8789570927619934),\n",
       " (8, 1.0),\n",
       " (0, 0.9943010807037354),\n",
       " (7, 0.9981901049613953),\n",
       " (5, 0.9999299049377441),\n",
       " (7, 0.9994679093360901),\n",
       " (9, 0.9998764991760254),\n",
       " (0, 0.9349932670593262),\n",
       " (1, 1.0),\n",
       " (6, 0.8962815403938293),\n",
       " (7, 0.954703688621521),\n",
       " (6, 0.9171225428581238),\n",
       " (7, 0.9510108232498169),\n",
       " (2, 0.793440580368042),\n",
       " (1, 0.9999381303787231),\n",
       " (2, 0.44173288345336914),\n",
       " (6, 0.8384090662002563)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_p96kBHFIUqj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZfB5eJ_IUqj"
   },
   "source": [
    "### 위스콘신 유방암 데이터셋 - **이진분류(Binary Classification) 문제**\n",
    "\n",
    "-   **이진 분류 문제 처리 모델의 두가지 방법**\n",
    "    1. positive(1)일 확률을 출력하도록 구현\n",
    "        - output layer: units=1, activation='sigmoid'\n",
    "        - loss: binary_crossentropy\n",
    "    2. negative(0)일 확률과 positive(1)일 확률을 출력하도록 구현 => 다중분류 처리 방식으로 해결\n",
    "        - output layer: units=2, activation='softmax', y(정답)은 one hot encoding 처리\n",
    "        - loss: categorical_crossentropy\n",
    "-   위스콘신 대학교에서 제공한 종양의 악성/양성여부 분류를 위한 데이터셋\n",
    "-   Feature\n",
    "    -   종양에 대한 다양한 측정값들\n",
    "-   Target의 class\n",
    "    -   0 - malignant(악성종양)\n",
    "    -   1 - benign(양성종양)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "k-d7GmezIUqj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "sUUsUobQIUqj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569,)\n",
      "(569, 1)\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "print(y.shape)\n",
    "y = y.reshape(-1, 1)\n",
    "print(y.shape)\n",
    "# X.shape, y.shape, X.dtype\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "zFL165uGIUqj"
   },
   "outputs": [],
   "source": [
    "# 전처리\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "KEqB9PtHIUqj"
   },
   "outputs": [],
   "source": [
    "# class name <-> class index\n",
    "classes = np.array([\"악성종양\", \"양성종양\"])\n",
    "class_to_idx = {\"악성종양\":0, \"양성종양\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "flZUWWGjIUqj"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "## 모델의 weight, bias -> float32. X, y는 weight, bias와 계산을 하게 되기 때문에 타입을 맞춰준다.\n",
    "trainset = TensorDataset(\n",
    "    torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "testset = TensorDataset(\n",
    "    torch.tensor(X_test_scaled, dtype=torch.float32),\n",
    "    torch.tensor(y_test, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "lRLgIofiIUqj"
   },
   "outputs": [],
   "source": [
    "trainset.classes = classes\n",
    "trainset.class_to_idx = class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "k514HWtxIUqj"
   },
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = DataLoader(trainset, batch_size=200, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(testset, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "OwjXxjpcIUqj"
   },
   "outputs": [],
   "source": [
    "######### 모델 정의\n",
    "class BreastCancerModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(30, 32)\n",
    "        self.lr2 = nn.Linear(32, 8)\n",
    "        self.lr3 = nn.Linear(8, 1) # 출력 Layer 처리하는 함수. out_features=1 : positive일 확률.\n",
    "        self.relu = nn.ReLU()\n",
    "        self.logistic = nn.Sigmoid() # 입력값을 0 ~ 1 사이 실수로 반환.\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.lr1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lr2(X)\n",
    "        X = self.relu(X)\n",
    "        # 출력 Layer\n",
    "        output = self.lr3(X)\n",
    "        output = self.logistic(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "npYL5Yp8IUqj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BreastCancerModel(\n",
       "  (lr1): Linear(in_features=30, out_features=32, bias=True)\n",
       "  (lr2): Linear(in_features=32, out_features=8, bias=True)\n",
       "  (lr3): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (logistic): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_model = BreastCancerModel()\n",
    "b_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "QW-EUpFvIUqj",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BreastCancerModel                        [10, 1]                   --\n",
       "├─Linear: 1-1                            [10, 32]                  992\n",
       "├─ReLU: 1-2                              [10, 32]                  --\n",
       "├─Linear: 1-3                            [10, 8]                   264\n",
       "├─ReLU: 1-4                              [10, 8]                   --\n",
       "├─Linear: 1-5                            [10, 1]                   9\n",
       "├─Sigmoid: 1-6                           [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 1,265\n",
       "Trainable params: 1,265\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(b_model, (10, 30), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "A6uuJV8wIUqj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4373],\n",
       "        [0.4188],\n",
       "        [0.4377],\n",
       "        [0.4230],\n",
       "        [0.4423],\n",
       "        [0.4313],\n",
       "        [0.4651],\n",
       "        [0.4278],\n",
       "        [0.4373],\n",
       "        [0.4494]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy data로 출력\n",
    "dummy_x = torch.randn(10, 30)\n",
    "# dummy_x.shape\n",
    "result = b_model(dummy_x)\n",
    "print(result.shape)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "2vFqCMGsIUqk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(result > 0.5).type(torch.int32)  # bool -> int (True: 1, False: 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "xECcnrcyIUqk"
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 1000\n",
    "######## 학습(train)\n",
    "b_model = b_model.to(device)\n",
    "optimizer = optim.Adam(b_model.parameters(), lr=lr)\n",
    "loss_fn = nn.BCELoss()   # 함수이름: binary crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "93vfwS6sIUqk",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0001/1000] train loss: 0.724372386932373, valid loss: 0.7299314439296722, valid accuracy: 0.3706293706293706\n",
      ">>>>>> 1에서 성능이 개선되어 저장합니다. 0.7299314439296722\n",
      "[0002/1000] train loss: 0.7119273543357849, valid loss: 0.720769077539444, valid accuracy: 0.3706293706293706\n",
      ">>>>>> 2에서 성능이 개선되어 저장합니다. 0.720769077539444\n",
      "[0003/1000] train loss: 0.7080086171627045, valid loss: 0.7118813097476959, valid accuracy: 0.3706293706293706\n",
      ">>>>>> 3에서 성능이 개선되어 저장합니다. 0.7118813097476959\n",
      "[0004/1000] train loss: 0.6965263187885284, valid loss: 0.7033416032791138, valid accuracy: 0.3706293706293706\n",
      ">>>>>> 4에서 성능이 개선되어 저장합니다. 0.7033416032791138\n",
      "[0005/1000] train loss: 0.6875647604465485, valid loss: 0.6950466632843018, valid accuracy: 0.3706293706293706\n",
      ">>>>>> 5에서 성능이 개선되어 저장합니다. 0.6950466632843018\n",
      "[0006/1000] train loss: 0.6800101697444916, valid loss: 0.6866016089916229, valid accuracy: 0.3706293706293706\n",
      ">>>>>> 6에서 성능이 개선되어 저장합니다. 0.6866016089916229\n",
      "[0007/1000] train loss: 0.6731688380241394, valid loss: 0.6775385439395905, valid accuracy: 0.38461538461538464\n",
      ">>>>>> 7에서 성능이 개선되어 저장합니다. 0.6775385439395905\n",
      "[0008/1000] train loss: 0.6622574627399445, valid loss: 0.6679334342479706, valid accuracy: 0.44755244755244755\n",
      ">>>>>> 8에서 성능이 개선되어 저장합니다. 0.6679334342479706\n",
      "[0009/1000] train loss: 0.6525880992412567, valid loss: 0.6579507887363434, valid accuracy: 0.5664335664335665\n",
      ">>>>>> 9에서 성능이 개선되어 저장합니다. 0.6579507887363434\n",
      "[0010/1000] train loss: 0.6435746848583221, valid loss: 0.6478294134140015, valid accuracy: 0.6293706293706294\n",
      ">>>>>> 10에서 성능이 개선되어 저장합니다. 0.6478294134140015\n",
      "[0011/1000] train loss: 0.6336948573589325, valid loss: 0.637403130531311, valid accuracy: 0.7482517482517482\n",
      ">>>>>> 11에서 성능이 개선되어 저장합니다. 0.637403130531311\n",
      "[0012/1000] train loss: 0.6230511963367462, valid loss: 0.6267796456813812, valid accuracy: 0.8041958041958042\n",
      ">>>>>> 12에서 성능이 개선되어 저장합니다. 0.6267796456813812\n",
      "[0013/1000] train loss: 0.6111548244953156, valid loss: 0.6158474087715149, valid accuracy: 0.8251748251748252\n",
      ">>>>>> 13에서 성능이 개선되어 저장합니다. 0.6158474087715149\n",
      "[0014/1000] train loss: 0.599577784538269, valid loss: 0.6046806871891022, valid accuracy: 0.8461538461538461\n",
      ">>>>>> 14에서 성능이 개선되어 저장합니다. 0.6046806871891022\n",
      "[0015/1000] train loss: 0.5857094526290894, valid loss: 0.5932759940624237, valid accuracy: 0.8601398601398601\n",
      ">>>>>> 15에서 성능이 개선되어 저장합니다. 0.5932759940624237\n",
      "[0016/1000] train loss: 0.576123982667923, valid loss: 0.5816324055194855, valid accuracy: 0.8741258741258742\n",
      ">>>>>> 16에서 성능이 개선되어 저장합니다. 0.5816324055194855\n",
      "[0017/1000] train loss: 0.5664035975933075, valid loss: 0.5696928799152374, valid accuracy: 0.8741258741258742\n",
      ">>>>>> 17에서 성능이 개선되어 저장합니다. 0.5696928799152374\n",
      "[0018/1000] train loss: 0.5515187680721283, valid loss: 0.5574518144130707, valid accuracy: 0.8741258741258742\n",
      ">>>>>> 18에서 성능이 개선되어 저장합니다. 0.5574518144130707\n",
      "[0019/1000] train loss: 0.540558785200119, valid loss: 0.5449617207050323, valid accuracy: 0.8741258741258742\n",
      ">>>>>> 19에서 성능이 개선되어 저장합니다. 0.5449617207050323\n",
      "[0020/1000] train loss: 0.5261410176753998, valid loss: 0.5321513116359711, valid accuracy: 0.8951048951048951\n",
      ">>>>>> 20에서 성능이 개선되어 저장합니다. 0.5321513116359711\n",
      "[0021/1000] train loss: 0.5110829174518585, valid loss: 0.5190902948379517, valid accuracy: 0.9090909090909091\n",
      ">>>>>> 21에서 성능이 개선되어 저장합니다. 0.5190902948379517\n",
      "[0022/1000] train loss: 0.5016482919454575, valid loss: 0.5057250261306763, valid accuracy: 0.916083916083916\n",
      ">>>>>> 22에서 성능이 개선되어 저장합니다. 0.5057250261306763\n",
      "[0023/1000] train loss: 0.4845881760120392, valid loss: 0.4917201101779938, valid accuracy: 0.916083916083916\n",
      ">>>>>> 23에서 성능이 개선되어 저장합니다. 0.4917201101779938\n",
      "[0024/1000] train loss: 0.4669090360403061, valid loss: 0.4771054983139038, valid accuracy: 0.916083916083916\n",
      ">>>>>> 24에서 성능이 개선되어 저장합니다. 0.4771054983139038\n",
      "[0025/1000] train loss: 0.45412296056747437, valid loss: 0.46224212646484375, valid accuracy: 0.916083916083916\n",
      ">>>>>> 25에서 성능이 개선되어 저장합니다. 0.46224212646484375\n",
      "[0026/1000] train loss: 0.4385755807161331, valid loss: 0.44715258479118347, valid accuracy: 0.9230769230769231\n",
      ">>>>>> 26에서 성능이 개선되어 저장합니다. 0.44715258479118347\n",
      "[0027/1000] train loss: 0.42233602702617645, valid loss: 0.4320504665374756, valid accuracy: 0.9230769230769231\n",
      ">>>>>> 27에서 성능이 개선되어 저장합니다. 0.4320504665374756\n",
      "[0028/1000] train loss: 0.4020862728357315, valid loss: 0.4171338379383087, valid accuracy: 0.9230769230769231\n",
      ">>>>>> 28에서 성능이 개선되어 저장합니다. 0.4171338379383087\n",
      "[0029/1000] train loss: 0.38822825253009796, valid loss: 0.40228012204170227, valid accuracy: 0.9230769230769231\n",
      ">>>>>> 29에서 성능이 개선되어 저장합니다. 0.40228012204170227\n",
      "[0030/1000] train loss: 0.3716118633747101, valid loss: 0.38771215081214905, valid accuracy: 0.9230769230769231\n",
      ">>>>>> 30에서 성능이 개선되어 저장합니다. 0.38771215081214905\n",
      "[0031/1000] train loss: 0.3561229705810547, valid loss: 0.3733423501253128, valid accuracy: 0.9230769230769231\n",
      ">>>>>> 31에서 성능이 개선되어 저장합니다. 0.3733423501253128\n",
      "[0032/1000] train loss: 0.33802416920661926, valid loss: 0.3592580556869507, valid accuracy: 0.9230769230769231\n",
      ">>>>>> 32에서 성능이 개선되어 저장합니다. 0.3592580556869507\n",
      "[0033/1000] train loss: 0.3327280580997467, valid loss: 0.34546010196208954, valid accuracy: 0.9230769230769231\n",
      ">>>>>> 33에서 성능이 개선되어 저장합니다. 0.34546010196208954\n",
      "[0034/1000] train loss: 0.31092338263988495, valid loss: 0.3320017606019974, valid accuracy: 0.9230769230769231\n",
      ">>>>>> 34에서 성능이 개선되어 저장합니다. 0.3320017606019974\n",
      "[0035/1000] train loss: 0.30110570788383484, valid loss: 0.31887970864772797, valid accuracy: 0.9230769230769231\n",
      ">>>>>> 35에서 성능이 개선되어 저장합니다. 0.31887970864772797\n",
      "[0036/1000] train loss: 0.28891438245773315, valid loss: 0.3062862455844879, valid accuracy: 0.9230769230769231\n",
      ">>>>>> 36에서 성능이 개선되어 저장합니다. 0.3062862455844879\n",
      "[0037/1000] train loss: 0.27194221317768097, valid loss: 0.29421569406986237, valid accuracy: 0.9230769230769231\n",
      ">>>>>> 37에서 성능이 개선되어 저장합니다. 0.29421569406986237\n",
      "[0038/1000] train loss: 0.2628202587366104, valid loss: 0.2827466130256653, valid accuracy: 0.9230769230769231\n",
      ">>>>>> 38에서 성능이 개선되어 저장합니다. 0.2827466130256653\n",
      "[0039/1000] train loss: 0.25389809161424637, valid loss: 0.2717965096235275, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 39에서 성능이 개선되어 저장합니다. 0.2717965096235275\n",
      "[0040/1000] train loss: 0.24280854314565659, valid loss: 0.2614286094903946, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 40에서 성능이 개선되어 저장합니다. 0.2614286094903946\n",
      "[0041/1000] train loss: 0.23190287500619888, valid loss: 0.25159965455532074, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 41에서 성능이 개선되어 저장합니다. 0.25159965455532074\n",
      "[0042/1000] train loss: 0.21883543580770493, valid loss: 0.2423028200864792, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 42에서 성능이 개선되어 저장합니다. 0.2423028200864792\n",
      "[0043/1000] train loss: 0.20192711800336838, valid loss: 0.23361323028802872, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 43에서 성능이 개선되어 저장합니다. 0.23361323028802872\n",
      "[0044/1000] train loss: 0.20005109161138535, valid loss: 0.22545556724071503, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 44에서 성능이 개선되어 저장합니다. 0.22545556724071503\n",
      "[0045/1000] train loss: 0.19350534677505493, valid loss: 0.21775516867637634, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 45에서 성능이 개선되어 저장합니다. 0.21775516867637634\n",
      "[0046/1000] train loss: 0.18487218767404556, valid loss: 0.21054360270500183, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 46에서 성능이 개선되어 저장합니다. 0.21054360270500183\n",
      "[0047/1000] train loss: 0.17623195052146912, valid loss: 0.20374495536088943, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 47에서 성능이 개선되어 저장합니다. 0.20374495536088943\n",
      "[0048/1000] train loss: 0.1639932617545128, valid loss: 0.19736452400684357, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 48에서 성능이 개선되어 저장합니다. 0.19736452400684357\n",
      "[0049/1000] train loss: 0.1644623950123787, valid loss: 0.19130226224660873, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 49에서 성능이 개선되어 저장합니다. 0.19130226224660873\n",
      "[0050/1000] train loss: 0.1556476727128029, valid loss: 0.18559282273054123, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 50에서 성능이 개선되어 저장합니다. 0.18559282273054123\n",
      "[0051/1000] train loss: 0.15131472796201706, valid loss: 0.18024563044309616, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 51에서 성능이 개선되어 저장합니다. 0.18024563044309616\n",
      "[0052/1000] train loss: 0.14515108615159988, valid loss: 0.17533717304468155, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 52에서 성능이 개선되어 저장합니다. 0.17533717304468155\n",
      "[0053/1000] train loss: 0.13282399624586105, valid loss: 0.17081017047166824, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 53에서 성능이 개선되어 저장합니다. 0.17081017047166824\n",
      "[0054/1000] train loss: 0.1352345272898674, valid loss: 0.1665678471326828, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 54에서 성능이 개선되어 저장합니다. 0.1665678471326828\n",
      "[0055/1000] train loss: 0.12466342747211456, valid loss: 0.16260214895009995, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 55에서 성능이 개선되어 저장합니다. 0.16260214895009995\n",
      "[0056/1000] train loss: 0.12768788635730743, valid loss: 0.1589316502213478, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 56에서 성능이 개선되어 저장합니다. 0.1589316502213478\n",
      "[0057/1000] train loss: 0.12365379184484482, valid loss: 0.15556498616933823, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 57에서 성능이 개선되어 저장합니다. 0.15556498616933823\n",
      "[0058/1000] train loss: 0.11961201205849648, valid loss: 0.15244053304195404, valid accuracy: 0.9300699300699301\n",
      ">>>>>> 58에서 성능이 개선되어 저장합니다. 0.15244053304195404\n",
      "[0059/1000] train loss: 0.11167586222290993, valid loss: 0.14952289313077927, valid accuracy: 0.9300699300699301\n",
      ">>>>>> 59에서 성능이 개선되어 저장합니다. 0.14952289313077927\n",
      "[0060/1000] train loss: 0.11164741963148117, valid loss: 0.1468009278178215, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 60에서 성능이 개선되어 저장합니다. 0.1468009278178215\n",
      "[0061/1000] train loss: 0.11054813489317894, valid loss: 0.14428798854351044, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 61에서 성능이 개선되어 저장합니다. 0.14428798854351044\n",
      "[0062/1000] train loss: 0.1044924184679985, valid loss: 0.14193733036518097, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 62에서 성능이 개선되어 저장합니다. 0.14193733036518097\n",
      "[0063/1000] train loss: 0.10070817917585373, valid loss: 0.13978300988674164, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 63에서 성능이 개선되어 저장합니다. 0.13978300988674164\n",
      "[0064/1000] train loss: 0.09786088019609451, valid loss: 0.1377575322985649, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 64에서 성능이 개선되어 저장합니다. 0.1377575322985649\n",
      "[0065/1000] train loss: 0.09752171486616135, valid loss: 0.1358911693096161, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 65에서 성능이 개선되어 저장합니다. 0.1358911693096161\n",
      "[0066/1000] train loss: 0.09614427015185356, valid loss: 0.13411309570074081, valid accuracy: 0.9370629370629371\n",
      ">>>>>> 66에서 성능이 개선되어 저장합니다. 0.13411309570074081\n",
      "[0067/1000] train loss: 0.08847380802035332, valid loss: 0.13240225613117218, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 67에서 성능이 개선되어 저장합니다. 0.13240225613117218\n",
      "[0068/1000] train loss: 0.08712019771337509, valid loss: 0.13077621161937714, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 68에서 성능이 개선되어 저장합니다. 0.13077621161937714\n",
      "[0069/1000] train loss: 0.08665928244590759, valid loss: 0.12925028055906296, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 69에서 성능이 개선되어 저장합니다. 0.12925028055906296\n",
      "[0070/1000] train loss: 0.07670937478542328, valid loss: 0.12780262157320976, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 70에서 성능이 개선되어 저장합니다. 0.12780262157320976\n",
      "[0071/1000] train loss: 0.07985718920826912, valid loss: 0.12641435489058495, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 71에서 성능이 개선되어 저장합니다. 0.12641435489058495\n",
      "[0072/1000] train loss: 0.08411913365125656, valid loss: 0.1251642145216465, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 72에서 성능이 개선되어 저장합니다. 0.1251642145216465\n",
      "[0073/1000] train loss: 0.07670126855373383, valid loss: 0.12394192814826965, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 73에서 성능이 개선되어 저장합니다. 0.12394192814826965\n",
      "[0074/1000] train loss: 0.07299224846065044, valid loss: 0.12280433252453804, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 74에서 성능이 개선되어 저장합니다. 0.12280433252453804\n",
      "[0075/1000] train loss: 0.07724510133266449, valid loss: 0.12177633866667747, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 75에서 성능이 개선되어 저장합니다. 0.12177633866667747\n",
      "[0076/1000] train loss: 0.0739804096519947, valid loss: 0.12083165720105171, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 76에서 성능이 개선되어 저장합니다. 0.12083165720105171\n",
      "[0077/1000] train loss: 0.07528728432953358, valid loss: 0.11994940415024757, valid accuracy: 0.9440559440559441\n",
      ">>>>>> 77에서 성능이 개선되어 저장합니다. 0.11994940415024757\n",
      "[0078/1000] train loss: 0.07525392435491085, valid loss: 0.11911174654960632, valid accuracy: 0.951048951048951\n",
      ">>>>>> 78에서 성능이 개선되어 저장합니다. 0.11911174654960632\n",
      "[0079/1000] train loss: 0.06840403005480766, valid loss: 0.11830228194594383, valid accuracy: 0.951048951048951\n",
      ">>>>>> 79에서 성능이 개선되어 저장합니다. 0.11830228194594383\n",
      "[0080/1000] train loss: 0.07183615118265152, valid loss: 0.1175532266497612, valid accuracy: 0.951048951048951\n",
      ">>>>>> 80에서 성능이 개선되어 저장합니다. 0.1175532266497612\n",
      "[0081/1000] train loss: 0.07056184485554695, valid loss: 0.11680811643600464, valid accuracy: 0.951048951048951\n",
      ">>>>>> 81에서 성능이 개선되어 저장합니다. 0.11680811643600464\n",
      "[0082/1000] train loss: 0.06926699355244637, valid loss: 0.11613671854138374, valid accuracy: 0.958041958041958\n",
      ">>>>>> 82에서 성능이 개선되어 저장합니다. 0.11613671854138374\n",
      "[0083/1000] train loss: 0.06597084552049637, valid loss: 0.11552909389138222, valid accuracy: 0.958041958041958\n",
      ">>>>>> 83에서 성능이 개선되어 저장합니다. 0.11552909389138222\n",
      "[0084/1000] train loss: 0.06277157925069332, valid loss: 0.11497795209288597, valid accuracy: 0.958041958041958\n",
      ">>>>>> 84에서 성능이 개선되어 저장합니다. 0.11497795209288597\n",
      "[0085/1000] train loss: 0.06412131153047085, valid loss: 0.11447206884622574, valid accuracy: 0.958041958041958\n",
      ">>>>>> 85에서 성능이 개선되어 저장합니다. 0.11447206884622574\n",
      "[0086/1000] train loss: 0.06587905064225197, valid loss: 0.11395074799656868, valid accuracy: 0.958041958041958\n",
      ">>>>>> 86에서 성능이 개선되어 저장합니다. 0.11395074799656868\n",
      "[0087/1000] train loss: 0.060859937220811844, valid loss: 0.11350293457508087, valid accuracy: 0.958041958041958\n",
      ">>>>>> 87에서 성능이 개선되어 저장합니다. 0.11350293457508087\n",
      "[0088/1000] train loss: 0.05395061895251274, valid loss: 0.11303343623876572, valid accuracy: 0.958041958041958\n",
      ">>>>>> 88에서 성능이 개선되어 저장합니다. 0.11303343623876572\n",
      "[0089/1000] train loss: 0.061969341710209846, valid loss: 0.1126365214586258, valid accuracy: 0.958041958041958\n",
      ">>>>>> 89에서 성능이 개선되어 저장합니다. 0.1126365214586258\n",
      "[0090/1000] train loss: 0.05984719470143318, valid loss: 0.11226010322570801, valid accuracy: 0.958041958041958\n",
      ">>>>>> 90에서 성능이 개선되어 저장합니다. 0.11226010322570801\n",
      "[0091/1000] train loss: 0.06049131415784359, valid loss: 0.11192295700311661, valid accuracy: 0.965034965034965\n",
      ">>>>>> 91에서 성능이 개선되어 저장합니다. 0.11192295700311661\n",
      "[0092/1000] train loss: 0.05693278834223747, valid loss: 0.11156240850687027, valid accuracy: 0.965034965034965\n",
      ">>>>>> 92에서 성능이 개선되어 저장합니다. 0.11156240850687027\n",
      "[0093/1000] train loss: 0.0585593543946743, valid loss: 0.11123767122626305, valid accuracy: 0.965034965034965\n",
      ">>>>>> 93에서 성능이 개선되어 저장합니다. 0.11123767122626305\n",
      "[0094/1000] train loss: 0.057798316702246666, valid loss: 0.11090506613254547, valid accuracy: 0.958041958041958\n",
      ">>>>>> 94에서 성능이 개선되어 저장합니다. 0.11090506613254547\n",
      "[0095/1000] train loss: 0.0559590682387352, valid loss: 0.11060943827033043, valid accuracy: 0.958041958041958\n",
      ">>>>>> 95에서 성능이 개선되어 저장합니다. 0.11060943827033043\n",
      "[0096/1000] train loss: 0.05500954017043114, valid loss: 0.11033343151211739, valid accuracy: 0.958041958041958\n",
      ">>>>>> 96에서 성능이 개선되어 저장합니다. 0.11033343151211739\n",
      "[0097/1000] train loss: 0.05478079617023468, valid loss: 0.11008122563362122, valid accuracy: 0.958041958041958\n",
      ">>>>>> 97에서 성능이 개선되어 저장합니다. 0.11008122563362122\n",
      "[0098/1000] train loss: 0.05326689966022968, valid loss: 0.10983464494347572, valid accuracy: 0.958041958041958\n",
      ">>>>>> 98에서 성능이 개선되어 저장합니다. 0.10983464494347572\n",
      "[0099/1000] train loss: 0.05330382287502289, valid loss: 0.10956382006406784, valid accuracy: 0.958041958041958\n",
      ">>>>>> 99에서 성능이 개선되어 저장합니다. 0.10956382006406784\n",
      "[0100/1000] train loss: 0.054477252066135406, valid loss: 0.10928769037127495, valid accuracy: 0.958041958041958\n",
      ">>>>>> 100에서 성능이 개선되어 저장합니다. 0.10928769037127495\n",
      "[0101/1000] train loss: 0.05176515318453312, valid loss: 0.10903607308864594, valid accuracy: 0.958041958041958\n",
      ">>>>>> 101에서 성능이 개선되어 저장합니다. 0.10903607308864594\n",
      "[0102/1000] train loss: 0.049637679010629654, valid loss: 0.10867328569293022, valid accuracy: 0.958041958041958\n",
      ">>>>>> 102에서 성능이 개선되어 저장합니다. 0.10867328569293022\n",
      "[0103/1000] train loss: 0.05141428858041763, valid loss: 0.10835613310337067, valid accuracy: 0.958041958041958\n",
      ">>>>>> 103에서 성능이 개선되어 저장합니다. 0.10835613310337067\n",
      "[0104/1000] train loss: 0.04921778663992882, valid loss: 0.10808068886399269, valid accuracy: 0.958041958041958\n",
      ">>>>>> 104에서 성능이 개선되어 저장합니다. 0.10808068886399269\n",
      "[0105/1000] train loss: 0.05096007511019707, valid loss: 0.10775185748934746, valid accuracy: 0.958041958041958\n",
      ">>>>>> 105에서 성능이 개선되어 저장합니다. 0.10775185748934746\n",
      "[0106/1000] train loss: 0.033870561979711056, valid loss: 0.1073547825217247, valid accuracy: 0.958041958041958\n",
      ">>>>>> 106에서 성능이 개선되어 저장합니다. 0.1073547825217247\n",
      "[0107/1000] train loss: 0.04920710436999798, valid loss: 0.10703035816550255, valid accuracy: 0.958041958041958\n",
      ">>>>>> 107에서 성능이 개선되어 저장합니다. 0.10703035816550255\n",
      "[0108/1000] train loss: 0.049452370032668114, valid loss: 0.10669498890638351, valid accuracy: 0.958041958041958\n",
      ">>>>>> 108에서 성능이 개선되어 저장합니다. 0.10669498890638351\n",
      "[0109/1000] train loss: 0.0486349742859602, valid loss: 0.10640678182244301, valid accuracy: 0.958041958041958\n",
      ">>>>>> 109에서 성능이 개선되어 저장합니다. 0.10640678182244301\n",
      "[0110/1000] train loss: 0.04409688711166382, valid loss: 0.10622160136699677, valid accuracy: 0.958041958041958\n",
      ">>>>>> 110에서 성능이 개선되어 저장합니다. 0.10622160136699677\n",
      "[0111/1000] train loss: 0.04401959478855133, valid loss: 0.10600006580352783, valid accuracy: 0.958041958041958\n",
      ">>>>>> 111에서 성능이 개선되어 저장합니다. 0.10600006580352783\n",
      "[0112/1000] train loss: 0.04660168942064047, valid loss: 0.10581643506884575, valid accuracy: 0.958041958041958\n",
      ">>>>>> 112에서 성능이 개선되어 저장합니다. 0.10581643506884575\n",
      "[0113/1000] train loss: 0.03999748267233372, valid loss: 0.10565537586808205, valid accuracy: 0.958041958041958\n",
      ">>>>>> 113에서 성능이 개선되어 저장합니다. 0.10565537586808205\n",
      "[0114/1000] train loss: 0.046861919574439526, valid loss: 0.10556751117110252, valid accuracy: 0.958041958041958\n",
      ">>>>>> 114에서 성능이 개선되어 저장합니다. 0.10556751117110252\n",
      "[0115/1000] train loss: 0.04567939043045044, valid loss: 0.10543511435389519, valid accuracy: 0.958041958041958\n",
      ">>>>>> 115에서 성능이 개선되어 저장합니다. 0.10543511435389519\n",
      "[0116/1000] train loss: 0.04153498634696007, valid loss: 0.10538983345031738, valid accuracy: 0.958041958041958\n",
      ">>>>>> 116에서 성능이 개선되어 저장합니다. 0.10538983345031738\n",
      "[0117/1000] train loss: 0.045514089055359364, valid loss: 0.10535336658358574, valid accuracy: 0.958041958041958\n",
      ">>>>>> 117에서 성능이 개선되어 저장합니다. 0.10535336658358574\n",
      "[0118/1000] train loss: 0.045452915132045746, valid loss: 0.10535980761051178, valid accuracy: 0.958041958041958\n",
      "[0119/1000] train loss: 0.041801998391747475, valid loss: 0.1052858754992485, valid accuracy: 0.958041958041958\n",
      ">>>>>> 119에서 성능이 개선되어 저장합니다. 0.1052858754992485\n",
      "[0120/1000] train loss: 0.04483750835061073, valid loss: 0.105233334004879, valid accuracy: 0.958041958041958\n",
      ">>>>>> 120에서 성능이 개선되어 저장합니다. 0.105233334004879\n",
      "[0121/1000] train loss: 0.043406473472714424, valid loss: 0.10517037659883499, valid accuracy: 0.958041958041958\n",
      ">>>>>> 121에서 성능이 개선되어 저장합니다. 0.10517037659883499\n",
      "[0122/1000] train loss: 0.043082039803266525, valid loss: 0.10508808121085167, valid accuracy: 0.958041958041958\n",
      ">>>>>> 122에서 성능이 개선되어 저장합니다. 0.10508808121085167\n",
      "[0123/1000] train loss: 0.042673529125750065, valid loss: 0.10505721345543861, valid accuracy: 0.958041958041958\n",
      ">>>>>> 123에서 성능이 개선되어 저장합니다. 0.10505721345543861\n",
      "[0124/1000] train loss: 0.041200414299964905, valid loss: 0.10498001053929329, valid accuracy: 0.958041958041958\n",
      ">>>>>> 124에서 성능이 개선되어 저장합니다. 0.10498001053929329\n",
      "[0125/1000] train loss: 0.041496770456433296, valid loss: 0.1048646830022335, valid accuracy: 0.958041958041958\n",
      ">>>>>> 125에서 성능이 개선되어 저장합니다. 0.1048646830022335\n",
      "[0126/1000] train loss: 0.04111653100699186, valid loss: 0.10480635240674019, valid accuracy: 0.958041958041958\n",
      ">>>>>> 126에서 성능이 개선되어 저장합니다. 0.10480635240674019\n",
      "[0127/1000] train loss: 0.03153404779732227, valid loss: 0.10467180982232094, valid accuracy: 0.958041958041958\n",
      ">>>>>> 127에서 성능이 개선되어 저장합니다. 0.10467180982232094\n",
      "[0128/1000] train loss: 0.041453562676906586, valid loss: 0.10454203933477402, valid accuracy: 0.958041958041958\n",
      ">>>>>> 128에서 성능이 개선되어 저장합니다. 0.10454203933477402\n",
      "[0129/1000] train loss: 0.0401687091216445, valid loss: 0.10444756969809532, valid accuracy: 0.958041958041958\n",
      ">>>>>> 129에서 성능이 개선되어 저장합니다. 0.10444756969809532\n",
      "[0130/1000] train loss: 0.039626648649573326, valid loss: 0.10427456721663475, valid accuracy: 0.958041958041958\n",
      ">>>>>> 130에서 성능이 개선되어 저장합니다. 0.10427456721663475\n",
      "[0131/1000] train loss: 0.040266647934913635, valid loss: 0.10417741909623146, valid accuracy: 0.958041958041958\n",
      ">>>>>> 131에서 성능이 개선되어 저장합니다. 0.10417741909623146\n",
      "[0132/1000] train loss: 0.0359161589294672, valid loss: 0.10413315147161484, valid accuracy: 0.958041958041958\n",
      ">>>>>> 132에서 성능이 개선되어 저장합니다. 0.10413315147161484\n",
      "[0133/1000] train loss: 0.04030139371752739, valid loss: 0.10405976697802544, valid accuracy: 0.958041958041958\n",
      ">>>>>> 133에서 성능이 개선되어 저장합니다. 0.10405976697802544\n",
      "[0134/1000] train loss: 0.02837416436523199, valid loss: 0.10403011739253998, valid accuracy: 0.958041958041958\n",
      ">>>>>> 134에서 성능이 개선되어 저장합니다. 0.10403011739253998\n",
      "[0135/1000] train loss: 0.03904182091355324, valid loss: 0.10395009443163872, valid accuracy: 0.958041958041958\n",
      ">>>>>> 135에서 성능이 개선되어 저장합니다. 0.10395009443163872\n",
      "[0136/1000] train loss: 0.0388934351503849, valid loss: 0.10378256067633629, valid accuracy: 0.958041958041958\n",
      ">>>>>> 136에서 성능이 개선되어 저장합니다. 0.10378256067633629\n",
      "[0137/1000] train loss: 0.03855035826563835, valid loss: 0.10361873358488083, valid accuracy: 0.958041958041958\n",
      ">>>>>> 137에서 성능이 개선되어 저장합니다. 0.10361873358488083\n",
      "[0138/1000] train loss: 0.037897732108831406, valid loss: 0.10354386270046234, valid accuracy: 0.958041958041958\n",
      ">>>>>> 138에서 성능이 개선되어 저장합니다. 0.10354386270046234\n",
      "[0139/1000] train loss: 0.03530350513756275, valid loss: 0.10340898111462593, valid accuracy: 0.958041958041958\n",
      ">>>>>> 139에서 성능이 개선되어 저장합니다. 0.10340898111462593\n",
      "[0140/1000] train loss: 0.037593722343444824, valid loss: 0.10325596109032631, valid accuracy: 0.958041958041958\n",
      ">>>>>> 140에서 성능이 개선되어 저장합니다. 0.10325596109032631\n",
      "[0141/1000] train loss: 0.03672230616211891, valid loss: 0.10307180508971214, valid accuracy: 0.958041958041958\n",
      ">>>>>> 141에서 성능이 개선되어 저장합니다. 0.10307180508971214\n",
      "[0142/1000] train loss: 0.03696190565824509, valid loss: 0.10294056683778763, valid accuracy: 0.958041958041958\n",
      ">>>>>> 142에서 성능이 개선되어 저장합니다. 0.10294056683778763\n",
      "[0143/1000] train loss: 0.03593140468001366, valid loss: 0.10275660082697868, valid accuracy: 0.958041958041958\n",
      ">>>>>> 143에서 성능이 개선되어 저장합니다. 0.10275660082697868\n",
      "[0144/1000] train loss: 0.03713240846991539, valid loss: 0.10261773690581322, valid accuracy: 0.958041958041958\n",
      ">>>>>> 144에서 성능이 개선되어 저장합니다. 0.10261773690581322\n",
      "[0145/1000] train loss: 0.03639271855354309, valid loss: 0.1024092361330986, valid accuracy: 0.958041958041958\n",
      ">>>>>> 145에서 성능이 개선되어 저장합니다. 0.1024092361330986\n",
      "[0146/1000] train loss: 0.03517757169902325, valid loss: 0.10222692042589188, valid accuracy: 0.958041958041958\n",
      ">>>>>> 146에서 성능이 개선되어 저장합니다. 0.10222692042589188\n",
      "[0147/1000] train loss: 0.0348922424018383, valid loss: 0.10200148820877075, valid accuracy: 0.958041958041958\n",
      ">>>>>> 147에서 성능이 개선되어 저장합니다. 0.10200148820877075\n",
      "[0148/1000] train loss: 0.03504764102399349, valid loss: 0.10177006945014, valid accuracy: 0.958041958041958\n",
      ">>>>>> 148에서 성능이 개선되어 저장합니다. 0.10177006945014\n",
      "[0149/1000] train loss: 0.03268359135836363, valid loss: 0.10160533711314201, valid accuracy: 0.958041958041958\n",
      ">>>>>> 149에서 성능이 개선되어 저장합니다. 0.10160533711314201\n",
      "[0150/1000] train loss: 0.02548717614263296, valid loss: 0.10148203000426292, valid accuracy: 0.958041958041958\n",
      ">>>>>> 150에서 성능이 개선되어 저장합니다. 0.10148203000426292\n",
      "[0151/1000] train loss: 0.03428120817989111, valid loss: 0.10130735486745834, valid accuracy: 0.958041958041958\n",
      ">>>>>> 151에서 성능이 개선되어 저장합니다. 0.10130735486745834\n",
      "[0152/1000] train loss: 0.034401025623083115, valid loss: 0.1011817678809166, valid accuracy: 0.958041958041958\n",
      ">>>>>> 152에서 성능이 개선되어 저장합니다. 0.1011817678809166\n",
      "[0153/1000] train loss: 0.03391601052135229, valid loss: 0.1011737659573555, valid accuracy: 0.958041958041958\n",
      ">>>>>> 153에서 성능이 개선되어 저장합니다. 0.1011737659573555\n",
      "[0154/1000] train loss: 0.03364192694425583, valid loss: 0.10107435658574104, valid accuracy: 0.958041958041958\n",
      ">>>>>> 154에서 성능이 개선되어 저장합니다. 0.10107435658574104\n",
      "[0155/1000] train loss: 0.03348097763955593, valid loss: 0.10096731781959534, valid accuracy: 0.958041958041958\n",
      ">>>>>> 155에서 성능이 개선되어 저장합니다. 0.10096731781959534\n",
      "[0156/1000] train loss: 0.03266597352921963, valid loss: 0.1008860170841217, valid accuracy: 0.958041958041958\n",
      ">>>>>> 156에서 성능이 개선되어 저장합니다. 0.1008860170841217\n",
      "[0157/1000] train loss: 0.033353869803249836, valid loss: 0.10075793042778969, valid accuracy: 0.958041958041958\n",
      ">>>>>> 157에서 성능이 개선되어 저장합니다. 0.10075793042778969\n",
      "[0158/1000] train loss: 0.033087863586843014, valid loss: 0.10070914775133133, valid accuracy: 0.958041958041958\n",
      ">>>>>> 158에서 성능이 개선되어 저장합니다. 0.10070914775133133\n",
      "[0159/1000] train loss: 0.03290882334113121, valid loss: 0.1005181148648262, valid accuracy: 0.958041958041958\n",
      ">>>>>> 159에서 성능이 개선되어 저장합니다. 0.1005181148648262\n",
      "[0160/1000] train loss: 0.0318604726344347, valid loss: 0.10044577717781067, valid accuracy: 0.958041958041958\n",
      ">>>>>> 160에서 성능이 개선되어 저장합니다. 0.10044577717781067\n",
      "[0161/1000] train loss: 0.03252995852380991, valid loss: 0.10038500279188156, valid accuracy: 0.958041958041958\n",
      ">>>>>> 161에서 성능이 개선되어 저장합니다. 0.10038500279188156\n",
      "[0162/1000] train loss: 0.031929098069667816, valid loss: 0.1002342440187931, valid accuracy: 0.958041958041958\n",
      ">>>>>> 162에서 성능이 개선되어 저장합니다. 0.1002342440187931\n",
      "[0163/1000] train loss: 0.030170018784701824, valid loss: 0.10014909505844116, valid accuracy: 0.958041958041958\n",
      ">>>>>> 163에서 성능이 개선되어 저장합니다. 0.10014909505844116\n",
      "[0164/1000] train loss: 0.03160068020224571, valid loss: 0.10012762248516083, valid accuracy: 0.958041958041958\n",
      ">>>>>> 164에서 성능이 개선되어 저장합니다. 0.10012762248516083\n",
      "[0165/1000] train loss: 0.028721905313432217, valid loss: 0.100144162774086, valid accuracy: 0.958041958041958\n",
      "[0166/1000] train loss: 0.03084378596395254, valid loss: 0.10014157742261887, valid accuracy: 0.958041958041958\n",
      "[0167/1000] train loss: 0.029972247779369354, valid loss: 0.1001964770257473, valid accuracy: 0.958041958041958\n",
      "[0168/1000] train loss: 0.029525763355195522, valid loss: 0.10014516115188599, valid accuracy: 0.958041958041958\n",
      "[0169/1000] train loss: 0.020858563017100096, valid loss: 0.1000673957169056, valid accuracy: 0.958041958041958\n",
      ">>>>>> 169에서 성능이 개선되어 저장합니다. 0.1000673957169056\n",
      "[0170/1000] train loss: 0.029990368522703648, valid loss: 0.09999408945441246, valid accuracy: 0.958041958041958\n",
      ">>>>>> 170에서 성능이 개선되어 저장합니다. 0.09999408945441246\n",
      "[0171/1000] train loss: 0.03001611866056919, valid loss: 0.09985034167766571, valid accuracy: 0.958041958041958\n",
      ">>>>>> 171에서 성능이 개선되어 저장합니다. 0.09985034167766571\n",
      "[0172/1000] train loss: 0.02915019541978836, valid loss: 0.09987205639481544, valid accuracy: 0.958041958041958\n",
      "[0173/1000] train loss: 0.02825650619342923, valid loss: 0.09971902891993523, valid accuracy: 0.958041958041958\n",
      ">>>>>> 173에서 성능이 개선되어 저장합니다. 0.09971902891993523\n",
      "[0174/1000] train loss: 0.01513149356469512, valid loss: 0.0994129590690136, valid accuracy: 0.958041958041958\n",
      ">>>>>> 174에서 성능이 개선되어 저장합니다. 0.0994129590690136\n",
      "[0175/1000] train loss: 0.027823874726891518, valid loss: 0.0991966612637043, valid accuracy: 0.958041958041958\n",
      ">>>>>> 175에서 성능이 개선되어 저장합니다. 0.0991966612637043\n",
      "[0176/1000] train loss: 0.024776190519332886, valid loss: 0.09884801506996155, valid accuracy: 0.958041958041958\n",
      ">>>>>> 176에서 성능이 개선되어 저장합니다. 0.09884801506996155\n",
      "[0177/1000] train loss: 0.02760711871087551, valid loss: 0.0986277349293232, valid accuracy: 0.958041958041958\n",
      ">>>>>> 177에서 성능이 개선되어 저장합니다. 0.0986277349293232\n",
      "[0178/1000] train loss: 0.027196897193789482, valid loss: 0.09841885045170784, valid accuracy: 0.958041958041958\n",
      ">>>>>> 178에서 성능이 개선되어 저장합니다. 0.09841885045170784\n",
      "[0179/1000] train loss: 0.026343987323343754, valid loss: 0.09841324016451836, valid accuracy: 0.958041958041958\n",
      ">>>>>> 179에서 성능이 개선되어 저장합니다. 0.09841324016451836\n",
      "[0180/1000] train loss: 0.0276003060862422, valid loss: 0.09841831773519516, valid accuracy: 0.958041958041958\n",
      "[0181/1000] train loss: 0.02744035329669714, valid loss: 0.09841064736247063, valid accuracy: 0.958041958041958\n",
      ">>>>>> 181에서 성능이 개선되어 저장합니다. 0.09841064736247063\n",
      "[0182/1000] train loss: 0.02682699915021658, valid loss: 0.09842832013964653, valid accuracy: 0.958041958041958\n",
      "[0183/1000] train loss: 0.018173477612435818, valid loss: 0.09845812991261482, valid accuracy: 0.958041958041958\n",
      "[0184/1000] train loss: 0.026382237672805786, valid loss: 0.09846080467104912, valid accuracy: 0.958041958041958\n",
      "[0185/1000] train loss: 0.025973418727517128, valid loss: 0.09847386553883553, valid accuracy: 0.958041958041958\n",
      "[0186/1000] train loss: 0.016036180779337883, valid loss: 0.09848416224122047, valid accuracy: 0.958041958041958\n",
      "[0187/1000] train loss: 0.02611318603157997, valid loss: 0.09844008088111877, valid accuracy: 0.958041958041958\n",
      "[0188/1000] train loss: 0.025856684893369675, valid loss: 0.09854660928249359, valid accuracy: 0.958041958041958\n",
      "[0189/1000] train loss: 0.017460885923355818, valid loss: 0.09845591336488724, valid accuracy: 0.958041958041958\n",
      "[0190/1000] train loss: 0.0257948637008667, valid loss: 0.09846572950482368, valid accuracy: 0.958041958041958\n",
      "[0191/1000] train loss: 0.02547732461243868, valid loss: 0.09846213087439537, valid accuracy: 0.958041958041958\n",
      "191 에폭에서 조기종료 합니다. 0.09841064736247063에서 개선되지 않음.\n",
      "걸린시간(초): 1.4132335186004639\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "### 모델 학습(train) 로직 작성\n",
    "#### 검증 결과 -> train loss, valid_loss, valid_accuracy\n",
    "### 모델 성능이 개선될 때 마다 저장.\n",
    "### 조기종료 - 10 epoch 동안 성능 개선이 없으면 조기종료\n",
    "\n",
    "save_path = \"saved_models/bc_model.pt\"\n",
    "best_score = torch.inf   # validation loss 기준으로 저장/조기종료 여부 확인.\n",
    "patience = 10\n",
    "trigger_cnt = 0\n",
    "\n",
    "train_losses, valid_losses, valid_acces = [], [], []\n",
    "\n",
    "s = time.time()\n",
    "for epoch in range(epochs):\n",
    "    ################### train #######################\n",
    "    b_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_train, y_train in train_loader:\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        pred = b_model(X_train)  # positive일 확률\n",
    "        loss = loss_fn(pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    ################### validation ##################\n",
    "    b_model.eval()\n",
    "    valid_loss = valid_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            pred_test = b_model(X_test) # positive일 확률\n",
    "            valid_loss += loss_fn(pred_test, y_test).item()\n",
    "            #  이진 분류에서 accuracy\n",
    "            valid_acc += torch.sum((pred_test > 0.5).type(torch.int32) == y_test).item()\n",
    "        valid_loss /= len(test_loader)\n",
    "        valid_acc /= len(test_loader.dataset)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_acces.append(valid_acc)\n",
    "\n",
    "    log_template = \"[{:04d}/{}] train loss: {}, valid loss: {}, valid accuracy: {}\"\n",
    "    print(log_template.format(epoch+1, epochs, train_loss, valid_loss, valid_acc))\n",
    "    # 모델 저장, 조기종료\n",
    "    if valid_loss <  best_score: # 성능 개선\n",
    "        print(f\">>>>>> {epoch+1}에서 성능이 개선되어 저장합니다. {valid_loss}\")\n",
    "        torch.save(b_model, save_path)\n",
    "        best_score = valid_loss\n",
    "        trigger_cnt = 0\n",
    "    else:\n",
    "        trigger_cnt += 1\n",
    "        if patience == trigger_cnt:\n",
    "            print(f\"{epoch+1} 에폭에서 조기종료 합니다. {best_score}에서 개선되지 않음.\")\n",
    "            break\n",
    "\n",
    "e = time.time()\n",
    "print(\"걸린시간(초):\", e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "J-9VvvGBIUqk"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## loss, acc 시각화\n",
    "plt.plot(range(len(train_losses)), train_losses, label='train loss')\n",
    "plt.plot(range(len(valid_losses)), valid_losses, label=\"valid loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(valid_acces)), valid_acces)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "id": "JK-udzqSIUqk",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BreastCancerModel(\n",
       "  (lr1): Linear(in_features=30, out_features=32, bias=True)\n",
       "  (lr2): Linear(in_features=32, out_features=8, bias=True)\n",
       "  (lr3): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (logistic): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = torch.load(save_path, weights_only=False)\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "bKpGntfpIUqk"
   },
   "outputs": [],
   "source": [
    "def predict_bc(model, X, device=\"cpu\"):\n",
    "    # model로 X를 추론한 결과를 반환\n",
    "    # label, 확률\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        pred_proba = model(X) #[[0.8], [0.1], ...]\n",
    "        pred_class = (pred_proba > 0.5).type(torch.int32) #[[1], [0], ...]\n",
    "        for class_index, proba in zip(pred_class, pred_proba):\n",
    "            # print(class_index, proba if class_index.item() == 1 else 1-proba)\n",
    "            result.append((class_index.item(), proba if class_index.item() == 1 else 1-proba))\n",
    "            # proba if class_index.item() == 1 else 1-proba\n",
    "            ## proba: 양성일 확률\n",
    "            # 모델이 예측한 class가 1: proba값을 확률사용.\n",
    "            # 모델이 예측한 class가 0: 1-proba 값을 확률로 사용(양성확률를 음성확률로 변환.)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "b5qoR2wFIUqk"
   },
   "outputs": [],
   "source": [
    "new_data = torch.tensor(X_test_scaled[:5], dtype=torch.float32)\n",
    "# print(new_data.shape)\n",
    "result = predict_bc(best_model, new_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "O7w3eaGTIUqk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, tensor([0.9979])),\n",
       " (0, tensor([1.0000])),\n",
       " (0, tensor([0.9998])),\n",
       " (1, tensor([0.9998])),\n",
       " (0, tensor([0.9985]))]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NOcMLqvIUqk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6oRfRD6IUqk"
   },
   "source": [
    "# 모델 유형별 구현 정리\n",
    "\n",
    "## 공통\n",
    "\n",
    "-   Input layer(첫번째 Layer)의 in_features\n",
    "    -   입력데이터의 feature(속성) 개수에 맞춰준다.\n",
    "-   Hidden layer 수\n",
    "    -   경험적(art)으로 정한다.\n",
    "    -   Hidden layer에 Linear를 사용하는 경우 보통 feature 수를 줄여 나간다. (핵심 특성들을 추출해나가는 과정의 개념.)\n",
    "\n",
    "## 회귀 모델\n",
    "\n",
    "-   output layer의 출력 unit개수(out_features)\n",
    "    -   정답의 개수\n",
    "    -   ex\n",
    "        -   집값: 1\n",
    "        -   아파트가격, 단독가격, 빌라가격: 3 => y의 개수에 맞춘다.\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   일반적으로 **None**\n",
    "    -   값의 범위가 설정되 있고 그 범위의 값을 출력하는 함수가 있을 경우\n",
    "        -   ex) 0 ~ 1: logistic(Sigmoid), -1 ~ 1: hyperbolic tangent(Tanh)\n",
    "-   loss함수\n",
    "    -   MSELoss\n",
    "-   평가지표\n",
    "    -   MSE, RMSE, R square($R^2$)\n",
    "\n",
    "## 다중분류 모델\n",
    "\n",
    "-   output layer의 unit 개수\n",
    "    -   정답 class(고유값)의 개수\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   Softmax: 클래스별 확률을 출력\n",
    "-   loss함수\n",
    "    -   **categrocial crossentropy**\n",
    "    -   파이토치 함수\n",
    "        -   **CrossEntropyLoss** = NLLLoss(정답) + LogSoftmax(모델 예측값)\n",
    "        -   **NLLLoss**\n",
    "            -   정답을 OneHot Encoding 처리 후 Loss를 계산한다.\n",
    "            -   입력으로 LogSoftmax 처리한 모델 예측값과 onehot encoding 안 된 정답을 받는다.\n",
    "        -   **LogSoftmax**\n",
    "            -   입력값에 Softmax 계산후 그 Log를 계산한다.\n",
    "                -   NLLLoss의 모델 예측값 입력값으로 처리할 때 사용한다.\n",
    "\n",
    "```python\n",
    "pred = model(input)\n",
    "loss1 = nn.NLLLoss(nn.LogSoftmax(dim=-1)(pred), y)\n",
    "# or\n",
    "loss2 = nn.CrossEntropyLoss()(pred, y)\n",
    "```\n",
    "\n",
    "## 이진분류 모델\n",
    "\n",
    "-   output layer의 unit 개수\n",
    "    -   1개 (positive일 확률)\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   Sigmoid(Logistic)\n",
    "-   loss 함수\n",
    "    -   **Binary crossentropy**\n",
    "    -   파이토치 함수: **BCELoss**\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
